BUILDING 
INTELLIGENT 
AGENTS 
THAT 
LEARN 
TO 
RETRIEVE 
AND 
EXTRACT 
INFORMATION 


By 


Tina 
Eliassi-Rad 


A 
dissertation 
submitted 
in 
partial 
fulfillment 
of 
the 
requirements 
for 
the 
degree 
of 


Doctor 
of 
Philosophy 
(Computer 
Sciences) 


at 
the 


UNIVERSITY 
OF 
WISCONSIN 
– 
MADISON 


2001 



For 
Branden 



Abstract 


The 
rapid 
growth 
of 
on-line 
information 
has 
created 
a 
surge 
of 
interest 
in 
tools 
that 
are 
able 
to 
retrieve 
and 
extract 
information 
from 
on-line 
documents. 
In 
this 
thesis, 
I 
present 
and 
evaluate 
a 
computer 
system 
that 
rapidly 
and 
easily 
builds 
instructable 
and 
self-adaptive 
software 
agents 
for 
both 
the 
information 
retrieval 
(IR) 
and 
the 
information 
extraction 
(IE) 
tasks. 


My 
system 
is 
called 
Wawa 
(short 
for 
Wisconsin 
Adaptive 
Web 
Assistant). 
Wawa 
interacts 
with 
the 
user 
and 
an 
on-line 
(textual) 
environment 
(e.g., 
the 
Web) 
to 
build 
an 
intelligent 
agent 
for 
retrieving 
and 
extracting 
information. 
Wawa 
has 
two 
sub-systems: 
(i) 
an 
information 
retrieval 
(IR) 
sub-system, 
called 
WAWA-IR; 
and, 
(ii) 
an 
information 
extraction 
(IE) 
sub-system, 
called 
WAWAI
E. 
Wawa-IR 
is 
a 
general 
search-engine 
agent, 
which 
can 
be 
trained 
to 
produce 
specialized 
and 
personalized 
IR 
agents. 
Wawa-IE 
is 
a 
general 
extractor 
system, 
which 
creates 
specialized 
agents 
that 
accurately 
extract 
pieces 
of 
information 
from 
documents 
in 
the 
domain 
of 
interest. 


Wawa 
utilizes 
a 
theory-renement 
approach 
to 
build 
its 
intelligent 
agents. 
There 
are 
four 
four 
primary 
advantages 
of 
using 
such 
an 
approach. 
First, 
Wawa's 
agents 
are 
able 
to 
perform 
reasonably 
well 
initially 
because 
they 
are 
able 
to 
utilize 
users’ 
prior 
knowledge. 
Second, 
users’ 
prior 
knowledge 
does 
not 
have 
to 
be 
correct 
since 
it 
is 
rened 
through 
learning. 
Third, 
the 
use 
of 
prior 
knowledge, 
plus 
the 
continual 
dialog 
between 
the 
user 
and 
an 
agent, 
decreases 
the 
need 
for 
a 
large 
number 
of 
training 
examples 
because 
training 
is 
not 
limited 
to 
a 
binary 
representation 
of 
positive 
and 
negative 
examples. 
Finally, 
Wawa 
provides 
an 
appealing 
middle 
ground 
between 
non-adaptive 
agent 
programming 
languages 
and 
systems 
that 
solely 
learn 
user 
preferences 
from 
training 
examples. 


Wawa's 
agents 
have 
performed 
quite 
well 
in 
empirical 
studies. 
Wawa-IR 
experiments 
demonstrate 
the 
ecacy 
of 
incorporating 
the 
feedback 
provided 



by 
the 
Web 
into 
the 
agent's 
neural 
networks 
to 
improve 
the 
evaluation 
of 
pot
ential 
hyperlinks 
to 
traverse. 
Wawa-IE 
experiments 
produce 
results 
that 
are 
competitive 
with 
other 
state-of-art 
systems. 
Moreover, 
they 
demonstrate 
that 
Wawa-IE 
agents 
are 
able 
to 
intelligently 
and 
eciently 
select 
from 
the 
space 
of 
possible 
extractions 
and 
solve 
multi-slot 
extraction 
problems. 



Acknowledgements 


I 
am 
grateful 
to 
the 
following 
people 
and 
organizations: 


• 
My 
advisor, 
Jude 
Shavlik, 
for 
his 
constant 
support, 
guidance, 
and 
gene
rosity 
throughout 
this 
research. 
• 
My 
committee 
members, 
Mark 
Craven, 
Olvi 
Mangasarian, 
David 
Page, 
and 
Louise 
Robbins, 
for 
their 
time 
and 
consideration. 
• 
My 
husband, 
Branden 
Fitelson, 
for 
his 
unwavering 
love 
and 
support 
duri
ng 
the 
past 
8.5 
years. 
• 
My 
parents 
for 
their 
love, 
support, 
generosity, 
and 
belief 
in 
my 
abilities. 
• 
My 
brothers, 
Babak 
and 
Nima, 
for 
being 
wonderful 
siblings. 
• 
Zari 
and 
the 
entire 
Hafez 
family 
for 
their 
constant 
kindness 
over 
the 
past 
13 
years. 
• 
Anne 
Condon 
for 
being 
a 
great 
mentor 
since 
spring 
of 
1992. 
• 
Peter 
Andreae 
for 
his 
useful 
comments 
on 
this 
research. 
• 
Rich 
Maclin 
for 
being 
supportive 
whenever 
our 
paths 
crossed. 
• 
Lorene 
Webber 
for 
all 
her 
administrative 
help. 
• 
National 
Science 
Foundation 
(NSF) 
grant 
IRI-9502990, 
National 
Library 
of 
Medicine 
(NLM) 
grant 
1 
R01 
LM07050-01, 
and 
University 
of 
Wisconsin 
Vilas 
Trust 
for 
supporting 
this 
research. 

List 
of 
Figures 


1 
An 
Overview 
of 
Wawa 
....................... 
2 
2 
The 
Interaction 
between 
a 
User, 
an 
Intelligent 
Agent, 
and 
the 


Agent'sEnvironment 
........................ 
4 


3 
ATwo-LayerFeed-ForwardNetwork 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
7 
4 
An 
Examples 
of 
the 
Kbann 
Algorithm.............. 
10 


5 
ScoringaPagewithaWAWAAgent 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
17 
6 
Central 
Functions 
of 
Wawa'sAgents 
............... 
17 
7 
Wawa's 
Central 
Functions 
Score 
Web 
Pages 
and 
Hyperlinks 
. 
. 
18 
8 
InternalRepresentationofWebPages 
............... 
19 
9 
Using 
a 
Three-Word 
Sliding 
Window 
to 
Capture 
Word-Order 
In


formationonaPage 
........................ 
20 
10 
Mapping 
Advice 
into 
ScorePage 
Network 
........... 
27 
11 
ScoringaPagewithaSlidingWindow 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
30 


12 
ReestimatingtheValueofaHyperlink 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
36 


13 
Interface 
of 
Wawa-IR'sHome-PageFinder............ 
41 


14 
Extraction 
of 
Speaker 
Names 
with 
Wawa-IE 
.......... 
55 
15 
Wawa-IE'sModiedWalkSATAlgorithm 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
61 
16 
Wawa-IE'sModiedGSATAlgorithm 
.............. 
62 
17 
Wawa-IE's 
Hill-Climbing 
Algorithm 
With 
Random 
Restarts 
. 
. 
63 
18 
Wawa-IE'sStochasticSelector 
.................. 
65 
19 
BuildingaTrainedIEagent 
.................... 
67 
20 
TestingaTrainedIEAgent 
.................... 
69 


21 
Wawa-IE's 
Speaker 
Extractor: 
Precision 
& 
Recall 
Curves 
. 
. 
. 
77 



22 
Subcellular-Localization 
Domain: 
Precision 
& 
Recall 
Curves 
. 
. 
79 
23 
Disorder-Association 
Domian: 
Precision 
& 
Recall 
Curves 
. 
. 
. 
. 
81 
24 
Subcellular-Localization 
Domain: 
F1-measure 
versus 
Percentage 
of 
Negative 
Training 
Candidates 
Used 
for 
Dierent 
Selector 
Alg
orithms 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
82 
25 
Disorder-Association 
Domain: 
F1-measure 
versus 
Percentage 
of 
Negative 
Training 
Candidates 
Used 
for 
Dierent 
Selector 
Algor
ithms 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
83 
26 
Subcellular-Localization 
Domain: 
F1-measure 
versus 
Number 
of 
Positive 
Training 
Instances 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
84 
27 
Subcellular-Localization 
Domain: 
F1-measure 
versus 
Dierent 
Groups 
of 
Advice 
Rules 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
85 
28 
Disorder-Association 
Domain: 
F1-measure 
versus 
Number 
of 
Positive 
Training 
Instances 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
86 
29 
Disorder-Association 
Domain: 
F1-measure 
versus 
Dierent 
Groups 
of 
Advice 
Rules 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
87 



List 
of 
Tables 


1 
TheBackpropagationAlgorithm. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
8 
2 
A 
Term 
× 
DocumentMatrix.................... 
12 
3 
Mapping 
between 
Categories 
Assigned 
to 
Web 
Pages 
and 
Their 


NumericScores 
........................... 
14 


4 
SampleExtractedInputFeatures 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
22 
5 
Permissible 
Actions 
in 
an 
Advice 
Statement 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
24 
6 
SampleAdvice 
........................... 
25 


7 
Wawa's 
Information-Retrieval 
Algorithm 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
32 


8 
The 
Supervised-Learning 
Technique 
Used 
in 
Training 
ScorePage 


Network 
............................... 
43 
9 
Empirical 
Results: 
Wawa-IR 
vs 
Ahoy! 
and 
HotBot 
..... 
47 
10 
Empirical 
Results 
on 
Dierent 
Versions 
of 
Wawa-IR's 
Home-

PageFinder 
............................. 
48 
11 
Average 
Number 
of 
Pages 
Fetched 
by 
Wawa-IR 
Before 
the 
Tar


getHomePage 
........................... 
49 
12 
Two 
Dierent 
Wawa-IR 
Home-Page 
Finders 
versus 
Google 
. 
50 
13 
Notationforthe2001Experiments 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
51 
14 
Home-Page 
Finder 
Performances 
in 
2001 
under 
Dierent 
Wawa-

IR 
Settings 
............................. 
51 


15 
Wawa's 
Information-Extraction 
Algorithm 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
56 


16 
Sample 
Rules 
Used 
in 
the 
Domain 
Theories 
of 
Speaker 
and 
Lo


cationSlots 
............................. 
74 
17 
Results 
on 
the 
Speaker 
Slot 
for 
Seminar 
Announcements 
Task 
. 
75 
18 
Results 
on 
the 
Location 
Slot 
for 
Seminar 
Announcements 
Task 
. 
76 



Contents 


Abstract 
ii 


Acknowledgements 
iv 


1 
Introduction 
1 


1.1 
WisconsinAdaptiveWebAssistant 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
1 


1.2 
ThesisStatement 
.......................... 
4 


1.3 
ThesisOverview........................... 
4 


2 
Background 
6 


2.1 
Multi-Layer 
Feed-Forward 
Neural 
Networks 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
6 


2.2 
Knowledge-Based 
Articial 
Neural 
Networks 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
9 


2.3 
ReinforcementLearning 
...................... 
10 


2.4 
InformationRetrieval 
........................ 
12 


2.5 
InformationExtraction 
....................... 
14 


3 
WAWA's 
Core 
16 


3.1 
InputFeatures............................ 
18 


3.2 
AdviceLanguage 
.......................... 
23 


3.2.1 
Complex 
Advice 
Constructs 
and 
Predicates 
. 
. 
. 
. 
. 
. 
. 
24 


3.2.2 
AdviceVariables 
...................... 
25 


3.3 
Compilation 
of 
Advice 
into 
Neural 
Networks 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
26 


3.4 
Scoring 
Arbitrarily 
Long 
Pages 
and 
Links 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
28 


4 
Using 
WAWA 
to 
Retrieve 
Information 
from 
the 
Web 
31 


4.1 
IRSystemDescription 
....................... 
31 


4.2 
Training 
WAWA's 
Two 
Neural 
Networks 
for 
IR 
. 
. 
. 
. 
. 
. 
. 
. 
. 
34 


4.3 
Deriving 
Training 
Examples 
for 
ScoreLink 
........... 
35 



ix 


4.4 
Summary 
.............................. 
38 


5 
Retrieval 
Experiments 
with 
WAWA 
40 


5.1 
An 
Instructable 
and 
Adaptive 
Home-Page 
Finder 
. 
. 
. 
. 
. 
. 
. 
. 
40 


5.2 
MotivationandMethodology.................... 
42 


5.3 
ResultsandDiscussionfrom1998 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
46 


5.4 
ResultsandDiscussionfrom2001 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
49 


5.5 
Summary 
.............................. 
52 


6 
Using 
WAWA 
to 
Extract 
Information 
from 
Text 
53 


6.1 
IESystemDescription 
....................... 
56 


6.2 
CandidateGeneration 
....................... 
58 


6.3 
CandidateSelection......................... 
59 


6.4 
TraininganIEAgent 
........................ 
66 


6.5 
TestingaTrainedIEAgent 
.................... 
68 


6.6 
Summary 
.............................. 
69 


7 
Extraction 
Experiments 
with 
WAWA 
71 


7.1 
CMU 
Seminar-Announcement 
Domain 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
72 


7.2 
BiomedicalDomains 
........................ 
77 


7.3 
ReducingtheComputationalBurden 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
81 


7.4 
ScalingExperiments 
........................ 
83 


7.5 
Summary 
.............................. 
87 


8 
Related 
Work 
88 


8.1 
LearningtoRetrievefromTextandtheWeb 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
88 


8.2 
InstructableSoftware 
........................ 
89 


8.3 
Learning 
to 
Extract 
Information 
from 
Text 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
91 


8.3.1 
Using 
Relational 
Learners 
to 
Extract 
From 
Text 
. 
. 
. 
. 
. 
91 


8.3.2 
Using 
Hidden 
Markov 
Models 
to 
Extract 
From 
Text 
. 
. 
. 
95 


8.3.3 
Using 
Extraction 
Patterns 
to 
Categorize 
Text 
. 
. 
. 
. 
. 
. 
96 



x 


8.3.4 
Using 
Text 
Categorizers 
to 
Extract 
Patterns 
. 
. 
. 
. 
. 
. 
. 
97 


8.3.5 
TheNamed-EntityProblem 
................ 
98 


8.3.6 
Adaptive 
and 
Intelligent 
Sampling 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
. 
99 


9 
Conclusions 
101 


9.1 
Contributions 
............................ 
101 


9.2 
LimitationsandFutureDirections 
................. 
103 


9.3 
FinalRemarks............................ 
104 
A 
WAWA's 
Advice 
Language 
105 
B 
Advice 
Used 
in 
the 
Home-Page 
Finder 
116 
C 
Advice 
Used 
in 
the 
Seminar-Announcement 
Extractor 
129 
D 
Advice 
Used 
in 
the 
Subcellular-Localization 
Extractor 
133 


D.1 
MichaelWaddell'sRules 
...................... 
133 


D.2 
MyRules 
.............................. 
136 


E 
Advice 
Used 
in 
the 
Disorder-Association 
Extractor 
139 
Bibliography 
143 



Chapter 
1 


Introduction 


The 
exponential 
growth 
of 
on-line 
information 
(Lawrence 
and 
Giles 
1999) 
has 
increased 
demand 
for 
tools 
that 
are 
able 
to 
eciently 
retrieve 
and 
extract 
textual 
information 
from 
on-line 
documents. 
In 
a 
ideal 
world, 
you 
would 
be 
able 
to 
ins
tantaneously 
retrieve 
precisely 
the 
information 
you 
want 
(whether 
it 
is 
a 
whole 
document 
or 
fragments 
of 
it). 
What 
is 
the 
next 
best 
option? 
Consider 
having 
an 
assistant, 
which 
rapidly 
and 
easily 
builds 
instructable 
and 
self-adaptive 
software 
agents 
for 
both 
the 
information 
retrieval 
(IR) 
and 
the 
information 
extraction 
(IE) 
tasks. 
These 
intelligent 
software 
agents 
would 
learn 
your 
interests 
and 
automatically 
rene 
their 
models 
of 
your 
preferences 
over 
time. 
Their 
mission 
would 
be 
to 
spend 
24 
hours 
a 
day 
looking 
for 
documents 
of 
interest 
to 
you 
and 
answering 
specic 
questions 
that 
you 
might 
have. 
In 
this 
thesis, 
I 
present 
and 
evaluate 
such 
an 
assistant. 


1.1 
Wisconsin 
Adaptive 
Web 
Assistant 
My 
assistant 
is 
called 
Wawa 
(short 
for 
Wisconsin 
Adaptive 
Web 
Assistant). 
Wawa 
interacts 
with 
the 
user 
and 
an 
on-line 
(textual) 
environment 
(e.g., 
the 
Web) 
to 
build 
an 
intelligent 
agent 
for 
retrieving 
and/or 
extracting 
information. 
Figure 
1 
illustrates 
an 
overview 
of 
Wawa. 
Wawa 
has 
two 
sub-systems: 
(i) 
an 
information-retrieval 
sub-system, 
called 
Wawa-IR; 
and, 
(ii) 
an 
information-
extraction 
sub-system, 
called 
WAWA-IE. 
Wawa-IR 
is 
a 
general 
search-engine 
agent, 
which 
can 
be 
trained 
to 
produce 
specialized 
and 
personalized 
IR 
agents. 




Wawa-IE 
is 
a 
general 
extractor 
system, 
which 
creates 
specialized 
agents 
that 
extract 
pieces 
of 
information 
from 
documents 
in 
the 
domain 
of 
interest. 


User 

Environment 
IR Agent 


IE Agent 


Figure 
1: 
An 
Overview 
of 
Wawa 


Wawa 
builds 
its 
agents 
based 
on 
ideas 
from 
the 
theory-renement 
commun
ity 
within 
machine 
learning 
(Pazzani 
and 
Kibler 
1992; 
Ourston 
and 
Mooney 
1994; 
Towell 
and 
Shavlik 
1994). 
Users 
specify 
their 
prior 
knowledge 
about 
the 
desired 
task. 
This 
knowledge 
is 
then 
\compiled” 
into 
\knowledge 
based” 
neural 
networks 
(Towell 
and 
Shavlik 
1994), 
thereby 
allowing 
subsequent 
renement 
whenever 
training 
examples 
are 
available. 
The 
advantages 
of 
using 
a 
theory-
renement 
approach 
to 
build 
intelligent 
agents 
are 
as 
follows: 


• 
Wawa's 
agents 
are 
able 
to 
perform 
reasonably 
well 
initially 
because 
they 
are 
able 
to 
utilize 
users’ 
prior 
knowledge. 
• 
Users’ 
prior 
knowledge 
does 
not 
have 
to 
be 
correct 
since 
it 
is 
rened 
through 
learning. 
• 
The 
use 
of 
prior 
knowledge, 
plus 
the 
continual 
dialog 
between 
the 
user 
and 
an 
agent, 
decreases 
the 
need 
for 
a 
large 
number 
of 
training 
examples 
because 
human-machine 
communication 
is 
not 
limited 
to 
solely 
providing 
positive 
and 
negative 
examples. 

• 
Wawa 
provides 
an 
appealing 
middle 
ground 
between 
non-adaptive 
agent 
programming 
languages 
(Etzioni 
and 
Weld 
1995; 
Wooldridge 
and 
Jenn
ings 
1995) 
and 
systems 
that 
solely 
learn 
user 
preferences 
from 
training 
examples 
(Pazzani, 
Muramatsu, 
and 
Billsus 
1996; 
Joachims, 
Freitag, 
and 
Mitchell 
1997). 
Wawa's 
agents 
are 
arguably 
intelligent 
because 
they 
can 
adapt 
their 
behavi
or 
according 
to 
the 
users’ 
instructions 
and 
the 
feedback 
they 
get 
from 
their 
environments. 
Specically, 
they 
are 
learning 
agents 
that 
use 
neural 
networks 
to 
store 
and 
modify 
their 
knowledge. 
Figure 
2 
illustrates 
the 
interaction 
between 
the 
user, 
an 
intelligent 
(Wawa) 
agent, 
and 
the 
agent's 
environment. 
The 
user1 
observes 
the 
agent's 
behavior 
(e.g., 
the 
quality 
of 
the 
pages 
retrieved) 
and 
prov
ides 
helpful 
instructions 
to 
the 
agent. 
Following 
Maclin 
and 
Shavlik 
(1996), 
I 
refer 
to 
users’ 
instructions 
as 
advice, 
since 
this 
name 
emphasizes 
that 
the 
agent 
does 
not 
blindly 
follow 
the 
user-provided 
instructions, 
but 
instead 
renes 
the 
advice 
based 
on 
its 
experiences. 
The 
user 
inputs 
his/her 
advice 
into 
a 
user-
friendly 
advice 
interface. 
The 
given 
advice 
is 
then 
processed 
and 
mapped 
into 
the 
agent's 
knowledge 
base 
(i.e., 
its 
two 
neural 
networks), 
where 
it 
gets 
rened 
based 
on 
the 
agent's 
experiences. 
Hence, 
the 
agent 
is 
able 
to 
represent 
the 
user 
model 
in 
its 
neural 
networks, 
which 
have 
representations 
for 
which 
eective 
learning 
algorithms 
are 
known 
(Mitchell 
1997). 


1 
I 
envision 
that 
there 
are 
two 
types 
of 
potential 
users 
of 
Wawa: 
(1) 
application 
developers, 
who 
build 
an 
intelligent 
agent 
on 
top 
of 
Wawa 
and 
(2) 
application 
users, 
who 
use 
the 
resulting 
agent. 
(When 
I 
use 
the 
phrase 
user 
in 
this 
thesis, 
I 
mean 
the 
former.) 
Both 
types 
of 
users 
can 
provide 
advice 
to 
the 
underlying 
neural 
networks, 
but 
I 
envision 
that 
usually 
the 
application 
users 
will 
indirectly 
do 
this 
through 
some 
specialized 
interface 
that 
the 
application 
developers 
create. 
A 
scenario 
like 
this 
is 
discussed 
in 
Chapter 
5. 



AdviceUserAgentBehaviorEnvironmentActionReformulatedAdviceMapperRule-to-NetworkProcessorAdviceInterfaceAdviceUserAgentBehaviorEnvironmentActionReformulatedAdviceMapperRule-to-NetworkProcessorAdviceInterfaceAdvice
Figure 
2: 
The 
Interaction 
between 
a 
User, 
an 
Intelligent 
Agent, 
and 
the 
Agent's 
Environment 


1.2 
Thesis 
Statement 
In 
this 
thesis, 
I 
present 
and 
evaluate 
Wawa, 
which 
is 
a 
system 
for 
rapidly 
building 
intelligent 
software 
agents 
that 
retrieve 
and 
extract 
information. 
The 
thesis 
of 
this 
dissertation 
is 
as 
follows: 


Theory-renement 
techniques 
are 
quite 
useful 
in 
solving 
information-retrieval 
and 
information-extraction 
tasks. 
Agents 
using 
such 
techniques 
are 
able 
to 
(i) 
produce 
reasonable 
performance 
initially 
(without 
requiring 
that 
the 
knowledge 
provided 
by 
the 
user 
be 
100% 
correct) 
and 
(ii) 
reduce 
the 
burden 
on 
the 
user 
to 
provide 
training 
examples 
(which 
are 
tedious 
to 
obtain 
in 
both 
tasks). 


1.3 
Thesis 
Overview 
This 
thesis 
is 
organized 
as 
follows. 
Chapter 
2 
provides 
the 
necessary 
backg
round 
for 
the 
concepts 
and 
techniques 
used 
in 
Wawa. 
I 
present 
Wawa's 
fund
amental 
operations 
in 
Chapter 
3. 
Wawa's 
information-retrieval 
(Wawa-IR) 
sub-system 
and 
its 
case 
studies 
are 
discussed 
in 
Chapters 
4 
and 
5, 
respectively. 



Wawa's 
information-extraction 
(Wawa-IE) 
sub-system 
along 
with 
its 
experim
ental 
studies 
are 
discussed 
in 
Chapters 
6 
and 
7, 
respectively. 
Related 
work 
is 
presented 
in 
Chapter 
8. 
I 
discuss 
the 
contributions 
of 
this 
thesis, 
Wawa's 
limitations, 
some 
future 
directions, 
and 
concluding 
remarks 
in 
Chapter 
9. 
App
endix 
A 
presents 
Wawa's 
advice 
language 
in 
its 
entirety. 
Appendices 
B, 
C, 
D, 
and 
E 
provide 
the 
advice 
rules 
used 
in 
the 
empirical 
studies 
done 
on 
Wawa-IR 
and 
Wawa-IE. 



Chapter 
2 


Background 


This 
chapter 
provides 
the 
necessary 
background 
for 
the 
concepts 
used 
in 
Wawa. 
These 
concepts 
are 
multi-layer 
feed-forward 
neural 
networks, 
knowledge-based 
neural 
networks, 
reinforcement 
learning, 
information 
retrieval, 
and 
information 
extraction. 
Readers 
who 
are 
familiar 
with 
these 
topics 
may 
wish 
to 
skip 
this 
chapter. 


2.1 
Multi-Layer 
Feed-Forward 
Neural 
Netw
orks 
Multi-layer 
feed-forward 
neural 
networks 
learn 
to 
recognize 
patterns. 
Figure 
3 
shows 
a 
two-layer 
feed-forward 
network.1 


Given 
a 
set 
of 
input 
vectors 
and 
their 
corresponding 
output 
vectors, 
(X,Y 
), 
a 
multi-layer 
feed 
forward 
neural 
network 
can 
be 
trained 
to 
learn 
a 
function, 
f, 
which 
maps 
new 
input 
vectors, 
X0, 
into 
the 
corresponding 
output 
vectors, 
Y 
. 
. 


The 
ability 
to 
capture 
nonlinear 
functions 
makes 
multi-layer 
feed-forward 
neural 
networks 
powerful. 
Activation 
functions 
are 
used 
on 
the 
hidden 
units 
of 
a 
feed-forward 
neural 
network 
to 
introduce 
nonlinearity 
into 
the 
network. 
There 
are 
three 
commonly 
used 
activation 
functions: 
(i) 
the 
step 
function, 
(ii) 
the 
sign 
function, 
and 
(iii) 
the 
sigmoid 
function. 
The 
step 
function 
returns 
1 
if 
the 
weighted 
sum 
of 
its 
inputs 
is 
greater 
than 
or 
equal 
to 
some 
pre-dened 
threshold, 
t; 
otherwise, 
it 
returns 
0. 
The 
sign 
function 
returns 
1 
if 
the 
weighted 


1 
Figure 
3 
was 
adapted 
from 
Rich 
and 
Knight 
(1991), 
page 
502. 



o1 o2 oL 
h1 h2 hMh3 
E 
E 
E 
output units 
hidden units 
input units 
wj®iwk®jx1 x2 x3 x4 xN 
o1 o2 oL 
h1 h2 hMh3 
E 
E 
E 
output units 
hidden units 
input units 
wj®iwk®jx1 x2 x3 x4 xN 
Figure 
3: 
A 
Two-Layer 
Feed-Forward 
Network 


sum 
of 
its 
inputs 
is 
greater 
than 
or 
equal 
to 
zero; 
otherwise, 
it 
returns 
0. 
The 
sigmoid 
function 
returns 
1+e
1 
..ini 
where 
ini 
is 
the 
weighted 
sum 
of 
inputs 
into 
unit 
i 
plus 
the 
pre-dened 
bias 
on 
unit 
i. 
Activation 
functions 
are 
also 
used 
on 
the 
output 
units 
to 
capture 
the 
distribution 
of 
the 
output 
values. 


The 
most 
popular 
method 
of 
training 
multi-layer 
feed-forward 
networks 
is 
called 
backpropagation 
(BP) 
(Rumelhart, 
Hinton, 
and 
Williams 
1986). 
Tab
le 
1 
describes 
the 
BP 
algorithm. 
The 
sigmoid 
function 
is 
a 
popular 
activation 
function 
for 
backpropagation 
learning 
since 
it 
is 
dierentiable. 


A 
good 
stopping 
criteria 
for 
BP 
training 
is 
to 
set 
aside 
some 
of 
the 
examples 
in 
the 
training 
set 
into 
a 
new 
set 
(known 
as 
the 
tuning 
set). 
Whenever 
the 
accuracy 
on 
the 
tuning 
set 
starts 
decreasing, 
we 
are 
overtting 
the 
training 
data 
and 
should 
stop 
training. 


In 
Wawa, 
the 
output 
units 
output 
the 
weighted 
sum 
of 
their 
inputs 
and 
the 
activation 
function 
for 
the 
hidden 
units 
is 
sigmoidal. 



Table 
1: 
The 
Backpropagation 
Algorithm 


Inputs: 
a 
multi-layer 
feed-forward 
network, 
a 
set 
of 
input/output 
pairs 
(known 
as 
the 
training 
set), 
and 
a 
learning 
rate 
() 


Output: 
a 
trained 
multi-layer 
feed-forward 
network 


Algorithm: 


• 
initialize 
all 
weights 
to 
small 
random 
numbers 
• 
repeat 
until 
stopping 
criteria 
has 
been 
met 
– 
for 
each 
example 
hX, 
Y 
. 
in 
the 
training 
set 
do 
the 
following 
. 
compute 
the 
output, 
O, 
for 
this 
example 
by 
instantiating 
X 
into 
the 
input 
units 
and 
doing 
forward-propagation 
. 
compute 
the 
error 
at 
the 
output 
units, 
E 
= 
Y 
. 
O 
. 
update 
the 
weights 
into 
the 
output 
units, 
Wj!i 
= 
Wj!i 
+ 
. 
× 
aj 
× 
Ei 
× 
g0(ini) 
where 
aj 
= 
g(inj) 
is 
the 
activation 
of 
unit 
j, 
g0(ini) 
is 
the 
derivative 
of 
the 
activation 
function 
g, 
and 
ini 
=(j 
Wj!i 
× 
aj)+ 
biasi 


. 
for 
each 
hidden 
layer 
in 
the 
network 
do 
the 
following 
· 
compute 
the 
error 
at 
each 
node, 


j 
= 
g0(inj) 
i 
Wj!i 
× 
i 
where 
i 
= 
Ei 
× 
g0(ini) 


· 
update 
the 
weights 
into 
the 
hidden 
layer, 
Wk!j 
= 
Wk!j 
+ 
. 
× 
Xk 
× 
j 
where 
Xk 
is 
the 
activation 
of 
the 
jth 
unit 
in 
the 
input 
vector 



9 


2.2 
Knowledge-Based 
Articial 
Neural 
Netw
orks 
A 
knowledge-based 
articial 
neural 
network 
(Kbann) 
allows 
a 
user 
to 
input 
prior 
knowledge 
into 
a 
multi-layer 
feed-forward 
neural 
network 
(Towell 
and 
Shavlik 
1994). 
The 
user 
expresses 
her 
prior 
knowledge 
by 
a 
set 
of 
propositional 
rules. 
Then, 
an 
AND-OR 
dependency 
graph 
is 
constructed. 
Each 
node 
in 
the 
AND-OR 
dependency 
graph 
becomes 
a 
unit 
in 
Kbann. 
Additional 
units 
are 
added 
for 
OR 
nodes. 
The 
biases 
of 
each 
AND 
unit 
and 
the 
weights 
coming 
into 
the 
AND 
unit 
are 
set 
such 
that 
the 
unit 
will 
get 
activated 
only 
when 
all 
of 
its 
inputs 
are 
true. 
Similarly, 
the 
biases 
of 
each 
OR 
unit 
and 
the 
weights 
coming 
into 
the 
OR 
unit 
are 
initialized 
such 
that 
the 
unit 
will 
get 
activated 
only 
when 
at 
least 
one 
of 
its 
inputs 
is 
true. 
Links 
with 
low 
weights 
are 
added 
between 
layers 
of 
the 
network 
to 
allow 
learning 
over 
the 
long 
run. 


Figure 
4 
illustrates 
the 
Kbann 
algorithm.2 
In 
part 
(a), 
some 
prior 
knowle
dge 
is 
given 
in 
the 
form 
of 
propositional 
rules. 
Part 
(b) 
shows 
the 
AND-OR 
dependency 
graph 
for 
the 
rules 
given 
in 
part 
(a). 
In 
part 
(c), 
each 
node 
in 
the 
graph 
becomes 
a 
unit 
in 
the 
network 
and 
appropriate 
weights 
are 
added 
to 
the 
links 
to 
capture 
the 
semantics 
of 
each 
rule. 
For 
example, 
the 
weights 
on 
links 
X 
. 
Z 
and 
Y 
. 
Z 
are 
set 
to 
5 
and 
the 
bias3 
of 
the 
unit 
Z 
is 
set 
to 
-6. 
This 
means 
that 
unit 
Z 
will 
be 
true 
if 
and 
only 
if 
both 
units 
X 
and 
Y 
are 
true. 
To 
enable 
future 
learning, 
low-weighted 
links 
are 
added 
to 
the 
network 
in 
part 
(d). 


After 
Kbann 
is 
constructed, 
we 
can 
apply 
the 
backpropagation 
algorithm 
(see 
Section 
2.1) 
to 
learn 
from 
the 
training 
set. 


2 
Figure 
4 
was 
adapted 
from 
Maclin 
(1995), 
page 
17. 


3 
For 
an 
AND 
unit, 
the 
bias 
equals 
5(#unnegated 
antecedents 
. 
0:5). 
For 
an 
OR 
unit, 
the 
bias 
equals 
..5(#negated 
antecedents..0:5). 
Kbann 
uses 
sigmoidal 
activation 
function 
on 
its 
units. 
If 
the 
activation 
function 
outputs 
a 
value 
. 
0.5, 
then 
the 
unit 
is 
activated. 
Otherwise, 
the 
unit 
is 
not 
activated. 




(a) 
If X and Y then Z 
If A and B then X 
(b) 
X 
Z 
Y 
If B and (not C) then X 
If C and D then Y 
A B C D 
(c) Z (d) Z 
X Y X Y 
A B C D A B C D 

Figure 
4: 
An 
Example 
of 
the 
Kbann 
Algorithm. 
Box 
(a) 
contains 
a 
set 
of 
propositional 
rules. 
Box 
(b) 
shows 
an 
AND-OR 
dependency 
graph 
for 
rules 
in 
box 
(a). 
In 
box 
(c), 
each 
node 
in 
the 
AND-OR 
graph 
is 
mapped 
to 
a 
unit. 
The 
weights 
on 
links 
and 
the 
biases 
on 
units 
are 
set 
such 
that 
they 
reect 
either 
an 
AND 
gate 
or 
an 
OR 
gate. 
Additional 
units 
are 
added 
to 
capture 
the 
functionality 
of 
an 
OR 
gate. 
In 
box 
(d), 
links 
with 
low 
weights 
are 
added 
between 
layers 
to 
allow 
future 
learning. 


2.3 
Reinforcement 
Learning 
In 
a 
reinforcement-learning 
(RL) 
paradigm, 
the 
learner 
attempts 
to 
learn 
a 
course 
of 
actions 
which 
maximizes 
the 
total 
rewards 
he/she 
receives 
from 
the 
environment. 
A 
RL 
system 
has 
ve 
major 
elements 
(Sutton 
and 
Barto 
1998):4 


• 
An 
agent 
which 
is 
the 
learner 
in 
the 
RL 
paradigm. 
• 
An 
environment 
which 
is 
a 
collection 
of 
states. 
The 
agent 
is 
able 
to 
4 
Some 
RL 
systems 
have 
an 
optional 
element 
which 
describes 
the 
behavior 
of 
the 
environm
ent. 
This 
element 
is 
called 
the 
model 
of 
the 
environment. 
Given 
a 
state 
and 
an 
action, 
it 
predicts 
the 
next 
state 
and 
the 
immediate 
reward. 
Wawa 
does 
not 
use 
a 
model 
of 
the 
environment, 
since 
it 
is 
a 
daunting 
task 
to 
model 
the 
World-Wide 
Web. 



interact 
with 
the 
environment 
by 
performing 
actions 
which 
take 
the 
agent 
from 
one 
state 
to 
the 
other. 


• 
A 
policy 
which 
species 
a 
mapping 
from 
the 
agent's 
current 
state 
to 
its 
chosen 
action 
(i.e., 
the 
agent's 
behavior 
at 
a 
given 
time). 
• 
A 
reward 
function 
which 
returns 
the 
immediate 
reward 
the 
agent 
receives 
for 
performing 
a 
particular 
action 
at 
a 
particular 
state 
in 
the 
environment. 
• 
A 
value 
function 
which 
denes 
the 
predicted 
goodness 
of 
an 
action 
in 
the 
long 
run. 
Wawa 
uses 
a 
class 
of 
RL 
methods 
called 
temporal-dierence 
learning 
(Sutt
on 
1988). 
In 
particular, 
Wawa 
uses 
a 
method 
called 
Q-learning 
(Watkins 
1989). 
In 
Q-learning, 
a 
system 
tries 
to 
learn 
a 
function 
called 
the 
Q-function. 
This 
function 
takes 
as 
input 
a 
state, 
s, 
and 
an 
action, 
a 
and 
outputs 
the 
sum 
of 
the 
immediate 
reward 
the 
agent 
will 
receive 
for 
performing 
action 
a 
from 
state 
s 
and 
the 
discounted 
value 
of 
the 
optimal 
policy 
taken 
from 
the 
resultant 
state, 
s. 
. 
The 
optimal 
policy 
from 
s. 
is 
the 
action, 
a0, 
which 
maximizes 
the 
Q-function 
with 
inputs 
s. 
and 
a. 
. 
Therefore, 
we 
can 
write 
the 
Q-function 
as: 


Q(s, 
a)= 
r(s, 
a)+ 
. 
. 
maxQ(s0;a0) 


a. 


where 
r(s, 
a) 
is 
the 
immediate 
reward 
for 
taking 
action 
a 
from 
state 
s, 
and 


. 
is 
the 
discounted 
value 
for 
rewards 
in 
the 
future. 
Wawa's 
information 
retrieval 
sub-system 
learns 
a 
modied 
version 
of 
the 
Q-function 
by 
using 
a 
knowledge-based 
neural 
network 
called 
ScoreLink 
(see 
Section 
4.3 
for 
details). 
ScoreLink 
learns 
to 
predict 
which 
links 
to 
follow. 
The 


^

output 
of 
ScoreLink 
is 
Q(s, 
a), 
which 
is 
Wawa's 
estimate 
of 
the 
actual 
Q(s, 
a). 
ScoreLink 
learns 
to 
predict 
the 
value 
of 
a 
link, 
l, 
by 
backpropagating 
on 
the 
dierence 
between 
the 
value 
of 
l 
before 
and 
after 
fetching 
the 
page, 
p, 
to 
which 
it 
points. 



2.4 
Information 
Retrieval 
Information 
retrieval 
(IR) 
systems 
take 
as 
input 
a 
set 
of 
documents 
(a.k.a., 
the 
corpus) 
and 
a 
query 
(i.e., 
a 
set 
of 
keywords). 
They 
return 
documents 
from 
the 
corpus 
that 
are 
relevant 
to 
the 
given 
query. 


Most 
IR 
systems 
preprocess 
documents 
by 
removing 
commonly 
used 
words 
(a.k.a., 
\stop” 
words) 
and 
stemming 
words 
(e.g., 
replacing 
\walked” 
with 
\walk"). 
After 
the 
preprocessing 
phase, 
word-order 
information 
is 
lost 
and 
the 
remaining 
words 
are 
called 
terms. 
Then, 
a 
term 
× 
document 
matrix 
is 
created, 
where 
the 
documents 
are 
the 
rows 
of 
the 
matrix 
and 
the 
terms 
are 
the 
columns 
of 
the 
matrix. 
Table 
2 
depicts 
such 
a 
matrix.5 


Table 
2: 
A 
Term 
× 
Document 
Matrix 


term1 
term2 
. 
. 
. 
termj 
. 
. 
. 
termm 
doc1 
w11 
w21 
::. 
w1j 
::. 
w1m 
doc2 
w21 
w22 
::. 
w2j 
::. 
w2m 


.... 
. 


.... 
. 


.. 
. 
::. 
. 
::. 
. 
doci 
wi1 
wi2 
::. 
wij 
::. 
wim 


.... 
. 


.... 
. 


.. 
. 
::. 
. 
::. 
. 
docn 
wn1 
wn2 
::. 
wnj 
::. 
wnm 



The 
entry 
wij 
is 
commonly 
dened 
as 
follows: 


wij 
= 
TF 
(termj, 
doci) 
× 
log( 
jD| 
)

DF 
(termj 
) 


where 
the 
function 
TF 
(termj, 
doci) 
returns 
the 
number 
of 
times 
termj 
appears 
in 
document 
doci, 
jD| 
is 
the 
number 
of 
documents 
in 
the 
corpus, 
and 
the 
funct
ion 
DF 
(termj) 
returns 
the 
number 
of 
times 
termj 
appears 
in 
all 
the 
docum
ents 
in 
the 
corpus. 
This 
representation 
of 
documents 
is 
known 
as 
the 
vector 
model 
or 
the 
bag-of-words 
representation 
(Salton 
1991) 
and 
the 
weighing 
strate
gy 
is 
known 
as 
T 
F=IDF 
, 
short 
for 
term 
frequency/inverse 
document 
frequency 
(Salton 
and 
Buckley 
1988). 


5 
Table 
2 
was 
adapted 
from 
Rose 
(1994), 
page 
66. 



In 
the 
vector 
model, 
users’ 
queries 
are 
also 
represented 
as 
term 
vectors. 
IR 
systems 
using 
the 
vector 
model 
employ 
similarity 
measures 
to 
nd 
relevant 
documents 
to 
a 
query. 
A 
common 
similarity 
measure 
is 
the 
cosine 
measure, 
where 
the 
relevance 
of 
a 
document 
to 
the 
user's 
query 
is 
measured 
by 
the 
cosine 
of 
the 
angle 
between 
the 
document 
vector 
and 
the 
query 
vector. 
The 
smaller 
the 
cosine, 
the 
more 
relevant 
the 
document 
is 
considered 
to 
be 
to 
the 
user's 
query. 


IR 
systems 
are 
commonly 
evaluated 
by 
two 
measures: 
precision 
and 
recall. 
Recall 
represents 
the 
percentage 
of 
relevant 
documents 
that 
are 
retrieved 
from 
the 
corpus. 
Precision 
represents 
the 
percentage 
of 
relevant 
documents 
that 
are 
in 
the 
retrieved 
documents. 
Their 
denitions 
are: 


jRELEV 
ANT 
. 
RETRIEV 
EDj

P 
recision 
= 


jRETRIEV 
ED| 


jRELEV 
ANT 
. 
RETRIEV 
EDj

Recall 
= 


jRELEV 
ANT 
| 


where 
RELEVANT 
represents 
the 
set 
of 
documents 
in 
our 
corpus 
that 
are 
relev
ant 
to 
a 
particular 
query 
and 
RETRIEVED 
is 
the 
set 
of 
documents 
retrieved 
for 
that 
query. 


An 
IR 
system 
tries 
to 
maximizes 
both 
recall 
and 
precision. 
The 
F. 
-measure 


(2 
+1:0)RecallP 
recision 


combines 
precision 
and 
recall 
and 
is 
dened 
to 
be 
(), 
where 


(2 
P 
recision)+Recall 


. 
. 
[0, 
1]. 
When 
. 
= 
1, 
precision 
and 
recall 
are 
given 
the 
same 
weight 
in 
the 
F-measure. 
The 
F1-measure 
is 
more 
versatile 
than 
either 
precision 
or 
recall 
for 
explaining 
relative 
performance 
of 
dierent 
systems, 
since 
it 
takes 
into 
account 
the 
inherent 
tradeoff 
that 
exists 
between 
precision 
and 
recall. 


Most 
IR 
systems 
categorize 
a 
document 
as 
either 
relevant 
or 
irrelevant. 
Ins
tead 
of 
taking 
this 
black 
or 
white 
view 
of 
the 
world, 
Wawa 
learns 
to 
rate 
Web 
documents 
on 
a 
scale 
of 
..10:0 
to 
10.0. 
This 
numeric 
scale 
is 
then 
mapped 
into 
ve 
categories 
(perfect, 
good, 
okay, 
indierent, 
and 
terrible) 
in 
order 
to 
simplify 
the 
task 
of 
labeling 
(Web) 
pages 
for 
users. 
Table 
3 
illustrates 
this 
mapping. 



Table 
3: 
Mapping 
between 
Categories 
Assigned 
to 
Web 
Pages 
and 
Their 
Num
eric 
Scores 


Categories 
for 
Web 
Pages 
Scores 
of 
Web 
Pages 


Perfect 
( 
9:0, 
10:0] 
Good 
(5:0, 
9:0] 
Okay 
(0:0, 
5:0] 
Indierent 
( 
..3:0, 
0:0] 
Terrible 
[..10:0, 
..3:0] 



2.5 
Information 
Extraction 
Information 
extraction 
(IE) 
systems 
are 
typically 
given 
a 
set 
of 
documents 
and 
a 
set 
of 
slots 
in 
a 
pre-dened 
template. 
They 
are 
supposed 
to 
extract 
phrases 
that 
accurately 
ll 
the 
slots 
in 
the 
given 
template 
(such 
phrases 
are 
refered 
to 
as 
\slot 
llers"). 
To 
accomplish 
this 
task, 
some 
IE 
systems 
attempt 
to 
learn 
the 
extraction 
patterns 
from 
training 
documents. 
These 
patterns 
(a.k.a., 
rules) 
are 
then 
applied 
to 
future 
documents 
to 
extract 
some 
assertions 
about 
those 
documents. 
For 
example, 
the 
following 
pattern 
extracts 
dates 
like 
\Sunday, 
August 
1, 
1999", 
\Sun, 
Aug 
1, 
99", 
and 
\Sunday, 
Aug 
1, 
1999", 
etc: 


\Day 
[ 
]* 
, 
[ 
]* 
Month 
[ 
]* 
DayNum 
[ 
]* 
, 
[ 
]* 
Year” 


where 
[ 
]* 
= 
0 
or 
more 
spaces, 
Day 
= 
[Monday 
| 
::. 
| 
Sunday 
| 
Mon 
| 
::. 
| 
Sun], 
Month 
= 
[January 
| 
::. 
| 
December 
| 
Jan 
| 
::. 
| 
Dec 
], 
DayNum 
= 
[1 
| 
2 
| 
::. 
| 
30 
| 
31], 
Year 
=[Digit 
Digit 
Digit 
Digit 
| 
Digit 
Digit], 
and 
Digit 
= 
[0 
| 
1 
| 
::. 
| 
9]. 
The 
notation 
N 
=[A 
| 
B 
| 
· 
] 
lists 
the 
possible 
values 
for 
variable 


N. 
For 
example, 
SEX 
= 
[Male 
| 
Female]. 
The 
documents 
given 
to 
an 
IE 
system 
can 
have 
dierent 
text 
styles 
(Soderl
and 
1999). 
They 
can 
be 
either 
structured 
(e.g., 
pages 
returned 
by 
a 
yellow-
pages 
telephone 
directory), 
semi-structured 
(e.g., 
seminar 
announcements), 
or 
free 
(e.g., 
news 
articles). 


There 
are 
two 
types 
of 
IE 
systems. 
The 
rst 
type 
is 
individual-slot 
(or 
single-
slot) 
systems, 
which 
produce 
a 
single 
lled 
template 
for 
each 
document. 
The 



second 
type 
is 
combination-slots 
(or 
multi-slot) 
systems, 
which 
produce 
more 
than 
one 
lled 
template 
for 
each 
document. 
The 
multi-slot 
extraction 
problem 
is 
harder 
than 
the 
single-slot 
extraction 
problem 
since 
the 
slot 
llers 
in 
the 
template 
depend 
on 
each 
other. 
For 
example, 
suppose 
the 
IE 
task 
is 
to 
extract 
proteins 
and 
their 
locations 
in 
the 
cell 
from 
a 
set 
of 
biological 
documents, 
where 
each 
document 
contains 
multiple 
instances 
of 
proteins 
and 
their 
locations 
in 
the 
cell. 
In 
this 
problem, 
the 
IE 
system 
must 
be 
able 
to 
match 
each 
protein 
with 
its 
location 
in 
the 
cell. 
Giving 
the 
user 
a 
list 
of 
proteins 
and 
a 
separate 
list 
of 
locations 
is 
useless. 


The 
input 
requirements 
and 
the 
syntactic 
structure 
of 
the 
learned 
patterns 
vary 
substantially 
from 
one 
IE 
system 
to 
the 
next. 
See 
Section 
8.3 
for 
a 
discuss
ion 
of 
dierent 
IE 
systems. 



Chapter 
3 


WAWA's 
Core 


This 
chapter 
presents 
Wawa's 
fundamental 
operations,1 
which 
are 
used 
in 
both 
the 
IR 
and 
the 
IE 
sub-systems 
of 
Wawa 
(see 
Chapters 
4 
and 
6 
for 
details 
on 
the 
IR 
and 
the 
IE 
sub-systems, 
respectively). 
These 
operations 
include 
extracting 
features 
from 
Web 
pages,2 
handling 
of 
Wawa's 
advice 
language, 
and 
scoring 
arbitrarily 
long 
pages 
with 
neural 
networks. 
Figure 
5 
illustrates 
how 
an 
agent 
uses 
these 
operations 
to 
score 
a 
page. 
The 
page 
processor 
gets 
a 
page 
from 
the 
environment 
(e.g., 
the 
Web) 
and 
produces 
an 
internal 
representation 
of 
the 
page 
(by 
extracting 
features 
from 
it). 
This 
new 
representation 
of 
the 
page 
is 
then 
given 
to 
the 
agent's 
knowledge 
base 
(i.e., 
the 
agent's 
neural 
network), 
which 
produces 
a 
score 
for 
the 
page 
by 
doing 
forward-propagation 
(Rumelhart, 
Hinton, 
and 
Williams 
1986). 
Finally, 
the 
agent's 
neural 
network 
incorporates 
the 
user's 
advice 
and 
the 
environment's 
feedback, 
both 
of 
which 
aect 
the 
score 
of 
a 
page. 


The 
knowledge 
base 
of 
a 
Wawa 
agent 
is 
centered 
around 
two 
basic 
functions: 
ScoreLink 
and 
ScorePage 
(see 
Figure 
6). 
If 
given 
highly 
accurate 
instances 
of 
such 
functions, 
standard 
heuristic 
search 
would 
lead 
to 
eective 
retrieval 
of 
text 
documents: 
the 
best-scoring 
links 
would 
be 
traversed 
and 
the 
highest-
scoring 
pages 
would 
be 
collected. 


Users 
are 
able 
to 
tailor 
an 
agent's 
behavior 
by 
providing 
advice 
about 
the 


1 
Portions 
of 
this 
chapter 
were 
previously 
published 
in 
Shavlik 
and 
Eliassi-Rad 
(1998a, 
1998b), 
Shavlik 
et 
al. 
(1999), 
and 
Eliassi-Rad 
and 
Shavlik 
(2001a). 


2 
For 
simplicity, 
the 
terms 
\Web 
page” 
and 
\document” 
are 
used 
interchangeably 
in 
this 
thesis. 



Score,
of the,
Page,
Internal,
Representation,
of the Page,
Page Processor,
(i.e., Parsing, Tagging, etc),
WAWA.sAgentEnvironmentInteraction,
via Advice &,
Training Examples,
UserWeb,
Page,
1342Interaction via,
Environmental,
Feedback,
Score,
of the,
Page,
Internal,
Representation,
of the Page,
Page Processor,
(i.e., Parsing, Tagging, etc),
WAWA.sAgentEnvironmentInteraction,
via Advice &,
Training Examples,
UserWeb,
Page,
1342Interaction via,
Environmental,
Feedback,
Figure 
5: 
Scoring 
a 
Page 
with 
a 
WAWA 
Agent 


..c 


LinkScore?
PageScore?
Figure 
6: 
Central 
Functions 
of 
Wawa's 
Agents 
Score 
Web 
Pages 
and 
Hyperlinks 


above 
functions. 
This 
advice 
is 
\compiled” 
into 
two 
\knowledge 
based” 
neur
al 
networks 
(see 
Section 
3.3) 
implementing 
the 
functions 
ScoreLink 
and 
ScorePage 
(see 
Figure 
7). 
These 
functions, 
respectively, 
guide 
the 
agent's 
wandering 
within 
the 
Web 
and 
judge 
the 
value 
of 
the 
pages 
encountered. 
Subs
equent 
reinforcements 
from 
the 
Web 
(e.g., 
encountering 
dead 
links) 
and 
any 
ratings 
of 
retrieved 
pages 
that 
the 
user 
wishes 
to 
provide 
are, 
respectively, 
used 
to 
rene 
the 
link-and 
page-scoring 
functions. 


A 
Wawa 
agent's 
ScorePage 
network 
is 
a 
supervised 
learner 
(Mitchell 
1997). 
That 
is, 
it 
learns 
through 
user-provided 
training 
examples 
and 
advice. 
A 
Wawa 
agent's 
ScoreLink 
network 
is 
a 
reinforcement 
learner 
(Sutton 
and 
Barto 
1998). 
This 
network 
automatically 
creates 
its 
own 
training 
examples, 
though 
it 
can 
also 
use 
any 
user-provided 
training 
examples 
and 
advice. 
Hence, 



* About this department.
* People.
* Academic Information.
* Contact Information.
http://www.cs.wisc.edu.
UW CS Home Page.
ScorePageo* About this department.
http://www.cs.wisc.edu/about.html.
About this department.
* People.
* Academic Information.
* Contact Information.
http://www.cs.wisc.edu.
UW CS Home Page.
ScorePageo* About this department.
http://www.cs.wisc.edu/about.html.
r in [-10.0, 10.0]. 


ScoreLinko
r in [-25.0, 25.0]. 



Figure 
7: 
Wawa's 
Central 
Functions 
Score 
Web 
Pages 
and 
Hyperlinks 


Wawa's 
design 
of 
the 
ScoreLink 
network 
has 
the 
important 
advantage 
of 
prod
ucing 
self-tuning 
agents 
since 
training 
examples 
are 
created 
by 
the 
agent 
itself 
(see 
Section 
4.3). 


3.1 
Input 
Features 
Wawa 
extracts 
features 
from 
either 
HTML 
or 
plain-text 
Web 
pages. 
In 
addition 
to 
representing 
Wawa's 
input 
units, 
these 
input 
features 
constitute 
the 
primi
tives 
in 
its 
advice 
language. 
This 
section 
presents 
Wawa's 
feature-extraction 
method. 


A 
standard 
representation 
of 
text 
used 
in 
IR 
is 
the 
bag-of-words 
representat
ion 
(Salton 
1991). 
In 
the 
bag-of-words 
representation, 
word 
order 
is 
lost 
and 
all 
that 
is 
used 
is 
a 
vector 
that 
records 
the 
words 
on 
the 
page 
(usually 
scaled 
according 
to 
the 
number 
of 
occurrences 
and 
other 
properties; 
see 
Section 
2.4). 
The 
top-right 
part 
of 
Figure 
8 
illustrates 
this 
representation. 


Generally, 
IR 
systems 
(Belew 
2000) 
reduce 
the 
dimensionality 
(i.e., 
numb
er 
of 
possible 
features) 
in 
the 
problem 
by 
discarding 
common 
(\stop") 
words 
and 
\stemming” 
all 
words 
to 
their 
root 
form 
(e.g., 
\walked” 
becomes 
\walk"). 
Wawa 
implements 
these 
two 
preprocessing 
steps 
by 
using 
a 
generic 
list 
of 
stop 
words 
and 
Porter's 
stemmer 
(1980). 
In 
particular, 
I 
use 
the 
popular 
Frakes 
and 
Cox's 
implementation 
of 
Porter's 
stemmer 
(Frakes 
and 
Baeza-Yates 
1992). 



LocalizedBags-of-Words 
Standard 
Approach 
spacerentpagesample 
com 
wwwBag-of-Words 
Aspects of Our 
pagesample 
words in title 
words in 
window 
spacerent 
words in 
URL 
com 
page 
wwwSliding 
Window 
URL: www.page.com 
Title: Sample Page 
space rent 
Original Web Page 
URL: www.page.com 
Title: A Sample Page 
This space 
for rent. 
Stop Words 
Removal & 
Stemming 
LocalizedBags-of-Words 
Standard 
Approach 
spacerentpagesample 
com 
wwwBag-of-Words 
Aspects of Our 
pagesample 
words in title 
words in 
window 
spacerent 
words in 
URL 
com 
page 
wwwSliding 
Window 
URL: www.page.com 
Title: Sample Page 
space rent 
Original Web Page 
URL: www.page.com 
Title: A Sample Page 
This space 
for rent. 
Stop Words 
Removal & 
Stemming 
Representation 

Figure 
8: 
Internal 
Representation 
of 
Web 
Pages 


The 
information 
provided 
by 
word 
order 
is 
usually 
important. 
For 
examp
le, 
without 
word-order 
information, 
instructions 
such 
as 
nd 
pages 
containing 
the 
phrase 
\Green 
Bay 
Packers” 
cannot 
be 
expressed. 
Given 
that 
Wawa 
uses 
neural 
networks 
to 
score 
pages 
and 
links, 
one 
approach 
to 
capturing 
word-
order 
information 
would 
be 
to 
use 
recurrent 
networks 
(Elman 
1991). 
Howe
ver, 
Wawa 
borrows 
an 
idea 
from 
NETtalk 
(Sejnowski 
and 
Rosenberg 
1987), 
though 
Wawa's 
basic 
unit 
is 
a 
word 
rather 
than 
an 
(alphabetic) 
letter 
as 
in 
NETtalk. 
Namely, 
Wawa 
\reads” 
a 
page 
by 
sliding 
a 
xed-size 
window 
across 
a 
page 
one 
word 
at 
a 
time. 
Typically, 
the 
sliding 
window 
contains 
15 
words. 
Figure 
9 
provides 
an 
example 
of 
a 
three-word 
sliding 
window 
going 
across 
a 
page. 


Most 
of 
the 
features 
Wawa 
uses 
to 
represent 
a 
page 
are 
dened 
with 
respect 
to 
the 
current 
center 
of 
the 
sliding 
window. 
The 
sliding 
window 
itself 
captures 
word 
order 
on 
a 
page; 
however, 
Wawa 
also 
maintains 
two 
bags 
of 
words 
(each 
of 
size 
10) 
that 
surround 
the 
sliding 
window. 
These 
neighboring 
bags 
allows 
Wawa 
to 
capture 
instructions 
such 
as 
nd 
pages 
where 
\Green 
Bay” 
is 
near 
\Packers". 



Green Bay Packers playedChicago Bears yesterday.B...BGreen Bay Packers playedChicago Bears yesterday.B...BSample PageBDifferent Positions of a 
Green B 
yesterday Bears Chicago 
Bears Chicago playedB 
Chicago played PackersB 
BayB PackersB played 
GreenB BayB Packers 
GreenB Bay BThree-Word Sliding WindowB

Step 1e B

Step 2e

Step 3e

Step 4e

Step 5e

Step 6e

Step 7e

Figure 
9: 
Using 
a 
Three-Word 
Sliding 
Window 
to 
Capture 
Word-Order 
Inform
ation 
on 
a 
Page 


In 
addition 
to 
preserving 
some 
word-order 
information, 
Wawa 
also 
takes 
advantage 
of 
the 
structure 
of 
HTML 
documents 
(when 
a 
fetched 
page 
is 
so 
form
atted). 
First, 
it 
augments 
the 
bag-of-words 
model, 
by 
using 
several 
localized 
bags, 
some 
of 
which 
are 
illustrated 
on 
the 
bottom-right 
part 
of 
Figure 
8. 
Bes
ides 
a 
bag 
for 
all 
the 
words 
on 
the 
page, 
Wawa 
has 
word 
bags 
for: 
the 
title, 
the 
url 
of 
a 
page, 
the 
sliding 
window, 
the 
left 
and 
right 
sides 
of 
the 
sliding 
window, 
the 
current 
hyperlink3 
(should 
the 
window 
be 
inside 
hypertext), 
and 
the 
current 
section's 
title. 
Wawa's 
parser 
of 
Web 
pages 
records 
the 
\parent” 
section 
title 
of 
each 
word; 
parents 
of 
words 
are 
indicated 
by 
the 
standard 
hH1. 
through 
hH6. 
section-header 
constructs 
of 
HTML, 
as 
well 
as 
other 
indicators 
such 
as 
table 
captions 
and 
table-column 
headings. 
Moreover, 
bags 
for 
the 
words 
in 
the 
grandparent 
and 
great-grandparent 
sections 
are 
kept, 
should 
the 
current 
window 
be 
nested 
that 
deeply. 


Wawa 
uses 
Brill's 
tagger 
(1994) 
to 
annotate 
each 
word 
on 
a 
page 
with 
a 
part-of-speech 
(POS) 
tag 
(i.e., 
noun, 
proper 
noun, 
verb, 
etc). 
This 
information 
is 
represented 
in 
the 
agent's 
neural 
networks 
as 
input 
features 
for 
the 
words 
in 
the 
sliding 
window. 
By 
adding 
POS 
tags, 
Wawa 
is 
able 
to 
distinguish 
between 


3 
A 
Web 
page 
has 
a 
set 
of 
URLs 
that 
point 
to 
it 
and 
a 
set 
of 
URLs 
within 
its 
contents. 
In 
this 
thesis, 
I 
refer 
to 
the 
former 
as 
URL 
and 
the 
later 
cases 
as 
hyperlinks, 
in 
an 
attempt 
to 
reduce 
confusion. 



dierent 
grammatical 
uses 
of 
a 
word. 
For 
example, 
this 
allows 
a 
user 
to 
express 
instructions 
such 
as 
nd 
pages 
where 
the 
words 
\y” 
and 
\bug” 
are 
nouns. 
Wawa 
also 
takes 
advantage 
of 
the 
inherent 
hierarchy 
in 
the 
POS 
tags 
(e.g.,a 
proper 
noun 
is 
also 
a 
noun, 
or 
a 
present 
participle 
verb 
is 
also 
a 
verb). 
For 
example, 
if 
the 
user 
indicates 
interest 
in 
the 
word 
\y” 
as 
a 
noun, 
Wawa 
looks 
for 
the 
presence 
of 
\y” 
as 
a 
noun 
and 
as 
a 
proper 
noun. 
However, 
if 
the 
user 
indicates 
interest 
in 
the 
word 
\Bill” 
as 
a 
proper 
noun, 
then 
Wawa 
only 
looks 
for 
the 
presence 
of 
\Bill” 
as 
a 
proper 
noun 
and 
not 
as 
a 
noun. 


Table 
4 
lists 
some 
of 
Wawa's 
extracted 
input 
features 
(see 
Appendix 
A 
for 
a 
full 
list 
of 
Wawa's 
input 
features). 
The 
features 
anywhereOnPage(hwordi) 
and 
anywhereInTitle(hwordi) 
take 
a 
word 
as 
input 
and 
return 
true 
if 
the 
word 
was 
on 
the 
page 
or 
inside 
the 
title 
of 
the 
page, 
respectively. 
These 
two 
features 
represent 
the 
word 
bags 
for 
the 
page 
and 
the 
title. 


In 
addition 
to 
the 
features 
representing 
bag-of-words 
and 
word 
order, 
Wawa 
also 
represents 
several 
xed 
positions. 
Besides 
the 
obvious 
case 
of 
the 
positions 
in 
the 
sliding 
window, 
Wawa 
represents 
the 
rst 
and 
last 
N 
words 
(for 
some 
xed 
N 
provided 
by 
the 
user) 
in 
the 
title, 
the 
url, 
the 
section 
titles, 
etc. 
Since 
urls 
and 
hyperlinks 
play 
an 
important 
role 
in 
the 
Web, 
Wawa 
captures 
the 
last 
N 
elds 
(i.e., 
delimited 
by 
dots) 
in 
the 
server 
portion 
of 
urls 
and 
hyperlinks, 


e.g. 
www 
wisc 
edu 
in 
http://www.wisc.edu/news.html. 
Besides 
the 
input 
features 
related 
to 
words 
and 
their 
positions 
on 
the 
page, 
a 
Wawa 
agent's 
input 
vector 
also 
includes 
various 
other 
features, 
such 
as 
the 
length 
of 
the 
page, 
the 
date 
the 
page 
was 
created 
or 
modied 
(should 
the 
page's 
server 
provide 
that 
information), 
whether 
the 
window 
is 
inside 
emphasized 
HTML 
text, 
the 
sizes 
of 
the 
various 
word 
bags, 
how 
many 
words 
mentioned 
in 
advice 
are 
present 
in 
the 
various 
bags, 
etc. 


Features 
describing 
POS 
tags 
for 
the 
words 
in 
the 
sliding 
window 
are 
repr
esented 
by 
the 
last 
three 
features 
in 
Table 
4. 
For 
example, 
the 
input 
feature 
POSatCenterOfWindow(noun) 
is 
true 
only 
when 
the 
current 
word 
at 
the 
center 



Table 
4: 
Sample 
Extracted 
Input 
Features 


anywhereOnPage(hwordi) 
anywhereInTitle(hwordi) 
· 
isNthWordInTitle(hNi, 
hwordi) 
· 
isNthWordFromENDofTitle(hNi, 
hwordi) 
· 
NthFromENDofURLhostname(hNi, 
hwordi) 
· 
leftNwordInWindow(hNi, 
hwordi) 
centerWordInWindow(hwordi) 
· 


numberOfWordsInTitle() 
numberOfAdviceWordsInTitle() 


· 


insideEmphasizedText() 
timePageWasLastModified() 


· 
POSatRightSpotInWindow(hNi, 
hP 
OS 
tagi) 
POSatCenterOfWindow(hP 
OS 
tagi) 
POSatLeftSpotInWindow(hNi, 
hP 
OS 
tagi) 


of 
the 
sliding 
window 
is 
tagged 
as 
a 
noun. 
The 
features 
POSatRightSpotIn-
Window 
and 
POSatLeftSpotInWindow 
specify 
the 
desired 
POS 
tag 
for 
the 
N 
th 
position 
to 
the 
right 
or 
left 
of 
the 
center 
of 
the 
sliding 
window, 
respectively. 


Wawa's 
design4 
leads 
to 
a 
large 
number 
of 
input 
features 
which 
allows 
it 
to 
have 
an 
expressive 
advice 
language. 
For 
example, 
Wawa 
uses 
many 
Boolean-valued 
features 
to 
represent 
a 
Web 
page, 
ranging 
from 
anywhereOnP
age(aardvark) 
to 
anywhereOnPage(zebra) 
to 
rightNwordInWindow(3, 
AAAI) 
to 
NthFromENDofURLhostname(1, 
edu). 
Assuming 
a 
typical 
vocabulary 
of 
tens 
of 
thousands 
of 
words, 
the 
number 
of 
input 
features 
is 
on 
the 
order 
of 
a 


4 
The 
current 
version 
of 
Wawa 
does 
not 
use 
any 
tf/idf 
methods 
(see 
Section 
2.4), 
due 
to 
the 
manner 
in 
which 
it 
compiles 
advice 
into 
networks 
(see 
Section 
3.3). 



million! 


One 
might 
ask 
how 
a 
learning 
system 
could 
hope 
to 
do 
well 
in 
such 
a 
large 
space 
of 
input 
features. 
Fortunately, 
Wawa's 
use 
of 
advice 
means 
that 
users 
indirectly 
select 
a 
subset 
of 
this 
huge 
set 
of 
implicit 
input 
features. 
Namely, 
they 
indirectly 
select 
only 
those 
features 
that 
involve 
the 
words 
appearing 
in 
their 
advice. 
The 
full 
set 
of 
input 
features 
is 
still 
there, 
but 
the 
weights 
out 
of 
input 
features 
used 
in 
advice 
have 
high 
values, 
while 
all 
other 
weights 
(i.e., 
unmentioned 
words 
and 
positions) 
have 
values 
near 
zero 
(see 
Section 
2.2. 
Thus, 
there 
is 
the 
potential 
for 
words 
not 
mentioned 
in 
advice 
to 
impact 
a 
network's 
output, 
after 
lots 
of 
training. 


Wawa 
also 
deals 
with 
the 
enormous 
input 
space 
by 
explicitly 
representing 
only 
what 
is 
on 
a 
page. 
That 
is, 
all 
zero-valued 
features, 
such 
as 
anywhereOnP
age(aardvark) 
= 
false, 
are 
only 
implicitly 
represented. 
Fortunately, 
the 
nature 
of 
weighted 
sums 
in 
both 
the 
forward 
and 
backward 
propagation 
phases 
of 
neur
al 
networks 
(Rumelhart, 
Hinton, 
and 
Williams 
1986) 
means 
that 
zero-valued 
nodes 
have 
no 
impact 
and 
hence 
can 
be 
ignored. 


3.2 
Advice 
Language 
The 
user-provided 
instructions 
are 
mapped 
into 
the 
ScorePage 
and 
Score-
Link 
networks 
using 
a 
Web-based 
language 
called 
advice. 
An 
expression 
in 
Wawa's 
advice 
language 
is 
an 
instruction 
of 
the 
following 
basic 
form: 


when 
condition 
then 
action 


The 
conditions 
represent 
aspects 
of 
the 
contents 
and 
structure 
of 
Web 
pages. 
Wawa's 
extracted 
input 
features 
(as 
described 
in 
Section 
3.1) 
constitute 
the 
primitives 
used 
in 
the 
conditions 
of 
advice 
rules. 
These 
primitive 
constructs 
can 
be 
combined 
to 
create 
more 
complicated 
constructs. 
Table 
5 
lists 
the 
actions 
of 
Wawa's 
advice 
language 
in 
BNF, 
short 
for 
Backus-Naur 
Form, 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
notation. 
The 
strength 
levels 
in 
actions 
represent 
the 
degree 



to 
which 
the 
user 
wants 
to 
increase 
or 
decrease 
the 
score 
of 
a 
page 
or 
a 
link. 
Appendix 
A 
presents 
the 
advice 
language 
in 
its 
entirety. 


Table 
5: 
Permissible 
Actions 
in 
an 
Advice 
Statement 



action 
. 
strength 
show 
page 


| 
strength 
avoid 
showing 
page 


| 
strength 
follow 
link 


| 
strength 
avoid 
following 
link 


| 
strength 
show 
page 
& 
follow 
link 


| 
strength 
avoid 
showing 
page 
& 
following 
link 
strength 
. 
weakly 
| 
moderately 
| 
strongly 
| 
denitely 



3.2.1 
Complex 
Advice 
Constructs 
and 
Predicates 
All 
features 
extracted 
from 
a 
page 
or 
a 
link 
constitute 
the 
basic 
constructs 
and 
predicates 
of 
the 
advice 
language.5 
These 
basic 
constructs 
and 
predicates 
can 
be 
combined 
via 
Boolean 
operators 
(i.e., 
AND, 
OR, 
NOT) 
to 
create 
complex 
predicates. 


Phrases 
(Croft, 
Turtle, 
and 
Lewis 
1991), 
which 
specify 
desired 
properties 
of 
consecutive 
words, 
play 
a 
central 
role 
in 
creating 
more 
complex 
predicates 
out 
of 
the 
primitive 
features 
that 
Wawa 
extracts 
from 
Web 
pages. 
Table 
6 
contains 
some 
of 
the 
more 
complicated 
predicates 
that 
Wawa 
denes 
in 
terms 
of 
the 
basic 
input 
features. 
The 
advice 
rules 
in 
this 
table 
correspond 
to 
instructions 
a 
user 
might 
provide 
if 
she 
is 
interested 
in 
nding 
Joe 
Smith's 
home-page.6 


Rule 
1 
indicates 
that 
when 
the 
system 
is 
sliding 
the 
window 
across 
the 
title 
of 
a 
page, 
it 
should 
look 
for 
any 
of 
the 
plausible 
variants 
of 
Joe 
Smith's 
rst 
name, 
followed 
by 
his 
last 
name, 
apostrophe 
s, 
and 
the 
phrase 
\home 
page.” 


5 
A 
predicate 
is 
a 
function 
that 
returns 
either 
true 
or 
false. 
I 
dene 
a 
construct 
as 
a 
function 
that 
returns 
numeric 
values. 


6 
The 
anyOf 
() 
construct 
used 
in 
the 
table 
is 
satised 
when 
any 
of 
the 
listed 
words 
is 
present. 



Table 
6: 
Sample 
Advice 



(1) 
WHEN 
consecutiveInTitle( 
anyOf(Joseph 
Joe 
J.) 
Smith's 
home 
page) 
STRONGLY 
SUGGEST 
SHOWING 
PAGE 


(2) 
WHEN 
hyperlinkEndsWith( 
anyOf(Joseph 
Joe 
Smith 
jsmith) 
/ 
anyOf(Joseph 
Joe 
Smith 
jsmith 
index 
home 
homepage 
my 
me) 
anyOf(htm 
html 
/ 
)) 
STRONGLY 
SUGGEST 
FOLLOWING 
LINK 


(3) 
WHEN 
(titleStartsWith(Joseph 
Joe 
J.) 
and 
titleEndsWith(Smith)) 
SUGGEST 
SHOWING 
PAGE 


(4) 
WHEN 
NOT(anywhereOnPage(Smith)) 
STRONGLY 
SUGGEST 
AVOID 
SHOWING 
PAGE 
Rule 
2 
demonstrates 
another 
useful 
piece 
of 
advice 
for 
home-page 
ndi
ng. 
This 
one 
gets 
compiled 
into 
the 
NthFromENDofHyperlink() 
input 
features, 
which 
are 
true 
when 
the 
specied 
word 
is 
the 
Nth 
one 
from 
the 
end 
of 
the 
current 
hyperlink. 
(Note 
that 
Wawa 
treats 
the 
'/’ 
in 
urls 
as 
a 
separate 
word.) 


Rule 
3 
depicts 
an 
interest 
in 
pages 
that 
have 
titles 
starting 
with 
any 
of 
the 
plausible 
variants 
of 
Joe 
Smith's 
rst 
name 
and 
ending 
with 
his 
last 
name. 


Rule 
4 
shows 
that 
advice 
can 
also 
specify 
when 
not 
to 
follow 
a 
link 
or 
show 
a 
page; 
negations 
and 
avoid 
instructions 
become 
negative 
weights 
in 
the 
neural 
networks. 


3.2.2 
Advice 
Variables 
Wawa's 
advice 
language 
contains 
variables, 
which 
by 
denition 
range 
over 
vario
us 
kinds 
of 
concepts, 
like 
names, 
places, 
etc. 
Advice 
variables 
are 
of 
particular 
relevance 
to 
Wawa's 
IE 
system 
(see 
Section 
6 
for 
details). 


To 
understand 
how 
variables 
are 
used 
in 
Wawa, 
assume 
that 
a 
user 
wishes 



26 


to 
utilize 
the 
system 
to 
create 
a 
home-page 
nder. 
She 
might 
wish 
to 
give 
such 
a 
system 
some 
(very 
good) 
advice 
like: 


when 
consecutiveInTitle(?FirstName 
?LastName 
's 
Home 
Page) 
then 
show 
page 


The 
leading 
question 
marks 
(?) 
indicate 
variables 
that 
are 
bound 
upon 
receiving 
a 
request 
to 
nd 
a 
specic 
person's 
home 
page. 
The 
use 
of 
variables 
allows 
the 
same 
advice 
to 
be 
applied 
to 
the 
task 
of 
nding 
the 
home 
pages 
of 
any 
number 
of 
dierent 
people. 
The 
next 
section 
further 
explains 
how 
variables 
are 
implemented 
in 
Wawa. 


3.3 
Compilation 
of 
Advice 
into 
Neural 
Netw
orks 
Advice 
is 
compiled 
into 
the 
ScorePage 
and 
ScoreLink 
networks 
using 
a 
varia
nt 
of 
the 
Kbann 
algorithm 
(Towell 
and 
Shavlik 
1994). 
The 
mapping 
process 
(see 
Section 
2.2) 
is 
analogous 
to 
compiling 
a 
traditional 
program 
into 
machine 
code, 
but 
Wawa 
instead 
compiles 
advice 
rules 
into 
an 
intermediate 
language 
expressed 
using 
neural 
networks. 
This 
provides 
the 
important 
advantage 
that 
Wawa's 
\machine 
code” 
can 
automatically 
be 
rened 
based 
on 
feedback 
prov
ided 
by 
either 
the 
user 
or 
the 
Web. 
Namely, 
Wawa 
can 
apply 
the 
backpropa
gation 
algorithm 
(Rumelhart, 
Hinton, 
and 
Williams 
1986) 
to 
learn 
from 
the 
training 
set. 


I 
will 
illustrate 
the 
mapping 
of 
an 
advice 
rule 
with 
variables 
through 
an 
example. 
Suppose 
Wawa 
is 
given 
the 
following 
advice 
rule: 


when 
consecutive( 
Professor 
?FirstName 
?LastName 
) 
then 
show 
page 


During 
advice 
compilation, 
Wawa 
maps 
the 
phrase 
by 
centering 
it 
over 
the 
sliding 
window 
(Figure 
10). 
In 
this 
example, 
the 
phrase 
is 
a 
sequence 
of 
three 



words, 
so 
it 
maps 
to 
three 
positions 
in 
the 
input 
units 
corresponding 
to 
the 
sliding 
window 
(with 
the 
variable 
?FirstName 
associated 
with 
the 
center 
of 
the 
sliding 
window). 


...oIs it true that the wordoat Left1inWindow iso 
.Professor.?oIs it true that the wordoat CenterInWindow isobound to ?FirstName?oIs it true that the wordoat Right1inWindow isobound to ?LastName?o...o2.5o5o5o5oScorePageBiaso 
=o12.5o
Figure 
10: 
Mapping 
Advice 
into 
ScorePage 
Network 


The 
variables 
in 
the 
input 
units 
are 
bound 
outside 
of 
the 
network 
and 
the 
units 
are 
turned 
on 
only 
if 
there 
is 
a 
match 
between 
the 
bindings 
and 
the 
words 
in 
the 
current 
position 
of 
the 
sliding 
window. 
Assume 
the 
bindings 
are: 


?FirstName 
. 
\Joe” 
?LastName 
. 
\Smith” 


Then, 
the 
input 
unit 
\Is 
it 
true 
that 
the 
word 
CenterInWindow 
is 
bound 
to 
?FirstName?” 
will 
be 
true 
(i.e., 
set 
to 
1) 
only 
if 
the 
current 
word 
in 
the 
center 
of 
the 
window 
is 
\Joe.” 
Similarly 
the 
input 
unit 
\Is 
it 
true 
that 
the 
Right1inWindow 
is 
bound 
to 
?LastName?” 
will 
be 
set 
to 
1 
only 
if 
the 
current 
word 
immediately 
to 
the 
right 
of 
the 
center 
of 
the 
window 
is 
\Smith.” 


Wawa 
then 
connects 
the 
referenced 
input 
units 
to 
a 
newly 
created 
hidden 
unit, 
using 
weights 
of 
value 
5. 
Next, 
the 
bias 
(i.e., 
the 
threshold) 
of 
the 
new 



28 


hidden 
unit 
is 
set 
such 
that 
all 
the 
required 
predicates 
must 
be 
true 
in 
order 
for 
the 
weighted 
sum 
of 
its 
inputs 
to 
exceed 
the 
bias 
and 
produce 
an 
activation 
of 
the 
sigmoidal 
hidden 
unit 
near 
1. 
Some 
additional 
zero-weighted 
links 
are 
also 
added 
to 
this 
new 
hidden 
unit, 
to 
further 
allow 
subsequent 
learning, 
as 
is 
standard 
in 
Kbann. 


Finally, 
Wawa 
links 
the 
hidden 
unit 
into 
the 
output 
unit 
with 
a 
weight 
determined 
by 
the 
strength 
given 
in 
the 
rule's 
action. 
Wawa 
interprets 
the 
action 
show 
page 
as 
\moderately 
increase 
the 
page's 
score.” 


The 
mapping 
of 
advice 
rules 
without 
variables 
follows 
the 
same 
process 
except 
that 
there 
is 
no 
variable-binding 
step. 


3.4 
Scoring 
Arbitrarily 
Long 
Pages 
and 
Links 
Wawa's 
use 
of 
neural 
networks 
means 
that 
it 
needs 
a 
mechanism 
for 
processing 
arbitrarily 
long 
Web 
pages 
with 
the 
xed-sized 
input 
vectors 
used 
by 
neural 
networks. 
Wawa's 
sliding 
window 
resolves 
this 
problem. 
Recall 
that 
the 
sliding 
window 
extracts 
features 
from 
a 
page 
by 
moving 
across 
it 
one 
word 
at 
a 
time. 
There 
are, 
however, 
some 
HTML 
tags 
like 
hPi, 
h/Pi, 
hBRi, 
and 
hHR. 
that 
act 
as 
\window 
breakers.” 
Window 
breakers7 
do 
not 
allow 
the 
sliding 
window 
to 
cross 
over 
them 
because 
such 
markers 
indicate 
a 
new 
topic. 
When 
a 
window 
breaker 
is 
encountered, 
the 
unused 
positions 
in 
the 
sliding 
window 
are 
left 
unlled. 


The 
score 
of 
a 
page 
is 
computed 
in 
two 
stages. 
In 
stage 
one, 
Wawa 
sets 
the 
input 
units 
that 
represent 
global 
features 
of 
the 
page, 
such 
as 
the 
number 
of 
words 
on 
the 
page. 
Then, 
Wawa 
slides 
the 
window 
(hence, 
the 
name 
sliding 
window) 
across 
the 
page. 
For 
each 
window 
position, 
Wawa 
rst 
sets 
the 
values 
for 
the 
input 
units 
representing 
positions 
in 
the 
window 
(e.g., 
word 
at 
center 
of 
window) 
and 
then 
calculates 
the 
values 
for 
all 
hidden 
units 
(HUs) 
that 
are 
directly 
connected 
to 
input 
units. 
These 
HUs 
are 
called 
\level-one” 
HUs. 
In 


7 
Wawa 
considers 
the 
following 
tags 
as 
window 
breakers:\hPi,” 
\h/Pi,” 
\hBRi,” 
\hHRi,” 
\hBLOCKQUOTEi,” 
\h/BLOCKQUOTEi,” 
\hPREi,” 
\h/PREi,” 
\hXMPi,"and 
\h/XMPi.” 



other 
words, 
Wawa 
performs 
forward-propagation 
from 
the 
input 
units 
to 
all 
level-one 
HUs. 
This 
process 
gives 
Wawa 
a 
list 
of 
values 
for 
all 
the 
level-one 
HUs 
at 
each 
position 
of 
the 
sliding 
window. 
For 
each 
level-one 
HU, 
Wawa 
picks 
the 
highest 
value 
to 
represent 
its 
activation. 


In 
stage 
two, 
the 
highest 
values 
of 
level-one 
HUs 
and 
the 
values 
of 
input 
units 
for 
global 
features 
are 
used 
to 
compute 
the 
values 
for 
all 
other 
HUs 
and 
the 
output 
unit. 
That 
is, 
Wawa 
performs 
forward-propagation 
from 
the 
level-
one 
HUs 
and 
the 
\global” 
input 
units 
to 
the 
output 
unit 
(which 
obviously 
will 
evaluate 
the 
values 
of 
all 
other 
HUs 
in 
the 
process). 
The 
value 
produced 
by 
the 
ScorePage 
network 
in 
the 
second 
stage 
is 
returned 
as 
the 
page's 
score. 


Note 
that 
Wawa's 
two-phase 
forward-propagation 
process 
means 
that 
HUs 
cannot 
connect 
to 
both 
input 
units 
and 
other 
HUs. 
The 
compilation 
process 
ensures 
this 
by 
adding 
\dummy” 
HUs 
in 
necessary 
locations. 


Although 
the 
score 
of 
a 
page 
is 
computed 
in 
two 
stages, 
Wawa 
scans 
the 
sliding 
window 
only 
once 
across 
the 
page, 
which 
occurs 
in 
stage 
one. 
By 
forward-
propagating 
only 
to 
level-one 
HUs 
in 
the 
rst 
stage, 
Wawa 
is 
eectively 
trying 
to 
get 
the 
values 
of 
its 
complex 
features. 
In 
stage 
two, 
Wawa 
uses 
these 
values 
and 
the 
global 
input 
units’ 
values 
to 
nd 
the 
score 
of 
the 
page. 
For 
example, 
the 
two-stage 
process 
allows 
Wawa 
to 
capture 
advice 
such 
as 


when 
( 
consecutive(Milwaukee 
Brewers) 
and 
consecutive(Chicago 
Cubs) 
) 
then 
show 
page 


If 
Wawa 
only 
had 
a 
one-stage 
process, 
it 
would 
not 
be 
able 
to 
correctly 
capt
ure 
this 
advice 
rule 
because 
both 
phrases 
cannot 
be 
in 
the 
sliding 
window 
simultaneously. 
Figure 
11 
illustrates 
this 
point. 


The 
value 
of 
a 
hyperlink 
is 
computed 
similarly, 
except 
that 
the 
ScoreLink 
network 
is 
used 
and 
the 
sliding 
window 
is 
slid 
over 
the 
hypertext 
associated 
with 
that 
hyperlink 
and 
the 
15 
words 
surrounding 
the 
hypertext 
on 
both 
sides. 



...MCenterInWindowM=M
.Cubs.MLeft1inWindowM=M
.Chicago.MRight1inWindowM=M
...M...MRight1inWindowM=M
...MCenterInWindowM=M
.Brewers.MLeft1inWindowM=M
.Milwaukee.M...M5M5M5M5MBiasM 
=M7.5MBiasM 
=M7.5MBiasM 
=M7.5MScorePage2.5M5M5MLevel-One Hidden UnitMHighest activation is produced by the 8thMand 9th words on the page which correspondMto Chicago and Cubs, respectively.MLevel-One Hidden UnitMHighest activation is produced by the 4thMand 5th words on the page which correspondMto Milwaukee and Brewers, respectively.MLevel-Two Hidden UnitM...MCenterInWindowM=M
.Cubs.MLeft1inWindowM=M
.Chicago.MRight1inWindowM=M
...M...MRight1inWindowM=M
...MCenterInWindowM=M
.Brewers.MLeft1inWindowM=M
.Milwaukee.M...M5M5M5M5MBiasM 
=M7.5MBiasM 
=M7.5MBiasM 
=M7.5MScorePage2.5M5M5MLevel-One Hidden UnitMHighest activation is produced by the 8thMand 9th words on the page which correspondMto Chicago and Cubs, respectively.MLevel-One Hidden UnitMHighest activation is produced by the 4thMand 5th words on the page which correspondMto Milwaukee and Brewers, respectively.MLevel-Two Hidden UnitM
Sample Web PageM

Brewers vs. Cubs

Milwaukee Brewers wilU 
play Chicago Cubs oA 


AdviceM

Friday September 17 atMwhen ( consecutive(Milwaukee Brewers) andWrigley Field.Mconsecutive(Chicago Cubs) ) then show page

.M

Figure 
11: 
Scoring 
a 
Page 
with 
a 
Sliding 
Window. 
In 
stage 
one, 
the 
sliding 
window 
is 
scanned 
through 
the 
page 
to 
determine 
the 
highest 
values 
of 
the 
level-one 
hidden 
units. 
In 
stage 
two, 
the 
highest 
activations 
of 
the 
level-one 
hidden 
units 
are 
used 
to 
score 
the 
level-two 
hidden 
unit 
and 
the 
output 
unit, 
which 
produces 
the 
overall 
score 
of 
the 
page. 



Chapter 
4 


Using 
WAWA 
to 
Retrieve 
Information 
from 
the 
Web 


Given 
a 
set 
of 
documents 
(a.k.a. 
the 
corpus) 
and 
a 
query, 
which 
usually 
cons
ists 
of 
a 
bunch 
of 
keywords 
or 
keyphrases, 
an 
\ideal” 
IR 
system 
is 
supposed 
to 
return 
all 
and 
only 
the 
documents 
that 
are 
relevant 
to 
the 
user's 
query. 
Unfort
unately 
with 
the 
recent 
exponential 
growth 
of 
on-line 
information, 
it 
is 
almost 
impossible 
to 
nd 
such 
an 
\ideal” 
IR 
system 
(Lawrence 
and 
Giles 
1999). 
In 
an 
eort 
to 
improve 
on 
the 
performance 
of 
existing 
IR 
systems, 
there 
has 
been 
a 
lot 
of 
interest 
in 
using 
machine 
learning 
techniques 
to 
solve 
the 
IR 
problem 
(Drummond, 
Ionescu, 
and 
Holte 
1995; 
Pazzani, 
Muramatsu, 
and 
Billsus 
1996; 
Joachims, 
Freitag, 
and 
Mitchell 
1997; 
Rennie 
and 
McCallum 
1999). 
An 
IR 
learner 
attempts 
to 
model 
a 
user's 
preferences 
and 
return 
on-line 
documents 
\matching” 
those 
interests. 


This 
chapter1 
describes 
the 
design 
of 
Wawa's 
IR 
learner 
(namely 
Wawa-
IR), 
the 
dierent 
ways 
its 
two 
neural 
networks 
are 
trained, 
and 
the 
manner 
in 
which 
it 
automatically 
derives 
training 
examples. 


4.1 
IR 
System 
Description 
Wawa-IR 
is 
a 
general 
search 
engine 
agent 
that 
through 
training 
can 
be 
speciali
zed 
and 
personalized. 
Table 
7 
provides 
a 
high-level 
description 
of 
Wawa-IR. 


1 
Portions 
of 
this 
chapter 
were 
previously 
published 
in 
Shavlik 
and 
Eliassi-Rad 
(1998a, 
1998b), 
Shavlik 
et 
al. 
(1999), 
and 
Eliassi-Rad 
and 
Shavlik 
(2001a). 



Table 
7: 
Wawa's 
Information-Retrieval 
Algorithm 


Unless 
they 
have 
been 
saved 
to 
disk 
in 
a 
previous 
session, 
create 
the 
ScoreLink 
and 
ScoreP 
age 
neural 
networks 
by 
reading 
the 
user's 
initial 
advice, 
if 
any 
was 
provided 
(see 
Section 
3.3). 


Either 
(a) 
start 
by 
adding 
user-provided 
urls 
to 
the 
search 
queue; 
or 
(b) 
initialize 
the 
search 
queue 
with 
urls 
that 
will 
query 
the 
user's 
chosen 
set 
of 
Web 
search-engine 
sites. 


Execute 
the 
following 
concurrent 
processes. 


Process 
#1 


While 
the 
search 
queue 
is 
not 
empty 
nor 
the 
maximum 
number 
of 
urls 
have 
been 
visited, 


Let 
URLtoV 
isit 
= 
pop(search 
queue). 
Fetch 
URLtoV 
isit. 


Evaluate 
URLtoV 
isit 
using 
ScoreP 
age 
network. 
If 
score 
is 
high 
enough, 
insert 
URLtoV 
isit 
into 
the 
sorted 
list 
of 
best 
pages 
found. 


Use 
the 
score 
of 
URLtoV 
isit 
to 
improve 
the 
predictions 
of 
the 
ScoreLink 
network 
(see 
Section 
4.3 
for 
details). 


Evaluate 
the 
hyperlinks 
in 
URLtoV 
isit 
using 
ScoreLink 
network 
(however, 
only 
score 
those 
links 
that 
have 
not 
yet 
been 
followed 
this 
session). 


Insert 
these 
new 
urls 
into 
the 
(sorted) 
search 
queue 
if 
they 
t 
within 
its 
max-length 
bound. 


Process 
#2 


Whenever 
the 
user 
provides 
additional 
advice, 
insert 
it 
into 
the 
appropriate 
neural 
network. 


Process 
#3 


Whenever 
the 
person 
rates 
a 
fetched 
page, 
use 
this 
rating 
to 
create 
a 
training 
example 
for 
the 
ScoreP 
age 
neural 
network. 



Initially, 
Wawa-IR's 
two 
neural 
networks 
are 
created 
by 
either 
using 
the 
techniques 
described 
in 
Section 
3 
or 
by 
reading 
them 
from 
disk 
(should 
this 
be 
a 
resumption 
of 
a 
previous 
session). 
Then, 
the 
basic 
operation 
of 
Wawa-IR 
is 
heuristic 
search, 
with 
its 
ScoreLink 
network 
acting 
as 
the 
heuristic 
function. 
Rather 
than 
solely 
nding 
one 
goal 
node, 
Wawa-IR 
collects 
the 
100 
pages 
that 
ScorePage 
rates 
highest. 
The 
user 
can 
choose 
to 
seed 
the 
queue 
of 
pages 
to 
fetch 
in 
two 
ways: 
either 
by 
specifying 
a 
set 
of 
starting 
urls 
or 
by 
providing 
a 
simple 
query 
that 
Wawa-IR 
converts 
into 
\query” 
urls 
that 
are 
sent 
to 
a 
user-chosen 
subset 
of 
selectable 
search 
engine 
sites 
(currently 
AltaVista, 
Excite, 
Google, 
HotBot, 
InfoSeek, 
Lycos, 
Teoma, 
2 
WebCrawler, 
and 
Yahoo). 


Although 
not 
mentioned 
in 
Table 
7, 
the 
user 
may 
also 
specify 
values 
for 
the 
following 
parameters: 


• 
an 
upper 
bound 
on 
the 
distance 
the 
agent 
can 
wander 
from 
the 
initial 
urls, 
where 
distance 
is 
dened 
as 
the 
number 
hyperlinks 
followed 
from 
the 
initial 
url 
(default 
value 
is 
10) 
• 
minimum 
score 
a 
hyperlink 
must 
receive 
in 
order 
to 
be 
put 
in 
the 
search 
queue 
(default 
value 
is 
0.6 
on 
a 
scale 
of 
[0,1]) 
• 
maximum 
number 
of 
hyperlinks 
to 
add 
from 
a 
page 
(default 
value 
is 
50) 
• 
maximum 
kilobytes 
to 
read 
from 
each 
page 
(default 
value 
is 
100 
kilobytes) 
• 
maximum 
retrieval 
time 
per 
page 
(default 
value 
is 
90 
seconds). 
2 
Teoma 
is 
a 
new 
search 
engine 
which 
received 
rave 
reviews 
in 
the 
Internet 
Scout 
Report 
of 
June 
21, 
2001 
(http://scout.cs.wisc.edu). 



34 


4.2 
Training 
WAWA-IR's 
Two 
Neural 
Netw
orks 
There 
are 
three 
ways 
to 
train 
Wawa-IR's 
two 
neural 
networks: 
(i) 
system-
generated 
training 
examples, 
(ii) 
advice 
from 
the 
user, 
and 
(iii) 
user-generated 
training 
examples. 


Before 
fetching 
a 
page 
P 
, 
Wawa-IR 
predicts 
the 
value 
of 
retrieving 
P 
by 
using 
the 
ScoreLink 
network. 
This 
\predicted” 
value 
of 
P 
is 
based 
on 
the 
text 
surrounding 
the 
hyperlink 
to 
P 
and 
some 
global 
information 
on 
the 
\referring” 
page 
(e.g., 
the 
title, 
the 
url, 
etc). 
After 
fetching 
and 
analyzing 
the 
actual 
text 
of 
P 
, 
Wawa-IR 
re-estimates 
the 
value 
of 
P 
, 
this 
time 
using 
the 
ScorePage 
network. 
Any 
dierences 
between 
the 
\before” 
and 
\after” 
estimates 
of 
P 
's 
score 
constitute 
an 
error 
that 
can 
be 
used 
by 
backpropagation 
(Rumelhart, 
Hinton, 
and 
Williams 
1986) 
to 
improve 
the 
ScoreLink 
neural 
network. 
The 
details 
of 
this 
process 
are 
further 
described 
in 
Section 
4.3. 
This 
type 
of 
training 
is 
not 
performed 
on 
the 
pages 
that 
constitute 
the 
initial 
search 
queue 
because 
their 
values 
were 
not 
predicted 
by 
the 
ScoreLink 
network. 


In 
addition 
to 
the 
above 
system-internal 
method 
of 
automatically 
creati
ng 
training 
examples, 
the 
user 
can 
improve 
the 
ScorePage 
and 
ScoreLink 
neural 
networks 
in 
two 
ways: 
(i) 
by 
providing 
additional 
advice 
and 
(ii) 
by 
providing 
training 
examples. 
Observing 
the 
agent's 
behavior 
is 
likely 
to 
invoke 
thoughts 
of 
good 
additional 
instructions 
(as 
has 
repeatedly 
happened 
to 
me 
in 
my 
experiments). 
A 
Wawa-IR 
agent 
can 
accept 
new 
advice 
and 
augment 
its 
neural 
networks 
at 
any 
time. 
It 
simply 
adds 
to 
its 
networks 
additional 
hidden 
units 
that 
represent 
the 
compiled 
advice, 
a 
technique 
whose 
eectiveness 
was 
previously 
demonstrated 
on 
several 
tasks 
(Maclin 
and 
Shavlik 
1996). 
Providi
ng 
additional 
hints 
can 
rapidly 
and 
drastically 
improve 
the 
performance 
of 
a 
Wawa-IR 
agent, 
provided 
the 
advice 
is 
relevant. 
Maclin 
and 
Shavlik 
(1996) 
showed 
that 
their 
algorithm 
is 
robust 
when 
given 
advice 
incrementally. 
When 
\bad” 
advice 
was 
given, 
the 
agent 
was 
able 
to 
quickly 
learn 
to 
ignore 
it. 



Although 
more 
tedious, 
the 
user 
can 
also 
rate 
pages 
as 
a 
mechanism 
for 
providing 
training 
examples 
for 
use 
by 
backpropagation. 
This 
can 
be 
useful 
when 
the 
user 
is 
unable 
to 
articulate 
why 
the 
agent 
is 
misscoring 
pages 
and 
links. 
This 
standard 
learning-from-labeled-examples 
methodology 
has 
been 
previously 
investigated 
by 
other 
researchers, 
e.g., 
Pazzani 
et 
al. 
(1996), 
and 
this 
aspect 
of 
Wawa-IR 
is 
discussed 
in 
Section 
5. 
However, 
I 
conjecture 
that 
most 
of 
the 
improvement 
to 
Wawa-IR's 
neural 
networks, 
especially 
to 
ScorePage, 
will 
result 
from 
users 
providing 
advice. 
In 
my 
personal 
experience, 
it 
is 
easy 
to 
think 
of 
simple 
advice 
that 
would 
require 
a 
large 
number 
of 
labeled 
examples 
in 
order 
to 
learn 
purely 
inductively. 
In 
other 
words, 
one 
advice 
rule 
typically 
covers 
a 
large 
number 
of 
labeled 
examples. 
For 
example, 
a 
rule 
such 
as 


when 
consecutive(404 
file 
not 
found) 
then 
avoid 
showing 
page 


will 
cover 
all 
pages 
that 
contain 
the 
phrase 
\404 
le 
not 
found.” 


4.3 
Deriving 
Training 
Examples 
for 
ScoreLink 
Wawa-IR 
uses 
temporal-dierence 
methods 
(Sutton 
1988) 
to 
automatically 
train 
the 
ScoreLink 
network. 
Specically, 
it 
employs 
a 
form 
of 
Q-learning 
(Watkins 
1989), 
which 
is 
a 
type 
of 
reinforcement 
learning 
(Sutton 
and 
Barto 
1998). 
Recall 
that 
the 
dierence 
between 
Wawa-IR's 
prediction 
of 
the 
link's 
value 
before 
fetching 
a 
url 
and 
its 
new 
estimate 
serves 
as 
an 
error 
that 
back-
propagation 
tries 
to 
reduce. 
Whenever 
Wawa-IR 
has 
collected 
all 
the 
necess
ary 
information 
to 
re-estimate 
a 
link's 
value, 
it 
invokes 
backpropagation. 
In 
addition, 
it 
periodically 
reuses 
these 
training 
examples 
several 
times 
to 
rene 
the 
network. 
The 
main 
advantage 
of 
using 
reinforcement 
learning 
to 
train 
the 
ScoreLink 
network 
is 
that 
Wawa-IR 
is 
able 
to 
automatically 
construct 
these 
training 
examples 
without 
direct 
user 
intervention. 


As 
is 
typical 
in 
reinforcement 
learning, 
the 
value 
of 
an 
action 
(following 
a 
hyperlink 
in 
this 
case) 
is 
not 
solely 
determined 
by 
the 
immediate 
result 
of 
the 



action 
(i.e., 
the 
value 
of 
the 
page 
retrieved 
minus 
any 
retrieval-time 
penalty). 
Rather, 
it 
is 
important 
to 
also 
reward 
links 
that 
lead 
to 
pages 
with 
additional 
good 
links 
on 
them. 
Figure 
12 
and 
Equation 
1 
illustrate 
this 
point. 


Page Elink from C 
Page A Page B 
Link score 
from A to B 
Second bestscoring linkfrom B 
Best scoringlink from B 
Page D 
Page C 
Best scoring 

Figure 
12: 
Reestimating 
the 
Value 
of 
a 
Hyperlink 
Equation 
1: 
New 
Estimate 
of 
the 
Link 
A 
. 
B 
under 
Best-First 
Search 


if 
ScoreLink(B 
. 
C) 
> 
0 
then 
new 
estimate 
of 
ScoreLink(A 
. 
B) 
= 
fetchP 
enalty(B)+ 
ScoreP 
age(B) 


+ 
(fetchP 
enalty(C)+ 
ScoreP 
age(C)) 
+ 
2MAX(0, 
ScoreLink(B 
. 
D), 
ScoreLink(C 
. 
E)) 
else 


new 
estimate 
of 
ScoreLink(A 
. 
B) 
= 
fetchP 
enalty(B)+ 
ScoreP 
age(B) 


Wawa-IR 
denes 
the 
task 
of 
the 
ScoreLink 
function 
to 
be 
estimating 
the 
discounted 
sum 
of 
the 
scores 
of 
the 
fetched 
pages 
plus 
the 
cost 
of 
fetching 
them. 
The 
discount 
rate, 
. 
in 
Equation 
1, 
determines 
the 
amount 
by 
which 
the 
value 
of 
a 
page 
is 
discounted 
because 
it 
is 
encountered 
at 
a 
later 
time 
step. 
. 
has 
a 
default 
value 
of 
0.95 
(where 
. 
. 
[0, 
1]). 
The 
closer 
the 
value 
of 
. 
is 
to 
1, 
the 
more 
strongly 
the 
values 
of 
future 
pages 
are 
taken 
into 
account. 
The 
cost 
of 



fetching 
a 
page 
depends 
on 
its 
size 
and 
retrieval 
rate.3 


I 
assume 
that 
the 
system 
started 
its 
best-rst 
search 
at 
the 
page 
referred 
to 
by 
the 
hyperlink. 
In 
other 
words, 
if 
in 
Figure 
12, 
Page 
B 
was 
the 
root 
of 
a 
best-rst 
search, 
Wawa-IR 
would 
next 
visit 
C 
and 
then 
either 
D 
or 
E, 
depending 
on 
which 
referring 
hyperlink 
scored 
higher. 
Hence, 
the 
rst 
few 
terms 
of 
the 
sum 
would 
be 
the 
value 
of 
root 
page 
B, 
plus 
the 
value 
of 
C 
discounted 
by 
one 
time 
step. 
Wawa-IR 
then 
recursively 
estimates 
the 
remainder 
of 
this 
sum 
by 
using 
the 
discounted4 
higher 
score 
of 
the 
two 
urls 
that 
would 
be 
at 
the 
front 
of 
the 
search 
queue. 


Since 
Wawa-IR 
uses 
best-rst 
search, 
it 
actually 
may 
have 
a 
more 
promising 
url 
in 
its 
search 
queue 
than 
the 
link 
to 
C 
(assuming 
the 
move 
from 
A 
to 
B 
took 
place). 
In 
order 
to 
keep 
the 
calculation 
of 
the 
re-estimated 
ScoreLink 
function 
localized, 
I 
largely 
ignore 
this 
aspect 
of 
the 
system's 
behavior. 
Wawa-
IR 
only 
partially 
captures 
this 
phenomenon 
by 
adjusting 
the 
above 
calculation 
such 
that 
the 
links 
with 
negative 
predicted 
value 
are 
not 
followed.5 


The 
above 
scenario 
(of 
localizing 
the 
re-estimation 
of 
ScoreLink 
function) 
does 
not 
apply 
when 
an 
url 
cannot 
be 
fetched 
(i.e., 
a 
\dead 
link"). 
Upon 
such 
an 
occurrence, 
ScoreLink 
receives 
a 
large 
penalty. 
Depending 
on 
the 
type 
of 
failure, 
the 
default 
values 
range 
from 
-6.25 
(for 
failures 
such 
as 
\le 
not 
found") 
to 
0 
(for 
failures 
like 
\network 
was 
down"). 


The 
denition 
of 
Wawa-IR's 
\Q-function” 
(see 
Equation 
1) 
represents 
a 
best-rst, 
beam-search 
strategy. 
This 
denition 
is 
dierent 
from 
the 
traditional 
denition 
(see 
Equation 
2), 
which 
essentially 
assumes 
a 
hill-climbing 
approach. 


3 
The 
cost 
of 
fetching 
a 
page 
is 
dened 
to 
be 
(..1:25 
× 
rate)+(..0:001 
× 
rate 
× 
size) 
and 
has 
a 
maximum 
value 
of 
-4.85. 


4 
The 
score 
is 
doubly 
discounted 
here 
since 
the 
urls 
are 
two 
time 
steps 
in 
the 
future. 


5 
Wawa-IR's 
networks 
are 
able 
to 
produce 
negative 
values 
because 
their 
output 
units 
simply 
output 
their 
weighted 
sum 
of 
inputs, 
i.e., 
they 
are 
linear 
units. 
Note 
that 
the 
hidden 
units 
in 
Wawa-IR's 
networks 
use 
sigmoidal 
activation 
functions. 



Equation 
2: 
New 
Estimate 
of 
the 
Link 
A 
. 
B 
under 
Hill-Climbing 
Search 


if 
ScoreLink(B 
. 
C) 
> 
0 
then 
new 
estimate 
of 
ScoreLink(A 
. 
B) 
= 
fetchP 
enalty(B)+ 
ScoreP 
age(B) 


+ 
(fetchP 
enalty(C)+ 
ScoreP 
age(C)) 
+ 
2MAX(0, 
ScoreLink(C 
. 
E)) 
else 


new 
estimate 
of 
ScoreLink(A 
. 
B) 
= 
fetchP 
enalty(B)+ 
ScoreP 
age(B) 


Reviewing 
Figure 
12, 
one 
can 
understand 
the 
dierence 
between 
Wawa-IR's 
approach 
and 
the 
traditional 
way 
of 
dening 
the 
Q-function. 
In 
the 
traditional 
(i.e., 
hill-climbing) 
approach, 
since 
B 
. 
D 
was 
the 
second 
best-scoring 
link 
from 
B, 
its 
value 
is 
not 
reconsidered 
in 
the 
calculation 
of 
the 
score 
of 
A 
. 
B. 
This 
search 
strategy 
does 
not 
seem 
optimal 
for 
nding 
the 
most 
relevant 
pages 
on 
the 
Web. 
Instead, 
the 
link 
with 
the 
highest-score 
from 
the 
set 
of 
encountered 
links 
should 
always 
be 
traversed. 
For 
example, 
if 
an 
agent 
has 
to 
choose 
between 
the 
links 
B 
. 
D 
and 
C 
. 
E, 
it 
should 
follow 
the 
link 
that 
has 
the 
highest 
score 
and 
not 
ignore 
B 
. 
D 
simply 
because 
this 
link 
was 
seen 
at 
a 
previous 
step 
and 
did 
not 
have 
the 
highest 
value 
at 
that 
step. 


4.4 
Summary 
The 
main 
advantage 
of 
Wawa's 
IR 
system 
is 
its 
use 
of 
theory 
renement. 
That 
is, 
Wawa 
utilizes 
the 
user's 
prior 
knowledge, 
which 
need 
not 
be 
perfectly 
correct 
to 
guide 
Wawa-IR. 
Wawa-IR 
is 
a 
learning 
system, 
so 
it 
is 
able 
to 
improve 
the 
user's 
instructions. 
I 
am 
able 
to 
rapidly 
transform 
Wawa-IR, 
which 
is 
a 
general 
search 
engine, 
into 
a 
specialized 
and 
personalized 
IR 
agent 
by 
merely 
adding 
a 
simple 
\front-end” 
user 
interface 
that 
accepts 
domain-specic 
information 
and 



uses 
it 
to 
create 
rules 
in 
Wawa's 
advice 
language. 
Chapter 
5 
describes 
the 
rapid 
creation 
of 
an 
eective 
\home-page 
nder” 
agent 
from 
the 
generic 
Wawa-IR 
system. 


I 
also 
allow 
the 
user 
to 
continually 
provide 
advice 
to 
the 
agent. 
This 
charact
eristic 
of 
Wawa-IR 
enables 
the 
user 
to 
observe 
an 
agent 
and 
guide 
its 
behavior 
(whenever 
the 
user 
feels 
that 
Wawa-IR 
agent's 
user 
model 
is 
incorrect). 
Fin
ally, 
by 
learning 
the 
ScoreLink 
function, 
a 
Wawa-IR 
agent 
is 
able 
to 
more 
eectively 
search 
the 
Web 
(by 
learning 
about 
relevant 
links) 
and 
automatically 
create 
its 
own 
training 
examples 
via 
reinforcement 
learning 
(which 
in 
turn 
imp
roves 
the 
accuracy 
of 
the 
agent 
with 
respect 
to 
the 
relevancy 
of 
the 
pages 
returned). 


Due 
to 
my 
use 
of 
articial 
neural 
networks, 
it 
is 
dicult 
to 
understand 
what 
was 
learned 
(Craven 
and 
Shavlik 
1996). 
It 
would 
be 
nice 
if 
a 
Wawa-IR 
agent 
could 
explain 
its 
reasoning 
to 
the 
user. 
In 
an 
attempt 
to 
alleviate 
this 
problem, 
Wawa-IR 
has 
a 
\visualizer” 
for 
each 
of 
its 
two 
neural 
networks 
(Craven 
and 
Shavlik 
1992). 
The 
visualizer 
draws 
the 
neural 
networks 
containing 
the 
user's 
compiled 
advice 
and 
graphically 
displays 
information 
on 
all 
nodes 
and 
links 
in 
the 
network. 



Chapter 
5 


Retrieval 
Experiments 
with 
WAWA 


This 
chapter1 
describes 
a 
case 
study 
done 
in 
1998 
and 
repeated 
in 
2001 
to 
evaluate 
Wawa's 
IR 
system.2 
I 
built 
a 
home-page-nder 
agent 
by 
using 
Wawa's 
advice 
language. 
Appendix 
B 
presents 
the 
complete 
advice 
used 
for 
the 
home-
page 
nder. 
The 
results 
of 
this 
empirical 
study 
illustrate 
that 
by 
utilizing 
Wawa-IR, 
a 
user 
can 
build 
an 
eective 
agent 
for 
a 
web-based 
task 
quickly. 


5.1 
An 
Instructable 
and 
Adaptive 
Home-Page 
Finder 
In 
1998, 
I 
chose 
the 
task 
of 
building 
a 
home-page 
nder 
because 
of 
an 
existi
ng 
system 
named 
Ahoy! 
(Shakes, 
Langheinrich, 
and 
Etzioni 
1997), 
which 
provided 
a 
valuable 
benchmark. 
Ahoy! 
uses 
a 
technique 
called 
Dynamic 
Refere
nce 
Sifting, 
which 
lters 
the 
output 
of 
several 
Web 
indices 
and 
generates 
new 
guesses 
for 
urls 
when 
no 
promising 
candidates 
are 
found. 


I 
wrote 
a 
simple 
interface 
layered 
on 
top 
of 
Wawa-IR 
(see 
Figure 
13) 
that 
asks 
for 
whatever 
relevant 
information 
is 
known 
about 
the 
person 
whose 
home 
page 
is 
being 
sought: 
rst 
name, 
possible 
nicknames, 
middle 
name 
or 
initial, 
last 


1 
Portions 
of 
this 
chapter 
were 
previously 
published 
in 
Shavlik 
and 
Eliassi-Rad 
(1998a, 
1998b), 
Shavlik 
et 
al. 
(1999), 
and 
Eliassi-Rad 
and 
Shavlik 
(2001a). 


2 
I 
reran 
the 
experiment 
in 
2001 
to 
compare 
Wawa-IR's 
performance 
to 
Google, 
which 
did 
not 
exist 
in 
1998, 
and 
also 
to 
measure 
the 
sensitivity 
of 
some 
of 
the 
design 
choices 
made 
in 
the 
1998 
experiments. 



name, 
miscellaneous 
phrases, 
and 
a 
partial 
url 
(e.g., 
edu 
or 
ibm.com). 
I 
then 
wrote 
a 
small 
program 
that 
reads 
these 
elds 
and 
creates 
advice 
that 
is 
sent 
to 
Wawa-IR. 
I 
also 
wrote 
76 
general 
advice 
rules 
related 
to 
home-page 
nding, 
many 
of 
which 
are 
slight 
variants 
of 
others 
(e.g., 
with 
and 
without 
middle 
names 
or 
initials). 
Specializing 
Wawa-IR 
for 
this 
task 
and 
creating 
the 
initial 
general 
advice 
took 
only 
one 
day, 
plus 
I 
spent 
parts 
of 
another 
2-3 
days 
tinkering 
with 
the 
advice 
using 
100 
examples 
of 
a 
\training 
set” 
(described 
below). 
This 
step 
allowed 
me 
to 
manually 
rene 
my 
advice 
– 
a 
process 
which 
I 
expect 
will 
be 
typical 
of 
future 
users 
of 
Wawa-IR. 


Altavista, Excite, Infoseek, Lycos, Yahoo,

Figure 
13: 
Interface 
of 
Wawa-IR's 
Home-Page 
Finder 


To 
learn 
a 
general 
concept 
about 
home-page 
nding, 
I 
used 
the 
variable 
binding 
mechanism 
of 
Wawa-IR's 
advice 
language. 
Wawa-IR's 
home-page 



42 


nder 
accepts 
instructions 
that 
certain 
words 
should 
be 
bound 
to 
variables 
associated 
with 
rst 
names, 
last 
names, 
etc. 
I 
wrote 
general-purpose 
advice 
about 
home-page 
nding 
that 
uses 
these 
variables. 
Hence, 
rule 
1 
in 
Table 
6 
of 
Chapter 
3 
is 
actually 
written 
using 
advice 
variables 
(as 
illustrated 
in 
Figure 
10 
of 
Chapter 
3) 
and 
not 
the 
names 
of 
specic 
people. 


Since 
the 
current 
implementation 
of 
Wawa-IR 
can 
refer 
only 
to 
advice 
variables 
when 
they 
appear 
in 
the 
sliding 
window, 
advice 
that 
refers 
to 
other 
aspects 
of 
a 
Web 
page 
needs 
to 
be 
specially 
created 
and 
subsequently 
retracted 
for 
each 
request 
to 
nd 
a 
specic 
person's 
home 
page.3 
The 
number 
of 
these 
specic-person 
rules 
that 
Wawa-IR's 
home-page 
nder 
creates 
depends 
on 
how 
much 
information 
is 
provided 
about 
the 
target 
person. 
For 
the 
experiments 
below, 
I 
only 
provide 
information 
about 
people's 
names 
so 
that 
the 
home-page 
nder 
would 
be 
as 
generic 
as 
possible. 
This 
leads 
to 
the 
generation 
of 
one 
to 
two 
dozen 
rules, 
depending 
on 
whether 
or 
not 
middle 
names 
or 
initials 
are 
provided. 


5.2 
Motivation 
and 
Methodology 
In 
1998, 
I 
randomly 
selected 
215 
people 
from 
Aha's 
list 
of 
machine 
learni
ng 
(ML) 
and 
case-based 
reasoning 
(CBR) 
researchers 
(www.aic.nrl.navy.mil/ 


aha/people.html) 
to 
run 
experiments 
that 
evaluate 
Wawa-IR. 
Out 
of 
the 
215 
people 
selected, 
I 
randomly 
picked 
115 
of 
them 
to 
train 
Wawa-IR 
and 
used 
the 
remaining 
100 
as 
my 
test 
set.4 
In 
2001, 
I 
updated 
the 
data 
set 
by 
replacing 
the 
people 
that 
no 
longer 
had 
personal 
home 
pages 
with 
randomly 
selected 
people 
from 
the 
2001 
version 
of 
Aha's 
list. 
The 
\training” 
phase 
has 
two 
steps. 
In 
the 
rst 
step, 
I 
manually 
run 
the 
system 
on 
100 
people 
randomly 
picked 
from 
the 
training 
set 
(I 
will 
refer 
to 
this 


3 
Users 
can 
retract 
advice 
from 
Wawa-IR's 
neural 
networks. 
To 
retract 
an 
advice 
rule, 
Wawa-IR 
removes 
the 
network 
nodes 
and 
links 
associated 
with 
that 
rule. 


4 
I 
follow 
standard 
machine 
learning 
methodology 
in 
dividing 
the 
data 
set 
into 
two 
subsets, 
where 
one 
subset 
is 
used 
for 
training 
and 
the 
other 
for 
testing 
purposes. 



set 
as 
the 
advice-training 
set), 
rening 
my 
advice 
by 
hand 
before 
\freezing” 
the 
advice-giving 
phase. 
In 
the 
second 
step, 
I 
split 
the 
remaining 
15 
people 
into 
a 
set 
of 
10 
people 
for 
backpropagation 
training 
(I 
will 
refer 
to 
this 
set 
as 
the 
machine-training 
set) 
and 
a 
set 
consisting 
of 
5 
people 
for 
tuning 
(I 
will 
refer 
to 
this 
set 
as 
the 
tuning 
set). 


I 
do 
not 
perform 
backpropagation-based 
training 
during 
the 
rst 
step 
of 
the 
advice-training 
phase, 
since 
I 
want 
to 
see 
how 
accurate 
I 
can 
make 
my 
advice 
without 
any 
machine 
learning. 
Then, 
for 
each 
person 
in 
the 
machine-training 
set, 
Wawa-IR 
initiates 
a 
search 
(by 
using 
the 
designated 
search 
engines) 
to 
nd 
training 
pages 
and 
links. 
The 
ScorePage 
function 
is 
then 
trained 
via 
back-
propagation. 
The 
tuning 
set 
is 
used 
to 
avoid 
overtting 
the 
training 
examples.5 
For 
rening 
the 
ScoreLink 
function, 
Wawa-IR 
automatically 
generates 
traini
ng 
examples 
for 
each 
person 
via 
temporal-dierence 
learning 
(see 
Section 
4.3). 
Finally, 
I 
evaluate 
Wawa-IR's 
\trained” 
home-page 
nder 
on 
the 
test 
set. 
Duri
ng 
the 
testing 
phase, 
no 
learning 
takes 
place. 


I 
consider 
one 
person 
as 
one 
training 
example, 
even 
though 
for 
each 
person 
I 
rate 
several 
pages. 
Hence, 
the 
actual 
number 
of 
dierent 
input-output 
pairs 
processed 
by 
backpropagation 
is 
larger 
than 
the 
size 
of 
my 
training 
set 
(i.e., 
by 
a 
factor 
of 
about 
10). 
Table 
8 
describes 
my 
technique. 


Table 
8: 
The 
Supervised-Learning 
Technique 
Used 
in 
Training 
ScorePage 
Netw
ork. 
See 
text 
for 
explanation 
of 
desired 
output 
values 
used 
during 
training. 



While 
the 
error 
on 
the 
tuning 
set 
is 
not 
increasing 
do 
the 
following: 
For 
each 
person 
in 
the 
machine-training 
set 
do 
the 
following 
10 
times: 


If 
the 
person's 
home 
page 
was 
found, 
then 
train 
the 
ScorePage 
network 
on 
those 
pages 
that 
scored 
higher 
than 
the 
home 
page, 
the 
actual 
home 
page, 
and 
the 
ve 
pages 
that 
scored 
immediately 
below 
the 
actual 
home 
page. 


Otherwise, 
train 
the 
network 
on 
the 
5 
highest-scoring 
pages. 
Calculate 
the 
error 
on 
the 
tuning 
set. 



5 
When 
a 
network 
is 
overt, 
it 
performs 
very 
well 
on 
training 
data 
but 
poorly 
on 
new 
data. 



To 
do 
neural-network 
learning, 
I 
need 
to 
associate 
a 
desired 
score 
to 
each 
page 
Wawa-IR 
encounters. 
I 
will 
then 
be 
able 
to 
compare 
this 
score 
to 
the 
output 
of 
the 
ScorePage 
network 
for 
this 
page 
and 
nally 
perform 
error 
back-
propagation 
(Rumelhart, 
Hinton, 
and 
Williams 
1986). 
I 
use 
a 
simple 
heuristic 
for 
getting 
the 
desired 
score 
of 
a 
page. 
I 
dene 
a 
target 
page 
to 
be 
the 
actual 
home 
page 
of 
a 
person. 
Recall 
that 
the 
score 
of 
a 
page 
is 
a 
real 
number 
in 
the 
interval 
[-10.0, 
10.0]. 
My 
heuristic 
is 
as 
follows: 


• 
If 
the 
page 
encountered 
is 
the 
target 
page, 
its 
desired 
score 
is 
9.5. 
• 
If 
the 
page 
encountered 
has 
the 
same 
host 
as 
the 
target 
page, 
its 
desired 
score 
is 
7.0. 
• 
Otherwise, 
the 
desired 
score 
of 
the 
page 
encountered 
is 
-1.0. 
For 
example, 
suppose 
the 
target 
person 
is 
\Alan 
Turing” 
and 
the 
target 
page 
is 
http://www.turing.org.uk/turing/ 
(i.e., 
his 
home 
page). 
Upon 
enc
ountering 
a 
page 
at 
http://www.turing.org.uk/, 
I 
will 
set 
its 
desired 
score 
to 
7.0, 
since 
that 
page 
has 
the 
same 
host 
as 
Alan 
Turing's 
home 
page. 


In 
the 
2001 
experiments, 
I 
compare 
the 
sensitivity 
of 
Wawa-IR's 
homepage 
nder 
to 
the 
above 
heuristic 
by 
changing 
the 
desired 
score 
of 
pages 
with 
the 
same 
host 
as 
the 
target 
page 
from 
7.0 
to 
3.0.6 


To 
judge 
Wawa-IR's 
performance 
in 
the 
task 
of 
nding 
home-pages, 
I 
prov
ide 
it 
with 
the 
advice 
discussed 
above 
and 
presented 
in 
Appendix 
B. 
It 
is 
important 
to 
note 
that 
for 
this 
experiment 
I 
intentionally 
do 
not 
provide 
advice 
that 
is 
specic 
to 
ML, 
CBR, 
AI 
research, 
etc. 
By 
doing 
this, 
I 
am 
able 
to 
build 
a 
generalized 
home-page 
nder 
and 
not 
one 
that 
specializes 
in 
nding 
ML, 
CBR, 
and 
AI 
researchers. 
Wawa-IR 
has 
several 
options, 
which 
aect 
its 
performance 
both 
in 
the 
amount 
of 
execution 
time 
and 
the 
accuracy 
of 
its 
results. 
Before 
running 
any 
experiments, 
I 
choose 
small 
numbers 
for 
my 
parameters, 
using 
100 


6 
It 
turns 
out 
(see 
Section 
5.4 
that 
3.0 
works 
slightly 
better. 



45 


for 
the 
maximum 
number 
of 
pages 
fetched, 
and 
3 
as 
the 
maximum 
distance 
to 
travel 
away 
from 
the 
pages 
returned 
by 
the 
search 
engines. 


I 
start 
Wawa-IR 
by 
providing 
it 
the 
person's 
name 
as 
given 
on 
Aha's 
Web 
page, 
though 
I 
partially 
standardize 
my 
examples 
by 
using 
all 
common 
variants 
of 
rst 
names 
(e.g., 
\Joseph” 
and 
\Joe"). 
Wawa-IR 
then 
converts 
the 
name 
into 
an 
initial 
query 
(see 
the 
next 
paragraph). 
For 
the 
1998 
experiments, 
this 
initial 
query 
was 
sent 
to 
the 
following 
ve 
search 
engines: 
AltaVista, 
Excite, 
InfoSeek, 
Lycos, 
and 
Yahoo. 


In 
my 
1998 
experiments, 
I 
compared 
the 
performance 
of 
Wawa-IR 
with 
the 
performances 
of 
Ahoy! 
and 
HotBot, 
a 
search 
engine 
not 
used 
by 
Wawa-IR 
and 
the 
one 
that 
performed 
best 
in 
the 
home-page 
experiments 
of 
Shakes 
et 
al. 
(1997). 
I 
provided 
the 
names 
in 
my 
test 
set 
to 
Ahoy! 
via 
its 
Web 
interface. 
Ahoy! 
uses 
MetaCrawler 
as 
its 
search 
engine, 
which 
queries 
nine 
search 
engines 
as 
opposed 
to 
Wawa-IR, 
which 
queried 
only 
ve 
search 
engines 
in 
1998. 
I 
ran 
HotBot 
under 
two 
dierent 
conditions. 
The 
rst 
setting 
performed 
a 
specialized 
HotBot 
search 
for 
people; 
I 
used 
the 
names 
given 
on 
Aha's 
page 
for 
these 
queries. 
In 
the 
second 
variant, 
I 
provided 
HotBot 
with 
a 
general-
purpose 
disjunctive 
query, 
which 
contained 
the 
person's 
last 
name 
as 
a 
required 
word, 
and 
all 
the 
likely 
variants 
of 
the 
person's 
rst 
name. 
The 
latter 
was 
the 
same 
query 
that 
Wawa-IR 
initially 
sends 
to 
the 
ve 
search 
engines 
used 
in 
1998. 
For 
my 
experiments, 
I 
only 
looked 
at 
the 
rst 
100 
pages 
that 
HotBot 
returned 
and 
assumed 
that 
few 
people 
would 
look 
further 
into 
the 
results 
returned 
by 
a 
search 
engine. 


In 
my 
2001 
experiments, 
I 
compared 
the 
performances 
of 
dierent 
Wawa-IR 
settings 
with 
the 
performance 
of 
Google. 
I 
ran 
Wawa-IR 
with 
two 
dierent 
sets 
of 
search 
engines: 
(i) 
AltaVista, 
Excite, 
InfoSeek, 
Lycos, 
Teoma 
and 
(ii) 
Google. 
I 
did 
not 
use 
Yahoo, 
WebCrawler, 
and 
HotBot 
in 
my 
2001 
experiments 
since 
they 
are 
powered 
by 
Google, 
Excite, 
and 
Lycos, 
respectively. 
Since 
Google's 
query 
language 
does 
not 
allow 
me 
to 
express, 
in 
one 
attempt, 
the 
same 
general-purpose 
disjunctive 
query 
as 
the 
one 
described 



46 


in 
the 
last 
paragraph, 
I 
broke 
that 
query 
down 
into 
the 
following 
queries: 


1. 
\?FirstName 
?LastName” 
?FirstName 
+?LastName 
2. 
\?NickName 
?LastName” 
?NickName 
+?LastName 
3. 
\?FirstName 
?LastName” 
?FirstName 
+?LastName 
\home 
page” 
4. 
\?NickName 
?LastName” 
?NickName 
+?LastName 
\home 
page” 
5. 
\?FirstName 
?LastName” 
?FirstName 
+?LastName 
home-page 
6. 
\?NickName 
?LastName” 
?NickName 
+?LastName 
home-page 
The 
variables 
?FirstName, 
?NickName, 
and 
?LastName 
get 
bound 
to 
each 
pers
on's 
rst 
name, 
nick 
name 
(if 
one 
is 
available), 
and 
last 
name, 
respectively. 
Each 
query 
asks 
for 
pages 
that 
have 
all 
the 
terms 
of 
the 
query 
on 
the 
page. 
For 
each 
person, 
I 
examine 
the 
rst 
100 
pages 
that 
Google 
returns 
and 
report 
the 
highest 
overall 
rank 
of 
a 
person's 
home 
page 
(if 
the 
target 
page 
was 
found). 
I 
send 
the 
same 
queries 
to 
the 
search 
engines 
that 
seed 
Wawa-IR's 
home-page 
nder. 


Since 
people 
often 
have 
dierent 
urls 
pointing 
to 
their 
home 
pages, 
rather 
than 
comparing 
urls 
to 
those 
provided 
on 
Aha's 
page, 
I 
instead 
do 
an 
exact 
comparison 
on 
the 
contents 
of 
fetched 
pages 
to 
the 
contents 
of 
the 
page 
linked 
to 
Aha's 
site. 
Also, 
when 
running 
Wawa-IR, 
I 
never 
fetch 
any 
urls 
whose 
server 
matched 
that 
of 
Aha's 
page, 
thereby 
preventing 
Wawa-IR 
from 
visiting 
Aha's 
site. 


5.3 
Results 
and 
Discussion 
from 
1998 
Table 
9 
lists 
the 
best 
performance 
of 
Wawa-IR's 
home-page 
nder 
and 
the 
results 
from 
Ahoy! 
and 
HotBot. 
SL 
and 
RL 
are 
used 
to 
refer 
to 
superv
ised 
and 
reinforcement 
learning, 
respectively. 
Recall 
that, 
SL 
is 
used 
to 
train 



ScorePage 
and 
RL 
to 
train 
ScoreLink. 
Besides 
reporting 
the 
percentage 
of 
the 
100 
test 
set 
home-pages 
found, 
I 
report 
the 
average 
ordinal 
position 
(i.e., 
rank) 
given 
that 
a 
page 
is 
found, 
since 
Wawa-IR, 
Ahoy!, 
and 
HotBot 
all 
return 
sorted 
lists. 


Table 
9: 
Empirical 
Results: 
Wawa-IR 
vs 
Ahoy! 
and 
HotBot 
(SL 
= 
Superv
ised 
Learning 
and 
RL 
= 
Reinforcement 
Learning) 


System 
% 
Found 
Mean 
Rank 
Given 
Page 
Found 


Wawa-IR 
with 
SL, 
RL, 
& 
76 
rules 
92% 
1:3 
Ahoy! 
79% 
1:4 
HotBot 
person 
search 
66% 
12:0 
HotBot 
general 
search 
44% 
15:4 



These 
results 
provide 
strong 
evidence 
that 
the 
version 
of 
Wawa-IR, 
spec
ialized 
into 
a 
home-page 
nder 
by 
adding 
simple 
advice, 
produces 
a 
better 
home-page 
nder 
than 
does 
the 
proprietary 
people-nder 
created 
by 
HotBot 
or 
by 
Ahoy!. 
The 
dierence 
(in 
percentage 
of 
home-pages 
found) 
between 
Wawa-IR 
and 
HotBot 
in 
this 
experiment 
is 
statistically 
signicant 
at 
the 
99% 
condence 
level. 
The 
dierence 
between 
Wawa-IR 
and 
Ahoy! 
is 
statistic
ally 
signicant 
at 
the 
90% 
condence 
level. 
Recall 
that 
I 
specialize 
Wawa-IR's 
generic 
IR 
system 
for 
this 
task 
in 
only 
a 
few 
days. 


Table 
10 
lists 
the 
home-page 
nder's 
performance 
without 
supervised 
and/or 
reinforcement 
learning. 
The 
motivation 
is 
to 
see 
if 
I 
gain 
performance 
through 
learning. 
I 
also 
remove 
28 
of 
my 
initial 
76 
rules 
to 
see 
how 
much 
my 
performance 
degrades 
with 
less 
advice. 
The 
28 
rules 
removed 
refer 
to 
words 
one 
might 
nd 
in 
a 
home 
page 
that 
are 
not 
the 
person's 
name 
(such 
as 
\resume", 
\cv", 
\phone", 
\address", 
\email", 
etc). 
Table 
10 
also 
reports 
the 
performance 
of 
Wawa-IR 
when 
its 
home-page 
nder 
is 
trained 
with 
less 
than 
76 
advice 
rules 
and/or 
is 
not 
trained 
with 
supervised 
or 
reinforcement 
learning. 


The 
dierences 
between 
the 
Wawa-IR 
runs 
containing 
76 
advice 
rules 
with 
learning 
and 
without 
learning 
are 
not 
statistically 
signicant. 
When 
I 
reduce 



Table 
10: 
Empirical 
Results 
on 
Dierent 
Versions 
of 
Wawa-IR's 
Home-Page 
Finder 
(SL 
= 
Supervised 
Learning 
and 
RL 
= 
Reinforcement 
Learning) 


SL 
RL 
# 
of 
Advice 
Rules 
% 
Found 
Mean 
Rank 
Given 
Page 
Found 


?. 
76 
92% 
1.3 
. 
76 
91% 
1.2 
76 
90% 
1.6 
?. 
48 
89% 
1.2 
. 
48 
85% 
1.4 
48 
83% 
1.3 



the 
number 
of 
advice 
rules, 
Wawa-IR's 
performance 
deteriorates. 
The 
results 
show 
that 
Wawa-IR 
is 
able 
to 
learn 
and 
increase 
its 
accuracy 
by 
6 
percentage 
points 
(from 
83% 
with 
no 
learning 
to 
89% 
with 
both 
supervised 
and 
reinforcem
ent 
learning); 
however, 
the 
dierence 
is 
not 
statistically 
signicant 
at 
the 
90% 
condence 
level. 
It 
is 
not 
surprising 
that 
Wawa-IR 
is 
not 
able 
to 
reach 
its 
best 
performance, 
since 
I 
do 
not 
increase 
the 
size 
of 
my 
training 
data 
to 
compensate 
for 
the 
reduction 
in 
advice 
rules. 
Nonetheless, 
even 
with 
48 
rules, 
the 
diere
nce 
in 
percentage 
of 
home 
pages 
found 
by 
Wawa-IR 
and 
by 
HotBot 
(in 
this 
experiment) 
is 
statistically 
signicant 
at 
the 
95% 
condence 
level. 


In 
the 
cases 
where 
the 
target 
page 
for 
a 
specic 
person 
is 
found, 
the 
mean 
rank 
of 
the 
target 
page 
is 
similar 
in 
all 
runs. 
Recall 
that 
the 
mean 
rank 
of 
the 
target 
page 
refers 
to 
its 
ordinal 
position 
in 
the 
list 
of 
pages 
returned 
to 
the 
user. 
The 
mean 
rank 
can 
be 
lower 
with 
the 
runs 
that 
included 
some 
training 
since 
without 
training 
the 
target 
page 
might 
not 
get 
as 
high 
of 
a 
score 
as 
it 
would 
with 
a 
trained 
network. 


Assuming 
that 
Wawa-IR 
nds 
a 
home 
page, 
Table 
11 
lists 
the 
average 
number 
of 
pages 
fetched 
before 
the 
actual 
home 
page. 
Learning 
reduces 
the 
number 
of 
pages 
fetched 
before 
the 
target 
page 
is 
found. 
This 
is 
quite 
intuitive. 
With 
more 
learning, 
Wawa-IR 
is 
able 
to 
classify 
pages 
better 
and 
nd 
the 
target 
page 
quicker. 
However, 
in 
Table 
11, 
the 
average 
number 
of 
pages 
fetched 



(before 
the 
target 
page 
is 
found) 
is 
lower 
with 
48 
advice 
rules 
than 
with 
76 
advice 
rules. 
For 
example, 
with 
SL, 
RL, 
and 
76 
rules, 
the 
average 
is 
22. 
With 
SL, 
RL, 
and 
48 
rules, 
the 
average 
is 
15. 
At 
rst 
glance, 
this 
might 
not 
seem 
intuitive. 
The 
reason 
for 
this 
discrepancy 
can 
be 
found 
in 
the 
28 
advice 
rules 
that 
I 
take 
out. 
Recall 
that 
these 
28 
advice 
rules 
improve 
the 
agent's 
accuracy 
by 
refering 
to 
words 
one 
might 
nd 
in 
a 
home 
page 
that 
are 
not 
the 
person's 
name 
(such 
as 
\resume", 
\cv", 
\phone", 
\address", 
\email", 
etc). 
With 
these 
rules, 
the 
ScorePage 
and 
ScoreLink 
networks 
rate 
more 
pages 
and 
links 
as 
\promising” 
even 
though 
they 
are 
not 
home 
pages. 
Hence, 
more 
pages 
are 
fetched 
and 
processed 
before 
the 
target 
page 
is 
found. 


Table 
11: 
Average 
Number 
of 
Pages 
Fetched 
by 
Wawa-IR 
Before 
the 
Target 
Home 
Page 
(SL 
= 
Supervised 
Learning 
and 
RL 
= 
Reinforcement 
Learning) 


SL 
RL 
# 
of 
Advice 
Rules 
Avg 
Pages 
Fetched 
Before 
Home 
Page 


?. 
76 
22 
. 
76 
23 
76 
31 
?. 
48 
15 
. 
48 
17 
48 
24 



5.4 
Results 
and 
Discussion 
from 
2001 
Table 
12 
compares 
the 
best 
performances 
of 
Wawa-IR's 
home-page 
nder 
seeded 
with 
and 
without 
Google 
to 
the 
results 
from 
Google 
(run 
by 
its
elf). 
The 
Wawa-IR 
run 
seeded 
without 
Google 
uses 
the 
following 
search 
engines: 
AltaVista, 
Excite, 
InfoSeek, 
Lycos, 
and 
Teoma. 
For 
the 
runs 
reported 
in 
Table 
12, 
I 
trained 
the 
Wawa-IR 
agent 
with 
reinforcement 
learning, 
supervised 
learning,7 
and 
all 
76 
home-page 
nding 
advice 
rules. 


7 
I 
used 
a 
target 
score 
of 
3.0 
for 
the 
pages 
that 
had 
the 
same 
host 
as 
the 
target 
home 
page. 



Table 
12: 
Two 
Dierent 
Wawa-IR 
Home-Page 
Finders 
versus 
Google 



System 
% 
Found 
Mean 
RankVariance 
Given 
Page 
Was 
Found 



Wawa-IR 
with 
Google 


96% 


1.12± 
0.15 


Google 


95% 


2.0116.64 


Wawa-IR 
without 
Google 


91% 


1.14± 
0.15 


Wawa-IR 
seeded 
with 
Google 
is 
able 
to 
slightly 
improve 
on 
Google's 
performance 
by 
nding 
96 
of 
the 
100 
pages 
in 
the 
test 
set. 
Wawa-IR 
seeded 
without 
Google 
is 
not 
able 
to 
nd 
more 
home 
pages 
than 
Google. 
This 
is 
due 
to 
the 
fact 
that 
the 
aggregate 
of 
the 
ve 
search 
engines 
used 
is 
not 
as 
accurate 
as 
Google. 
In 
particular, 
Google 
appears 
to 
be 
quite 
good 
at 
nding 
home 
pages 
due 
to 
its 
PageRank 
scoring 
function, 
which 
globally 
ranks 
a 
Web 
page 
based 
on 
its 
location 
in 
the 
Web's 
graph 
structure 
and 
not 
on 
the 
page's 
content 
(Brin 
and 
Page 
1998). 


It 
is 
interesting 
to 
note 
that 
Wawa-IR 
and 
Google 
only 
share 
one 
page 
among 
the 
list 
of 
pages 
that 
they 
both 
do 
not 
nd. 
The 
four 
pages 
that 
Wawa-
IR 
missed 
belong 
to 
people 
with 
very 
common 
names. 
For 
example, 
Google 
is 
able 
to 
return 
Charles 
\Chuck” 
Anderson's 
home 
page. 
But 
Wawa-IR 
lls 
its 
queue 
with 
home 
pages 
of 
other 
people 
or 
companies 
named 
Charles 
\Chuck” 
Anderson. 
8 
On 
the 
other 
hand, 
the 
pages 
that 
Google 
does 
not 
nd 
are 
the 
ones 
that 
Wawa-IR 
is 
able 
to 
discover 
by 
following 
links 
out 
of 
the 
original 
pages 
returned 
by 
Google. 


Wawa-IR 
runs 
seeded 
with 
and 
without 
Google 
have 
the 
advantage 
of 
having 
a 
lower 
mean 
rank 
and 
variance 
than 
Google 
(1:12 
± 
0:15 
and 


1:14 
± 
0:15, 
respectively, 
as 
opposed 
to 
Google's 
2:00 
± 
16:64). 
I 
attribute 
this 
dierence 
to 
Wawa-IR's 
learning 
ability, 
which 
is 
able 
to 
bump 
home 
pages 
to 
the 
top 
of 
the 
list. 
Finally, 
this 
set 
of 
experiments 
shows 
how 
Wawa-IR 
can 
8 
There 
is 
a 
car 
dealership 
with 
the 
url 
www.chuckanderson.com 
that 
gets 
a 
very 
high 
score. 



be 
used 
to 
personalize 
search 
engines 
by 
reorganizing 
the 
results 
they 
return 
as 
well 
as 
searching 
for 
nearby 
pages 
that 
score 
high. 


Table 
14 
compares 
the 
performances 
of 
Wawa-IR's 
home-page 
nder 
under 
all 
the 
combinations 
of 
these 
two 
dierent 
settings: 
(i) 
a 
change 
in 
the 
Q-
function 
used 
during 
reinforcement 
learning, 
and 
(ii) 
a 
change 
in 
the 
target 
score 
of 
pages 
that 
have 
the 
same 
host 
as 
the 
target 
home 
pages. 
The 
search 
engines 
used 
in 
these 
experiments 
are 
AltaVista, 
Excite, 
InfoSeek, 
Lycos, 
and 
Teoma. 
All 
the 
experiments 
reported 
in 
Table 
14 
use 
the 
76 
home-page 
nding 
advice 
rules 
described 
earlier 
in 
this 
chapter 
(See 
Appendix 
B 
for 
a 
complete 
listing 
of 
the 
rules). 
Table 
13 
denes 
the 
notations 
used 
in 
Table 
14. 


Table 
13: 
Notation 
for 
the 
2001 
Experiments 
Reported 
in 
Table 
14 


RL-WAWA-Q 
RL-STD-Q 
SL-3 
SL-7 


Reinforcement 
learning 
with 
Wawa-IR's 
Q-function 
Reinforcement 
learning 
with 
the 
standard 
Q-function 
Supervised 
learning 
with 
target 
host 
score 
= 
3.0 
Supervised 
learning 
with 
target 
host 
score 
= 
7.0 


Table 
14: 
Home-Page 
Finder 
Performances 
in 
2001 
under 
Dierent 
Wawa-IR 
Settings. 
See 
Table 
13 
for 
Denitions 
of 
the 
terms 
in 
the 
\Setting” 
column. 


Wawa-IR 
% 
Found 
Mean 
Rank 
Avg 
Pages 
Fetched 
Setting 
Given 
Page 
Found 
Before 
Home 
Page 


RL-WAWA-Q 
91% 
1:14 
± 
0:15 
22 
SL-3 
RL-STD-Q 
89% 
1:19 
± 
0:30 
28 
SL-3 
RL-WAWA-Q 
88% 
1:40 
± 
0:68 
23 
SL-7 
RL-STD-Q 
86% 
1:48 
± 
0:99 
29 
SL-7 



Wawa-IR 
performs 
better 
with 
its 
own 
beam-search 
Q-function 
than 
with 
the 
standard 
hill-climbing 
Q-function 
of 
reinforcement 
learning. 
For 
example, 
when 
employing 
its 
own 
Q-function 
(and 
setting 
its 
target 
host 
score 
to 
3), 



52 


Wawa-IR 
only 
needs 
to 
fetch 
an 
average 
of 
22 
pages 
before 
it 
nds 
the 
target 
home 
pages, 
as 
opposed 
to 
needing 
an 
average 
of 
28 
pages 
with 
the 
standard 
Q-function. 


The 
nal 
set 
of 
experiments 
shows 
the 
sensitivity 
of 
Wawa-IR's 
perform
ance 
to 
my 
chosen 
supervised-learning 
heuristic. 
With 
all 
other 
settings 
xed, 
Wawa-IR 
experiments 
with 
target 
host 
score 
of 
3 
nd 
more 
home 
pages 
than 
those 
with 
target 
host 
score 
of 
7. 
This 
result 
shows 
that 
for 
the 
ScorePage 
network, 
a 
target 
host 
page 
is 
just 
another 
page 
that 
is 
not 
the 
target 
page 
and 
should 
not 
be 
given 
such 
a 
high 
score. 


5.5 
Summary 
These 
experiments 
illustrate 
how 
the 
generic 
Wawa-IR 
system 
can 
be 
used 
to 
rapidly 
create 
an 
eective 
\home-page 
nder” 
agent. 
I 
believe 
that 
many 
other 
useful 
specialized 
IR 
agents 
can 
be 
easily 
created 
simply 
by 
providing 
task-
specic 
advice 
to 
the 
generic 
Wawa-IR 
system. 
In 
particular, 
the 
Google 
experiment 
shows 
how 
Wawa-IR 
can 
be 
used 
as 
a 
post-processor 
to 
Web 
search 
engines 
by 
learning 
to 
reorganize 
the 
search 
engines’ 
results 
(for 
a 
specic 
task) 
and 
searching 
for 
nearby 
pages 
that 
score 
high. 
In 
this 
manner, 
Wawa-IR 
can 
be 
trained 
to 
become 
a 
personalized 
search 
engine. 


One 
cost 
of 
using 
my 
approach 
is 
that 
I 
fetch 
and 
analyze 
many 
Web 
pages. 
I 
have 
not 
focused 
on 
speed 
in 
my 
experiments, 
ignoring 
such 
questions 
as 
how 
well 
can 
Wawa-IR's 
homepage 
nder 
perform 
when 
it 
only 
fetches 
the 
capsule 
summaries 
that 
search 
engines 
return, 
etc. 



Chapter 
6 


Using 
WAWA 
to 
Extract 
Information 
from 
Text 


Information 
extraction 
(IE) 
is 
the 
process 
of 
pulling 
desired 
pieces 
of 
information 
out 
of 
a 
document, 
such 
as 
the 
name 
of 
a 
disease 
or 
the 
location 
of 
a 
seminar 
(Lehnert 
2000). 
Unfortunately, 
building 
an 
IE 
system 
requires 
either 
a 
large 
number 
of 
annotated 
examples1 
or 
an 
expert 
to 
provide 
sucient 
and 
correct 
knowledge 
about 
the 
domain 
of 
interest. 
Both 
of 
these 
requirements 
make 
it 
time-consuming 
and 
dicult 
to 
build 
an 
IE 
system. 


Similar 
to 
the 
IR 
case, 
I 
use 
Wawa's 
theory-renement 
mechanism 
to 
build 
an 
IE 
system, 
namely 
Wawa-IE. 
2 
By 
using 
theory 
renement, 
I 
am 
able 
to 
strike 
an 
eective 
balance 
between 
needing 
a 
large 
number 
of 
labeled 
examples 
and 
having 
a 
complete 
and 
correct 
set 
of 
domain 
knowledge. 


Wawa-IE 
takes 
advantage 
of 
the 
intuition 
that 
specialized 
IR 
problems 
are 
nearly 
inverse 
of 
IE 
problems. 
The 
general 
IR 
task 
is 
nearly 
an 
inverse 
of 
the 
keyword/keyphrase 
extraction 
task, 
where 
the 
user 
in 
interested 
in 
a 
set 
of 
descriptive 
words 
or 
phrases 
describing 
a 
document. 
I 
illustrate 
this 
intuition 
with 
an 
example. 
Assume 
we 
have 
access 
to 
an 
accurate 
home-page 
nder, 
which 
takes 
as 
input 
a 
person's 
name 
and 
returns 
her 
home 
page. 
The 
inverse 
of 
such 
an 
IR 
system 
is 
an 
IE 
system 
that 
takes 
in 
home 
pages 
and 
returns 
the 
names 
of 
the 
people 
to 
whom 
the 
pages 
belong. 
By 
using 
a 
generate-and-test 


1 
By 
annotated 
examples, 
I 
mean 
the 
result 
of 
the 
tedious 
process 
of 
reading 
the 
training 
documents 
and 
tagging 
each 
extraction 
by 
hand. 


2 
Portions 
of 
this 
chapter 
were 
previously 
published 
in 
Eliassi-Rad 
and 
Shavlik 
(2001a, 
2001b). 



approach 
to 
information 
extraction, 
I 
am 
able 
to 
utilize 
what 
is 
essentially 
an 
IR 
system 
to 
address 
the 
IE 
task. 
In 
the 
generation 
step, 
the 
user 
rst 
species 
the 
slots 
to 
be 
lled 
(along 
with 
their 
part-of-speech 
tags 
or 
parse 
structures), 
then 
Wawa-IE 
generates 
a 
list 
of 
candidate 
extractions 
from 
the 
document. 
Each 
entry 
in 
this 
list 
of 
candidate 
extractions 
is 
one 
complete 
set 
of 
slot 
llers 
for 
the 
user-dened 
extraction 
template. 
In 
the 
test 
step, 
Wawa-IE 
scores 
each 
possible 
entry 
in 
the 
list 
of 
candidate 
extractions 
as 
if 
they 
were 
keyphrases 
given 
to 
an 
IR 
system. 
The 
candidates 
that 
produce 
scores 
that 
are 
greater 
than 
a 
Wawa-learned 
threshold 
are 
returned 
as 
the 
extracted 
information. 


Building 
an 
IR 
agent 
for 
the 
IE 
task 
is 
straightforward 
in 
Wawa. 
The 
user 
provides 
a 
set 
of 
advice 
rules 
to 
Wawa-IE, 
which 
describes 
how 
the 
system 
should 
score 
possible 
bindings 
to 
the 
slots 
being 
lled 
during 
the 
IE 
process. 
I 
will 
call 
the 
names 
of 
the 
slots 
to 
be 
lled 
variables, 
and 
use 
\binding 
a 
variable” 
as 
a 
synonym 
for 
\lling 
a 
slot.” 
These 
initial 
advice 
rules 
are 
then 
\compiled” 
into 
the 
ScorePage 
network, 
which 
rates 
the 
goodness 
of 
a 
document 
in 
the 
context 
of 
the 
given 
variable 
bindings. 
Recall 
that 
ScorePage 
is 
a 
supervised 
learner. 
It 
learns 
by 
being 
trained 
on 
user-provided 
instructions 
and 
user-labeled 
pages. 
The 
ScoreLink 
network 
is 
not 
used 
in 
Wawa-IE 
since 
the 
IE 
task 
is 
only 
concerned 
with 
extracting 
pieces 
of 
text 
from 
documents. 


Like 
its 
Wawa-IR 
agents, 
Wawa-IE 
agents 
do 
not 
blindly 
follow 
user's 
advice, 
but 
instead 
the 
agents 
rene 
the 
advice 
based 
on 
the 
training 
examples. 
The 
use 
of 
user-provided 
advice 
typically 
leads 
to 
higher 
accuracy 
from 
fewer 
user-provided 
training 
examples 
(see 
Chapter 
7). 


As 
already 
mentioned 
in 
Section 
3.2.2, 
of 
particular 
relevance 
to 
my 
approach 
is 
the 
fact 
that 
Wawa-IE's 
advice 
language 
contains 
variables. 
To 
understand 
how 
Wawa-IE 
uses 
variables, 
assume 
that 
I 
want 
to 
extract 
speaker 
names 
from 
a 
collection 
of 
seminar 
announcements. 
I 
might 
wish 
to 
give 
such 
a 
system 
some 
advice 
like: 


when 
(consecutive( 
Speaker 
· 
?Speaker 
) 
AND 
nounPhrase(?Speaker 
)) 
then 
show 
page 



The 
leading 
question 
marks 
indicate 
the 
slot 
to 
be 
lled, 
and 
`’ 
matches 
any 
single 
word. 
Also, 
recall 
that 
the 
advice 
language 
allows 
the 
user 
to 
specify 
the 
required 
part 
of 
speech 
tag 
or 
parse 
structure 
for 
a 
slot. 
For 
example, 
the 
predicate 
nounPhrase(?Speaker 
) 
is 
true 
only 
if 
the 
value 
bound 
to 
?Speaker 
is 
a 
noun 
phrase. 
The 
condition 
of 
my 
example 
rule 
matches 
phrases 
like 
\Speaker 
is 
Joe 
Smith” 
or 
\Speaker: 
is 
Jane 
Doe".3 


Figure 
14 
illustrates 
an 
example 
of 
extracting 
speaker 
names 
from 
a 
semin
ar 
announcement 
using 
Wawa-IE. 
The 
announcement 
is 
fed 
to 
the 
candidate 
generator 
and 
selector, 
which 
produces 
a 
list 
of 
speaker 
candidates. 
Each 
entry 
in 
the 
candidates 
list 
is 
then 
bound 
to 
the 
variable 
?Speaker 
in 
advice. 
The 
output 
of 
the 
(trained) 
network 
is 
a 
real 
number 
(in 
the 
interval 
of 
-10.0 
to 
10.0) 
that 
represents 
Wawa-IE's 
condence 
in 
the 
speaker 
candidate 
being 
a 
correct 
slot 
ller 
for 
the 
given 
document. 


CandidateoGeneratoro& SelectoroSeminar Announcement:oDon.t miss Jane Doe &BJohn Smith.s talk! DoeB& Smith will talk aboutBthe Turing tarpit. SeeByou at 4pm in 2310 CSBuilding.Bscore of .Jane Doe. = 9.0pSpeakerExtractoro 
?SpeakerpSpeaker Candidates:oJane DoepJohn SmithBDoeBSmithB...BGeneration StepTest Step
Figure 
14: 
Extraction 
of 
Speaker 
Names 
with 
Wawa-IE 


3 
Wawa's 
document-parser 
treats 
punctuation 
characters 
as 
individual 
tokens. 



56 


6.1 
IE 
System 
Description 
Wawa-IE 
uses 
a 
candidate 
generator 
and 
selector 
algorithm 
along 
with 
the 
ScorePage 
network 
to 
build 
IE 
agents. 
Table 
15 
provides 
a 
high-level 
des
cription 
of 
Wawa-IE. 


Table 
15: 
Wawa's 
Information-Extraction 
Algorithm 



1. 
Compile 
user's 
initial 
advice 
into 
the 
ScorePage 
network. 
2. 
Run 
the 
candidate 
generator 
and 
selector 
on 
the 
training 
set 
and 
by 
using 
the 
untrainted 
ScorePage 
network 
from 
step 
1 
nd 
negative 
training 
examples. 
3. 
Train 
the 
ScorePage 
network 
on 
the 
user-provided 
positive 
training 
examples 
and 
the 
negative 
training 
examples 
generated 
in 
step 
2. 
4. 
Use 
a 
tuning 
set 
to 
learn 
the 
threshold 
on 
the 
output 
of 
the 
ScorePage 
network. 
5. 
Run 
the 
candidate 
generator 
and 
selector 
on 
the 
test 
set 
to 
nd 
extraction 
candidates 
for 
the 
test 
documents. 
6. 
Using 
the 
trained 
ScorePage 
network 
(from 
step 
3), 
score 
each 
test-set 
extraction 
candidate 
(produced 
in 
step 
4). 
7. 
Report 
the 
test-set 
extraction 
candidates 
that 
score 
above 
a 
system-learned 
threshold 
to 
the 
user. 
To 
build 
and 
test 
an 
IE 
agent, 
Wawa-IE 
requires 
the 
user 
to 
provide 
the 
following 
information: 


• 
The 
set 
of 
on-line 
documents 
from 
which 
the 
information 
is 
to 
be 
ext
racted. 
• 
The 
extraction 
slots 
like 
speaker 
names, 
etc. 

• 
The 
possible 
part-of-speech 
(POS) 
tags 
(e.g., 
noun, 
proper 
noun, 
verb, 
etc) 
or 
parse 
structures 
(e.g., 
noun 
phrase, 
verb 
phrase, 
etc) 
for 
each 
extraction 
slot. 
• 
A 
set 
of 
advice 
rules 
which 
refer 
to 
the 
extraction 
slots 
as 
variables. 
• 
A 
set 
of 
annotated 
examples, 
i.e., 
training 
documents 
in 
which 
extraction 
slots 
have 
been 
marked. 
Actually, 
the 
user 
does 
not 
have 
to 
explicitly 
provide 
the 
extraction 
slots 
and 
their 
POS 
tags 
separately 
from 
advice 
since 
they 
can 
be 
automatically 
extracted 
from 
the 
advice 
rules. 


During 
training, 
Wawa-IE 
rst 
compiles 
the 
user's 
advice 
into 
the 
ScorePage 
network. 
Wawa-IE 
next 
uses 
what 
I 
call 
an 
individual-slot 
cand
idate 
generator 
and 
a 
combination-slots 
candidate 
selector 
to 
create 
training 
examples 
for 
the 
ScorePage 
network. 
The 
individual-slot 
candidate 
generat
or 
produces 
a 
list 
of 
candidate 
extractions 
for 
each 
slot 
in 
the 
IE 
task. 
The 
combination-slots 
candidate 
selector 
picks 
candidates 
from 
each 
list 
produced 
by 
the 
individual-slot 
candidate 
generator 
and 
combines 
them 
to 
produce 
a 
sing
le 
list 
of 
candidate 
extractions 
for 
the 
all 
the 
slots 
in 
the 
IE 
tasks. 
The 
same 
candidate 
generation 
and 
selection 
process 
is 
used 
after 
training 
to 
generate 
the 
possible 
extractions 
that 
the 
trained 
network4 


During 
testing, 
given 
a 
document 
from 
which 
I 
wish 
to 
extract 
information, 
I 
generate 
a 
large 
number 
of 
candidate 
bindings, 
and 
then 
in 
turn 
I 
provide 
each 
set 
of 
bindings 
to 
the 
trained 
network. 
The 
neural 
network 
produces 
a 
numeric 
output 
for 
each 
set 
of 
bindings. 
Finally, 
my 
extraction 
process 
returns 
the 
bindings 
that 
are 
greater 
than 
a 
system-learned 
threshold. 


4 
I 
use 
the 
terms 
\trained 
network” 
and 
\trained 
agent” 
interchangeably 
throughout 
Sect
ions 
6 
and 
7, 
since 
the 
network 
represents 
the 
agent's 
knowledge-base. 



6.2 
Candidate 
Generation 
The 
rst 
step 
Wawa-IE 
takes 
(both 
during 
training 
and 
after) 
is 
to 
generate 
all 
possible 
individual 
llers 
for 
each 
slot 
on 
a 
given 
document. 
These 
candid
ate 
llers 
can 
be 
individual 
words 
or 
phrases. 
Recall 
that 
in 
Wawa-IE, 
an 
extraction 
slot 
is 
represented 
by 
user-provided 
variables 
in 
the 
initial 
advice. 
Moreover, 
the 
user 
can 
provide 
syntactic 
information 
(e.g., 
part-of-speech 
tags) 
about 
the 
variables 
representing 
extraction 
slots. 
Wawa-IE 
uses 
a 
slot's 
synt
actic 
information 
along 
with 
either 
a 
part-of-speech 
(POS) 
tagger 
(Brill 
1994) 
or 
a 
sentence 
analyzer 
(Riloff 
1998) 
to 
collect 
the 
slot's 
candidate 
llers. 


For 
cases 
where 
the 
user 
specied 
POS 
tags5 
for 
aslot 
(i.e., 
noun, 
proper 
noun, 
verb, 
etc), 
I 
rst 
annotate 
each 
word 
in 
a 
document 
with 
its 
POS 
using 
Brill's 
tagger 
(1994). 
Then, 
for 
each 
slot, 
I 
collect 
every 
word 
in 
the 
document 
that 
has 
the 
same 
POS 
tag 
as 
the 
tag 
assigned 
to 
this 
variable 
at 
least 
once 
somewhere 
in 
the 
IE 
task's 
advice. 


If 
the 
user 
indicated 
a 
parse 
structure 
for 
a 
slot 
(i.e., 
noun 
phrase, 
verb 
phrase, 
etc), 
then 
I 
use 
Sundance 
(Riloff 
1998), 
which 
builds 
a 
shallow 
parse 
tree 
by 
segmenting 
sentences 
into 
noun, 
verb, 
or 
prepositional 
phrases. 
I 
then 
collect 
those 
phrases 
that 
match 
the 
parse 
structure 
for 
the 
extraction 
slot 
and 
also 
generate 
all 
possible 
subphrases 
of 
consecutive 
words 
(since 
Sundance 
only 
does 
shallow 
parsing). 


The 
user 
can 
provide 
both 
parse 
structures 
and 
POS 
tags 
for 
an 
extraction 
slot. 
In 
these 
cases, 
I 
run 
both 
Brill's 
tagger 
and 
Sundance 
sentence 
analyze 
(as 
described 
above) 
to 
get 
extraction 
candidates 
for 
the 
slot. 
I 
then 
merge 
and 
remove 
the 
duplicates 
in 
the 
lists 
provided 
by 
the 
two 
programs. 
In 
addition, 
the 
user 
can 
provide 
POS 
tags 
for 
some 
of 
the 
extraction 
slots 
and 
parse 
structures 
for 
others. 


5 
The 
POS 
tags 
provided 
by 
the 
user 
for 
an 
extraction 
slot 
can 
be 
any 
POS 
tag 
dened 
in 
Brill's 
tagger. 



59 


For 
example, 
suppose 
the 
user 
species 
that 
the 
extraction 
slot 
should 
cont
ain 
either 
a 
word 
tagged 
as 
a 
proper 
noun 
or 
two 
consecutive 
words 
both 
tagged 
as 
proper 
nouns. 
After 
using 
the 
Brill's 
tagger 
on 
the 
user-provided 
document, 
I 
then 
collect 
all 
the 
words 
that 
were 
tagged 
as 
proper 
nouns, 
in 
addition 
to 
every 
sequence 
of 
two 
words 
that 
were 
both 
tagged 
as 
proper 
nouns. 
So, 
if 
the 
phrase 
\Jane 
Doe” 
appeared 
on 
the 
document 
and 
the 
tagger 
marked 
both 
words 
as 
proper 
nouns, 
I 
would 
collect 
\Jane,” 
\Doe,” 
and 
\Jane 
Doe.” 


6.3 
Candidate 
Selection 
After 
the 
candidate 
generation 
step, 
Wawa-IE 
typically 
has 
lengthy 
lists 
of 
candidate 
llers 
for 
each 
slot, 
and 
needs 
to 
focus 
on 
selecting 
good 
combinations 
that 
ll 
all 
the 
slots. 
Obviously, 
this 
process 
can 
be 
combinatorially 
demanding 
especially 
during 
training 
of 
a 
Wawa-IE 
agent, 
where 
backpropagation 
learning 
occurs 
multiple 
times 
over 
the 
entire 
training 
set. 
To 
reduce 
this 
computational 
complexity, 
Wawa-IE 
contains 
several 
methods 
(called 
selectors) 
for 
creating 
complete 
assignments 
to 
the 
slots 
from 
the 
lists 
of 
individual 
slot 
bindings. 


Wawa-IE's 
selectors 
range 
from 
suboptimal 
and 
cheap 
(like 
simple 
random 
sampling 
from 
each 
individual 
list) 
to 
optimal 
and 
expensive 
(like 
exhaustively 
producing 
all 
possible 
combinations 
of 
the 
individual 
slot 
llers). 
Among 
its 
heuristically 
inclined 
selectors, 
Wawa-IE 
has: 
(i) 
a 
modied 
WalkSAT 
alg
orithm 
(Selman, 
Kautz, 
and 
Cohen 
1996), 
(ii) 
a 
modied 
GSAT 
algorithm 
(Selman, 
Kautz, 
and 
Cohen 
1996), 
(iii) 
a 
hill-climbing 
algorithm 
with 
random 
restarts 
(Russell 
and 
Norvig 
1995), 
(iv) 
a 
stochastic 
selector, 
and 
(v) 
a 
high-
scoring 
simple-random-sampling 
selector. 
Section 
7 
provides 
a 
detailed 
discuss
ion 
of 
the 
advantages 
and 
disadvantages 
of 
each 
selector 
within 
the 
context 
of 
my 
case 
studies. 


I 
included 
WalkSAT 
and 
GSAT 
algorithms 
into 
my 
set 
of 
selectors 
for 
two 
reasons. 
First, 
the 
task 
of 
selecting 
combination-slots 
candidates 
is 
analogous 
to 
the 
problem 
of 
nding 
assignment 
for 
conjunctive 
normal 
form 
(CNF) 
formulas. 



When 
selecting 
combination-slots 
candidates, 
I 
am 
looking 
for 
assignments 
that 
produce 
the 
highest 
score 
on 
the 
ScorePage 
network. 
Second, 
both 
WalkSAT 
and 
GSAT 
algorithms 
have 
been 
shown 
to 
be 
quite 
eective 
in 
nding 
assignm
ent 
for 
certain 
classes 
of 
CNF 
formulas 
(Selman, 
Kautz, 
and 
Cohen 
1996). 
The 
hill-climbing 
algorithm 
with 
random 
restarts 
was 
included 
into 
Wawa-IE's 
set 
of 
selectors 
because 
it 
has 
been 
shown 
to 
be 
an 
eective 
search 
algorithm 
(Russ
ell 
and 
Norvig 
1995). 
The 
stochastic 
selector 
was 
included 
because 
it 
utilizes 
the 
statistical 
distribution 
of 
extraction 
candidates 
as 
it 
pertains 
to 
their 
scores. 
The 
high-scoring 
simple-random-sampling 
selector 
was 
added 
since 
it 
is 
a 
very 
simple 
heuristic-search 
algorithm. 


Figure 
15 
describes 
my 
modied 
WalkSAT 
algorithm. 
Wawa-IE 
builds 
the 
list 
of 
combination-slots 
candidate 
extractions 
for 
a 
document 
by 
randomly 
sel
ecting 
an 
item 
from 
each 
extraction 
slot's 
list 
of 
individual-slot 
candidates. 
This 
produces 
a 
combination-slots 
candidate 
extraction 
that 
contains 
a 
candid
ate 
ller 
for 
each 
slot 
in 
the 
template. 
If 
the 
score 
produced 
by 
the 
ScorePage 
network 
is 
high 
enough 
(i.e., 
over 
a 
user-provided 
threshold) 
for 
this 
set 
of 
varia
ble 
bindings, 
then 
Wawa-IE 
adds 
this 
combination 
to 
the 
list 
of 
combination-
slots 
candidates. 
Otherwise, 
it 
repeatedly 
and 
randomly 
selects 
a 
slot 
in 
the 
template. 
Then, 
with 
probability 
p, 
Wawa-IE 
randomly 
selects 
a 
candidate 
for 
the 
selected 
slot 
and 
adds 
the 
resulting 
combination-slots 
candidate 
to 
the 
list 
of 
combination-slots 
candidates. 
With 
probability 
1-p, 
it 
iterates 
over 
all 
poss
ible 
candidates 
for 
this 
slot 
and 
adds 
the 
candidate 
that 
produces 
the 
highest 
network 
score 
for 
the 
document 
to 
the 
list 
of 
combination-slots 
candidates. 


Figure 
16 
describes 
my 
modied 
GSAT 
algorithm. 
This 
algorithm 
is 
quite 
similar 
to 
my 
modied 
WalkSAT 
algorithm. 
In 
fact, 
it 
is 
the 
WalkSAT 
algor
ithm 
with 
p 
= 
0. 
That 
is, 
if 
the 
score 
of 
the 
randomly-selected 
combination-
slots 
candidate 
is 
not 
high 
enough, 
this 
algorithm 
randomly 
picks 
a 
slot 
and 
tries 
to 
nd 
a 
candidate 
for 
the 
picked 
slot 
that 
produces 
the 
highest 
network 
score 
for 
the 
document. 


I 
make 
two 
modications 
to 
the 
standard 
WalkSAT 
and 
GSAT 
algorithms. 



Inputs: XMAX-TRIES, MAX-ALTERATIONS, p, MAX-CANDS,
doc, threshold, L (where L is the lists of individual-slotXcandidate extractions for doc)XOutput:XTL (where TL is the list of combination-slots candidatextractions 
of size MAX-CANDS)XAlgorithm:X1. TL := { }X2. for i:=1 to MAX-TRIESX 
XS := randomly selected combination-slots candidate from L.dif (score of ST 
w.r.t. doc is in [threshold, 10.0]), then add S to TL.Xotherwis. 
Xfor j:=1 to MAX-ALTERATIONSXs := Randomly select a slot in S to changWith 
probability p, randomly select a candidate for s.XAdd S to TL.XWith probability 1-p, select the first candidate for s thatXmaximizes the score of ST 
w.r.t. doc. Add S to TL.X3. Sort TL in decreasing order of score of its entries.X4. Return the top MAX-CANDS entries as TL.X

Figure 
15: 
Wawa-IE's 
Modied 
WalkSAT 
Algorithm 


First, 
the 
default 
WalkSAT 
and 
GSAT 
algorithms 
check 
to 
see 
if 
an 
assignment 
was 
found 
that 
satises 
the 
CNF 
formula. 
In 
my 
version, 
I 
check 
to 
see 
if 
the 
score 
of 
the 
ScorePage 
network 
is 
above 
a 
user-provided 
threshold.6 
Second, 
the 
default 
WalkSAT 
and 
GSAT 
algorithms 
return 
after 
nding 
one 
assignment 
that 
satises 
the 
CNF 
formula. 
In 
my 
version, 
I 
collect 
a 
list 
of 
candidates 
and 
return 
the 
top-scoring 
N 
candidates 
(where 
N 
is 
dened 
by 
the 
user). 


Figure 
17 
shows 
Wawa-IE's 
hill-climbing 
algorithm 
with 
random 
restarts. 
In 
this 
selector, 
Wawa-IE 
randomly 
selects 
a 
set 
of 
values 
for 
the 
combination-
slots 
extraction 
candidate. 
Then, 
it 
tries 
to 
\climb” 
towards 
the 
candidates 


6 
In 
my 
experiments, 
I 
used 
a 
threshold 
of 
9.0. 



Inputs: XMAX-TRIES, MAX-ALTERATIONS, p, MAX-CANDS,
doc, threshold, L (where L is the lists of individual-slotXcandidate extractions for doc)XOutput:XTL (where TL is the list of combination-slots candidatextractions 
of size MAX-CANDS)XAlgorithm:X1. TL := { }X2. for i:=1 to MAX-TRIESX 
XS := randomly selected combination-slots candidate from L.dif (score of ST 
w.r.t. doc is in [threshold, 10.0]), then add S to TL.Xotherwis. 
Xfor j:=1 to MAX-ALTERATIONSXs := Randomly select a slot in S to changSelect 
the first candidate for s that maximizes thscore 
of ST 
w.r.t. doc. Add S to TL.X3. Sort TL in decreasing order of score of its entries.X4. Return the top MAX-CANDS entries as TL.X

Figure 
16: 
Wawa-IE's 
Modied 
GSAT 
Algorithm 


that 
produce 
the 
high 
scores 
by 
comparing 
dierent 
assignments 
for 
each 
ext
raction 
slot.7 
When 
Wawa-IE 
cannot 
\climb” 
any 
higher 
or 
has 
\climbed” 
MAX-CLIMBS 
times, 
it 
restarts 
from 
another 
randomly 
chosen 
point 
in 
the 
space 
of 
combination-slots 
extraction 
candidates. 


The 
basic 
idea 
behind 
the 
next 
selector, 
namely 
stochastic 
selector, 
is 
to 
estimate 
the 
goodness 
(with 
respect 
to 
the 
output 
of 
the 
ScorePage 
network) 
of 
a 
candidate 
for 
a 
single 
slot 
by 
averaging 
over 
multiple 
random 
candidate 
bindings 
for 
the 
other 
slots 
in 
the 
extraction 
template. 
For 
example, 
what 
is 
the 
expected 
score 
of 
the 
protein 
candidate 
\LOS1” 
when 
the 
location 
candidate 
is 
randomly 
selected? 
Figure 
18 
describes 
Wawa-IE's 
stochastic 
selector. 
For 
each 
slot 
in 
the 
extraction 
template, 
Wawa-IE 
rst 
uniformly 
samples 
from 


7 
In 
this 
selector 
and 
the 
stochastic 
selector 
(described 
next), 
the 
score(S) 
function 
refers 
to 
the 
output 
of 
the 
ScorePage 
network 
for 
the 
combination-slots 
extraction 
candidate, 
S. 



Inputs: XMAX-TRIES, MAX-CLIMBS, MAX-CANDS, doc, L (where L is thlists 
of individual-slot candidate extractions for doc)X

Output. 
TL (where TL is the list of combination-slots candidatextractions 
of size MAX-CANDS. 


Algorithm:X

1. TL := { }X

2. for i:=1 to MAX-TRIESX

 L 
S := randomly selected combination-slots candidate from L. 
best_S = S. 
max_score := score of S w.r.t. doc. 
prev_score := max_score. 
for j:=1 to MAX-CLIMB1 
for all slots s in . 


max_cand(s) := the first candidate for s that maximizes the score of So 
w.r.t. doc.XS. := S with the candidate max_cand(s) filling slot s.o 
Xmax_score := max(max_score, score of S.) .Xif (max_score == score of S.) then best_S = S..X 
if (max_score == prev_score),Xthen add S to TL and break out of the inner loop.X 
otherwiseS := best_S.Xprev_score := max_score.o3. Sort TL in decreasing order of score of its entries.X

4. Return the top MAX-CANDS entries as TL.X

Figure 
17: 
Wawa-IE's 
Hill-Climbing 
Algorithm 
With 
Random 
Restarts 


the 
list 
of 
individual-slot 
candidates. 
The 
uniform 
selection 
allows 
Wawa-IE 


to 
accurately 
select 
an 
initial 
list 
of 
sampled 
candidates. 
That 
is, 
if 
a 
candidate 


occurs 
more 
than 
once 
on 
the 
page, 
then 
it 
should 
have 
a 
higher 
probability 


of 
getting 
picked 
for 
the 
sampled 
list 
of 
candidates. 
The 
size 
of 
this 
sample 
is 


determined 
by 
the 
user. 


Then, 
Wawa-IE 
attempts 
to 
estimate 
the 
probability 
of 
picking 
a 
candidate 


for 
each 
individual-slot 
candidate 
list 
and 
iteratively 
denes 
it 
to 
be 


scorek(ci)

Pk(ci 
will 
be 
picked) 
= 
. 
(6.1)

N 


j=1 
scorek(cj 
) 



where 
scorek(ci) 
is 
the 
mean 
score 
of 
candidate 
ci 
at 
the 
kth 
try 
and 
is 
dened 
as 


..M

( 
scorem(ci))+(scorepriormprior)

scorek(ci)= 
m=1 


n+mprior 


The 
function 
score(ci) 
is 
equal 
to 
the 
output 
of 
the 
ScorePage 
network, 
when 
candidate 
i 
is 
assigned 
to 
slot 
se 
and 
values 
for 
all 
the 
other 
slots 
are 
picked 
based 
on 
their 
probabilities.8 
M 
is 
the 
number 
of 
times 
candidate 
i 
was 
picked. 
N 
is 
the 
number 
of 
unique 
candidates 
in 
the 
initial 
sampled 
set 
for 
slot 
se 
and 
n 
is 
the 
total 
number 
of 
candidates 
in 
the 
initial 
sampled 
set 
for 
slot 
se. 
scoreprior 
is 
an 
estimate 
for 
the 
prior 
mean 
score 
and 
mprior 
is 
the 
equivalent 
sample 
size 
(Mitchell 
1997).9 
The 
input 
parameters 
MAX-SAMPLE 
and 
MAX-TIME-STEPS 
are 
dened 
by 
the 
user. 
They, 
respectively, 
determine 
the 
size 
of 
the 
list 
sampled 
initially 
from 
an 
individual 
slot's 
list 
of 
candidates 
and 
the 
number 
of 
times 
Equation 
6.1 
should 
be 
updated. 
Note 
that 
MAX-SAMPLE 
and 
MAX-TIME-STEPS 
are 
analogous 
to 
MAX-TRIES 
and 
MAX-ALTERATIONS 
in 
WawaI
E's 
modied 
walkSAT 
and 
GSAT 
algorithms. 


Wawa-IE's 
high-scoring 
simple-random-sampling 
selector 
randomly 
picks 
N 
combination-slots 
candidates 
from 
the 
lists 
of 
individual-slot 
candidates, 
where 
N 
is 
provided 
by 
the 
user. 
When 
training, 
it 
only 
uses 
the 
N 
combinations 
that 
produce 
the 
highest 
scores 
on 
the 
untrained 
ScorePage 
network.10 


Wawa-IE 
does 
not 
need 
to 
generate 
combinations 
of 
llers 
when 
the 
IE 
task 
contains 
a 
template 
with 
only 
one 
slot 
(as 
is 
the 
case 
in 
one 
of 
my 
case 
studies 
presented 
in 
Section 
7). 
However, 
it 
is 
desirable 
to 
trim 
the 
list 
of 
candidate 
llers 
during 
the 
training 
process 
because 
training 
is 
done 
iteratively. 
Therefore, 
Wawa-IE 
heuristically 
selects 
from 
a 
slot's 
list 
of 
training 
candidate 


8 
Score 
of 
a 
candidate 
is 
mapped 
into 
[0, 
1]. 


9 
In 
my 
experiments, 
scoreprior 
=0:75 
and 
mprior 
= 
10 
for 
words 
with 
prior 
knowledge 
(i.e., 
words 
that 
are 
labeled 
by 
the 
user 
as 
possibly 
relevant). 
For 
all 
other 
words, 
scoreprior 
= 


0:5 
and 
mprior 
= 
3. 
10 
By 
untrained, 
I 
mean 
a 
network 
containing 
only 
compiled 
(initial) 
advice 
and 
without 
any 
further 
training 
via 
backpropagation 
and 
labeled 
examples. 



Inputs: XMAX-SAMPLE, MAX-TIME-STEPS, MAX-CANDS, doc,XL (where L is the lists of individual-slot candidatextractions 
for doc)X

Output:XTL (where TL is the list of combination-slots candidatextractions 
of size MAX-CANDS)XAlgorithm:X1. TL := { }X2. for each slot se in the list of all slots provided by the userXSampled_Sete := { }Xfor i:=1 to MAX-SAMPLEXAppend to Sampled_Sete a uniformly selected candidate from se.Xfor k:=1 to MAX-TIME-STEPSX

for each candidate ci in Sampled_SetedCalculate the probability of picking ci at the kth attemptXusing Equation 6.1X

3. for j = 1 to MAX-CANDSXS := combination-slots candidate stochastically chosen according toXEquation 6.1XAdd S to TL.d4. Return the top MAX-CANDS entries as TL.X

Figure 
18: 
Wawa-IE's 
Stochastic 
Selector 


llers 
(i.e., 
the 
candidate 
llers 
associated 
with 
the 
training 
set) 
by 
scoring 
each 
candidate 
ller 
using 
the 
untrained 
ScorePage 
network11 
and 
returning 
the 
highest 
scoring 
candidates 
plus 
some 
randomly 
sampled 
candidates. 
This 
process 
of 
picking 
informative 
candidate 
llers 
from 
the 
training 
data 
has 
some 
benecial 
side 
eects, 
which 
are 
described 
in 
more 
detail 
in 
the 
next 
section. 


11 
By 
untrained, 
I 
mean 
a 
network 
containing 
only 
compiled 
(initial) 
advice 
and 
without 
any 
further 
training 
via 
backpropagation 
and 
labeled 
examples. 



6.4 
Training 
an 
IE 
Agent 
Figure 
19 
shows 
the 
process 
of 
building 
a 
trained 
IE 
agent. 
Since 
(usually) 
only 
positive 
training 
examples 
are 
provided 
in 
IE 
domains, 
I 
rst 
need 
to 
generate 
some 
negative 
training 
examples.12 
To 
this 
end, 
I 
use 
the 
candidate 
generator 
and 
selector 
described 
above. 
The 
user 
selects 
which 
selector 
she 
wants 
to 
use 
during 
training. 
The 
list 
of 
negative 
training 
examples 
collected 
by 
the 
user-picked 
selector 
contains 
informative 
negative 
examples 
(i.e., 
near 
misses) 
because 
the 
heuristic 
search 
used 
in 
the 
selector 
scores 
the 
training 
documents 
on 
the 
untrained 
ScorePage 
network. 
That 
is, 
the 
(user-provided) 
prior 
knowledge 
scored 
these 
\near 
miss” 
extractions 
highly 
(as 
if 
they 
were 
true 
extractions). 


After 
the 
N 
highest-scoring 
negative 
examples 
are 
collected, 
I 
train 
the 
ScorePage 
neural 
network 
using 
these 
negative 
examples 
and 
all 
the 
prov
ided 
positive 
examples. 
By 
training 
the 
network 
to 
recognize 
(i.e., 
produce 
a 
high 
output 
score 
for) 
a 
correct 
extraction 
in 
the 
context 
of 
the 
document 
as 
a 
whole 
(see 
Section 
3.4), 
I 
am 
able 
to 
take 
advantage 
of 
the 
global 
layout 
of 
the 
information 
available 
in 
the 
documents 
of 
interest. 


Since 
the 
ScorePage 
network 
outputs 
a 
real 
number, 
Wawa-IE 
needs 
to 
learn 
a 
threshold 
on 
this 
output 
such 
that 
the 
bindings 
for 
the 
scores 
above 
the 
threshold 
are 
returned 
to 
the 
user 
as 
extractions 
and 
the 
rest 
are 
discarded. 
Note 
that 
the 
value 
of 
the 
threshold 
can 
be 
used 
to 
manipulate 
the 
performance 
of 
the 
IE 
agent. 
For 
example, 
if 
the 
threshold 
is 
set 
to 
a 
high 
number 
(e.g., 
8.5), 
then 
the 
agent 
might 
miss 
a 
lot 
of 
the 
correct 
llers 
for 
a 
slot 
(i.e., 
have 
low 
recall), 
but 
the 
number 
of 
extracted 
llers 
that 
are 
correct 
should 
be 
higher 
(i.e., 
high 
precision). 
As 
previously 
dened 
in 
Chapter 
2, 
Recall 
(van 
Rijsbergen 
1979) 
is 
the 
ratio 
of 
the 
number 
of 
correct 
llers 
extracted 
to 
the 
total 
number 
of 
llers 
in 
correct 
extraction 
slots. 
Precision 
(van 
Rijsbergen 
1979) 
is 
the 
ratio 


12 
Wawa-IE 
needs 
negative 
training 
examples 
because 
it 
frames 
the 
IE 
task 
as 
a 
classicat
ion 
problem. 



TrainingoSetoInitialoAdviceoIndividual-SlotoCandidate Generatoro& Combination-SlotsoCandidate SelectoroSlots & TheiroPOS Tags oroParse StructuresoLists of CandidateoCombination-SlotsoExtractionsoScorePageoTrained network iso 
placed into agent.soknowledge-base.oTrainingoSetoInitialoAdviceoIndividual-SlotoCandidate Generatoro& Combination-SlotsoCandidate SelectoroSlots & TheiroPOS Tags oroParse StructuresoLists of CandidateoCombination-SlotsoExtractionsoScorePageoTrained network iso 
placed into agent.soknowledge-base.o
IE Agento

Figure 
19: 
Building 
a 
Trained 
IE 
agent 


of 
the 
number 
of 
correct 
llers 
extracted 
to 
the 
total 
number 
of 
llers 
extracted. 


To 
avoid 
overtting 
the 
ScorePage 
network 
and 
to 
nd 
the 
best 
threshold 
on 
its 
output 
after 
training 
is 
done, 
I 
actually 
divide 
the 
training 
set 
into 
two 
disjoint 
sets. 
One 
of 
the 
sets 
is 
used 
to 
train 
the 
ScorePage 
network. 
The 
other 
set, 
the 
tuning 
set, 
is 
rst 
used 
to 
\stop” 
the 
training 
of 
the 
ScorePage 
network. 
Specically, 
I 
cycle 
through 
the 
training 
examples 
100 
times. 
After 
each 
iteration 
over 
the 
training 
examples, 
I 
use 
the 
lists 
of 
candidate 
llers 
assoc
iated 
with 
the 
tuning 
set 
to 
evaluate 
the 
F1-measure 
produced 
by 
the 
network 
for 
various 
settings 
of 
the 
threshold. 
Recall 
that, 
the 
F1-measure 
combines 
pre-

2P 
recisionRecall 
13

cision 
and 
recall 
using 
the 
following 
formula: 
F1 
= 
. 
I 
pick 


P 
recision+Recall 
the 
network 
that 
produced 
the 
highest 
F1-measure 
on 
my 
tuning 
set 
as 
my 
nal 
trained 
network. 


13 
The 
F1 
-measure 
(van 
Rijsbergen 
1979) 
is 
used 
regularly 
to 
compare 
the 
performances 
of 
IR 
and 
IE 
systems 
because 
it 
weights 
precision 
and 
recall 
equally 
and 
produces 
one 
single 
number. 



68 


I 
utilize 
the 
tuning 
set 
(a 
second 
time) 
to 
nd 
the 
optimal 
threshold 
on 
the 
output 
of 
the 
trained 
ScorePage 
network. 
Specically, 
I 
perform 
the 
following: 


• 
For 
each 
threshold 
value, 
t, 
from 
-10.0 
to 
10.0 
with 
increments 
of 
inc, 
do 
– 
Run 
the 
tuning 
set 
through 
the 
trained 
ScorePage 
network 
to 
nd 
the 
F1-measure 
(for 
the 
threshold 
t). 
• 
Set 
the 
optimal 
threshold 
to 
the 
threshold 
associated 
with 
the 
maximum 
F1-measure. 
The 
value 
of 
the 
increment, 
inc, 
is 
also 
dened 
during 
tuning. 
The 
initial 
value 
of 
inc 
is 
0.25. 
Then, 
if 
the 
variation 
among 
the 
F1-measures 
calculated 
on 
the 
tuning 
set 
is 
small 
(i.e., 
less 
than 
0.05 
and 
F1-measure 
. 
[0,1]), 
I 
reduce 
the 
inc 
by 
0.05. 
Note 
that, 
the 
smaller 
inc 
is, 
the 
more 
accurately 
the 
trained 
ScorePage 
can 
be 
evaluated 
(because 
the 
more 
sensitive 
it 
is 
to 
the 
score 
of 
a 
candidate 
extraction). 


6.5 
Testing 
a 
Trained 
IE 
Agent 
Figure 
20 
depicts 
the 
steps 
a 
trained 
IE 
agent 
takes 
to 
produce 
extractions. 
For 
each 
entry 
in 
the 
list 
of 
combination-slots 
extraction 
candidates, 
Wawa-IE 
rst 
binds 
the 
variables 
to 
their 
candidate 
values. 
Then, 
it 
performs 
a 
forward 
propagation 
on 
the 
trained 
ScorePage 
network 
and 
outputs 
the 
score 
of 
the 
network 
for 
the 
test 
document 
based 
on 
the 
candidate 
bindings. 
If 
the 
output 
value 
of 
the 
network 
is 
greater 
than 
the 
threshold 
dened 
during 
the 
tuning 
step, 
Wawa-IE 
records 
the 
bindings 
as 
an 
extraction. 
Otherwise, 
these 
bindings 
are 
discarded. 



Slots & TheiroPOS Tags oroParse StructuresoTest Seto(Docs UnseenoDuring Training)oTrained IE AgentoIndividual-SlotoCandidate Generator& Combination-SlotsoCandidate SelectorExtractionsoLists of CandidateoCombination-SlotsoExtractionsoSlots & TheiroPOS Tags oroParse StructuresoTest Seto(Docs UnseenoDuring Training)oTrained IE AgentoIndividual-SlotoCandidate Generator& Combination-SlotsoCandidate SelectorExtractionsoLists of CandidateoCombination-SlotsoExtractionso
Figure 
20: 
Testing 
a 
Trained 
IE 
Agent 


6.6 
Summary 
A 
novel 
aspect 
of 
Wawa-IE 
is 
its 
exploitation 
of 
the 
relationship 
between 
IR 
and 
IE. 
That 
is, 
I 
build 
IR 
agents 
that 
treat 
possible 
extractions 
as 
keywords, 
which 
are 
in 
turn 
judged 
within 
the 
context 
of 
the 
entire 
document. 


The 
use 
of 
theory 
renement 
allows 
me 
to 
take 
advantage 
of 
user's 
prior 
knowledge, 
which 
need 
not 
be 
perfectly 
correct 
since 
Wawa-IE 
is 
a 
learning 
system. 
This, 
in 
turn, 
reduces 
the 
need 
for 
labeled 
examples, 
which 
are 
very 
expensive 
to 
get 
in 
the 
IE 
task. 
Also, 
compiling 
users’ 
prior 
knowledge 
into 
the 
ScorePage 
network 
provides 
a 
good 
method 
for 
nding 
informative 
negative 
training 
examples 
(i.e., 
near 
misses). 


One 
cost 
of 
using 
my 
approach 
is 
that 
I 
require 
the 
user 
to 
provide 
the 
POS 
tags 
or 
parse 
structures 
of 
the 
extraction 
slots. 
I 
currently 
assume 
that 
Brill's 
tagger 
and 
Sundance 
are 
perfect 
(i.e., 
they 
tag 
words 
and 
parse 
sentences 
with 
100% 
accuracy). 
Brill's 
tagger 
annotates 
the 
words 
on 
a 
document 
with 
97.2% 
accuracy 
(Brill 
1994), 
so 
2.8% 
error 
rate 
propagates 
into 
my 
results. 
I 
was 
not 
able 
to 
nd 
accuracy 
estimates 
for 
Sundance. 



My 
approach 
is 
computationally 
demanding, 
due 
to 
its 
use 
of 
a 
generate-andt
est 
approach. 
But, 
CPU 
cycles 
are 
abundant, 
and 
our 
experiments 
presented 
in 
Chapter 
7 
show 
that 
Wawa-IE 
still 
performs 
well 
when 
only 
using 
a 
subset 
of 
all 
possible 
combinations 
of 
slot 
llers. 



Chapter 
7 


Extraction 
Experiments 
with 
WAWA 


This 
chapter1 
presents 
three 
case 
studies 
involving 
Wawa-IE. 
The 
rst 
information-extraction 
(IE) 
task 
involves 
extracting 
speaker 
and 
location 
names 
from 
a 
collection 
of 
seminar 
announcements 
(Freitag 
1998b). 
This 
task 
has 
been 
widely 
used 
in 
the 
literature 
and 
allows 
me 
to 
directly 
compare 
the 
perf
ormance 
of 
Wawa-IE 
to 
several 
existing 
systems. 
For 
this 
domain, 
I 
follow 
existing 
methodology 
and 
independently 
extract 
speaker 
and 
location 
names, 
because 
each 
document 
is 
assumed 
to 
contain 
only 
one 
announcement. 
That 
is, 
for 
each 
announcement, 
I 
do 
not 
try 
to 
pair 
up 
speakers 
and 
locations, 
instead 
I 
return 
a 
list 
of 
speakers 
and 
a 
separate 
list 
of 
locations. 
Hence, 
I 
do 
not 
use 
any 
\combination-slots 
selector” 
on 
this 
task. 


The 
second 
and 
third 
IE 
tasks 
involve 
extracting 
information 
from 
abstracts 
of 
biomedical 
articles 
(Ray 
and 
Craven 
2001). 
In 
the 
second 
IE 
task, 
protein 
names 
and 
their 
locations 
within 
the 
cell 
are 
to 
be 
extracted 
from 
a 
collection 
of 
abstracts 
on 
yeast. 
In 
the 
third 
IE 
task, 
genes 
names 
and 
the 
diseases 
associated 
with 
them 
are 
to 
be 
extracted 
from 
a 
collection 
of 
abstracts 
on 
human 
disorders. 
I 
chose 
these 
two 
domains 
because 
they 
illustrate 
a 
harder 
IE 
task 
than 
the 
rst 
domain. 
In 
these 
domains, 
the 
llers 
for 
extraction 
slots 
depend 
on 
each 
other 
because 
a 
single 
abstract 
can 
contain 
multiple 
llers 
for 
a 
slot. 
Hence 
for 
each 
document, 
a 
single 
list 
of 
<protein, 
location> 
pairs 
is 
extracted 
for 
the 
second 


1 
Portions 
of 
this 
chapter 
were 
previously 
published 
in 
Eliassi-Rad 
and 
Shavlik 
(2001a, 
2001b). 



72 


domain 
and 
a 
single 
list 
of 
<gene, 
disease> 
pairs 
is 
extracted 
for 
the 
third 
domain. 


7.1 
CMU 
Seminar-Announcement 
Domain 
This 
section 
describes 
experimental 
results 
that 
I 
performed 
to 
test 
Wawa-IE 
on 
the 
CMU 
seminar-announcements 
domain 
(Freitag 
1998b). 
This 
domain 
consists 
of 
485 
documents. 
The 
complete 
task 
is 
to 
extract 
the 
start 
time, 
end 
time, 
speaker, 
and 
location 
from 
an 
announcement. 
However, 
I 
only 
report 
on 
results 
for 
extracting 
speaker 
and 
location. 
I 
omit 
start 
and 
end 
times 
from 
my 
experiments 
since 
almost 
all 
systems 
perform 
very 
well 
on 
these 
slots. 


Seminar 
announcements 
are 
tagged 
using 
Brill's 
part-of-speech 
tagger 
(1994) 
and 
common 
(\stop") 
words 
are 
discarded 
(Belew 
2000). 
I 
did 
not 
stem 
the 
words 
in 
this 
study 
since 
converting 
words 
to 
their 
base 
forms 
removes 
inform
ation 
that 
would 
be 
useful 
in 
the 
extraction 
process. 
For 
example, 
I 
do 
not 
want 
the 
word 
\Professors” 
to 
stem 
to 
the 
word 
\Professor", 
since 
I 
want 
to 
give 
the 
IE 
agent 
the 
opportunity 
to 
learn 
that 
usually 
more 
than 
one 
speaker 
name 
appears 
after 
the 
word 
\Professors.” 


Experimental 
Methodology 


I 
compare 
Wawa-IE 
to 
seven 
other 
information 
extraction 
systems 
using 
the 
CMU 
seminar 
announcements 
domain 
(Freitag 
1998b). 
These 
systems 
are 
HMM 
(Freitag 
and 
McCallum 
1999), 
BWI 
(Freitag 
and 
Kushmerick 
2000), 
SRV 
(Frei
tag 
1998b), 
Naive 
Bayes 
(Freitag 
1998b), 
WHISK 
(Soderland 
1999), 
RAPIER 
(Califf 
1998), 
and 
RAPIER-WT 
(Califf 
1998). 
None 
of 
these 
systems 
exploits 
prior 
knowledge. 
Except 
for 
Naive 
Bayes, 
HMM, 
and 
BWI, 
the 
rest 
of 
the 
syst
ems 
use 
relational 
learning 
algorithms 
(Muggleton 
1995). 
RAPIER-WT 
is 
a 
variant 
of 
RAPIER 
where 
information 
about 
semantic 
classes 
is 
not 
utilized. 
HMM 
(Freitag 
and 
McCallum 
1999) 
employs 
a 
hidden 
Markov 
model 
to 
learn 



about 
extraction 
slots. 
BWI 
(Freitag 
and 
Kushmerick 
2000) 
combines 
wrapper 
induction 
techniques 
(Kushmerick 
2000) 
with 
AdaBoost 
(Schapire 
and 
Singer 
1998) 
to 
solve 
the 
IE 
task. 


Freitag 
(1998b) 
rst 
randomly 
divided 
the 
485 
documents 
in 
the 
seminar 
announcements 
domain 
into 
ten 
splits, 
and 
then 
randomly 
divided 
each 
of 
the 
ten 
splits 
into 
approximately 
240 
training 
examples 
and 
240 
testing 
examples. 
Except 
for 
WHISK, 
the 
results 
of 
the 
other 
systems 
are 
all 
based 
on 
the 
same 
10 
data 
splits. 
The 
results 
for 
WHISK 
are 
from 
a 
single 
trial 
with 
285 
documents 
in 
the 
training 
set 
and 
200 
documents 
in 
the 
testing 
set. 


I 
give 
Wawa-IE 
nine 
and 
ten 
advice 
rules 
in 
Backus-Naur 
Form, 
BNF, 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
notation 
about 
speakers 
and 
locations, 
respectively 
(see 
Appendix 
C). 
I 
wrote 
none 
of 
these 
advice 
rules 
with 
the 
specics 
of 
the 
CMU 
seminar 
announcements 
in 
mind. 
The 
rules 
describe 
my 
prior 
knowledge 
about 
what 
might 
be 
a 
speaker 
or 
a 
location 
in 
a 
general 
seminar 
announcement. 
It 
took 
me 
about 
half 
a 
day 
to 
write 
these 
rules 
and 
I 
did 
not 
manually 
rene 
these 
rules 
over 
time.2 
. 


For 
this 
case 
study, 
I 
choose 
to 
create 
the 
same 
number 
of 
negative 
training 
examples 
(for 
speaker 
and 
location 
independently) 
as 
the 
number 
of 
positive 
examples. 
I 
choose 
95% 
of 
the 
negatives, 
from 
the 
complete 
list 
of 
possibilities, 
by 
collecting 
those 
that 
score 
the 
highest 
on 
the 
untrained 
ScorePage 
network; 
the 
remaining 
5% 
are 
chosen 
randomly 
from 
the 
complete 
list. 


For 
this 
domain, 
I 
used 
four 
variables 
to 
learn 
about 
speaker 
names 
and 
four 
variables 
to 
learn 
about 
location 
names. 
The 
four 
variables 
for 
speaker 
names 
refer 
to 
rst 
names, 
nicknames, 
middle 
names 
(or 
initials), 
and 
last 
names, 
respectively. 
The 
four 
variables 
for 
location 
refer 
to 
a 
cardinal 
number 
(namely 
?LocNumber) 
and 
three 
other 
variables 
representing 
the 
non-numerical 
portions 
of 
a 
location 
phrase 
(namely, 
?LocName1, 
?LocName2, 
and 
?LocName3 
). 
For 
example, 
in 
the 
phrase 
\1210 
West 
Dayton,” 
?LocNumber, 
?LocName1, 
and 


2 
I 
also 
wrote 
a 
set 
of 
rules 
for 
this 
domain. 
The 
rules 
are 
lists 
in 
Section2 
of 
Appendix 
D. 
The 
results 
are 
reported 
in 
Eliassi-Rad 
and 
Shavlik 
(2001b) 



?LocName2 
get 
bound 
to 
1210, 
\West,” 
and 
\Dayton” 
respectively. 


Table 
16 
shows 
four 
rules 
used 
in 
the 
domain 
theories 
of 
speaker 
and 
locat
ion 
slots. 
Rule 
SR1 
matches 
phrases 
of 
length 
three 
that 
start 
with 
the 
word 
\Professor” 
and 
have 
two 
proper 
nouns 
for 
the 
remaining 
words.3 
In 
rule 
SR2, 
I 
am 
looking 
for 
phrases 
of 
length 
four 
where 
the 
rst 
word 
is 
\speaker,” 
foll
owed 
by 
another 
word 
which 
I 
do 
not 
care 
about, 
and 
trailed 
by 
two 
proper 
nouns. 
SR2 
matches 
phrases 
like 
\Speaker 
: 
Joe 
Smith” 
or 
\speaker 
is 
Jane 
Doe.” 
Rules 
LR1 
and 
LR2 
match 
phrases 
such 
as 
\Room 
2310 
CS.” 
LR2 
diers 
from 
LR1 
in 
that 
it 
requires 
the 
two 
words 
following 
\room” 
to 
be 
a 
cardinal 
number 
and 
a 
proper 
noun, 
respectively 
(i.e., 
LR2 
is 
a 
subset 
of 
LR1). 
Since 
I 
am 
more 
condent 
that 
phrases 
matching 
LR2 
describe 
locations, 
LR2 
sends 
a 
higher 
weight 
to 
the 
output 
unit 
of 
the 
ScorePage 
network 
than 
does 
LR1. 


Table 
16: 
Sample 
Rules 
Used 
in 
the 
Domain 
Theories 
of 
Speaker 
and 
Location 
Slots 



SR1 
When 
\Professor 
?FirstName/NNP 
?LastName/NNP” 
then 
strongly 
suggest 
showing 
page 



SR2 
When 
\Speaker 
. 
?FirstName/NNP 
?LastName/NNP” 
then 
strongly 
suggest 
showing 
page 


LR1 
When 
\Room 
?LocNumber 
?LocName” 
then 
suggest 
showing 
page 


LR2 
When 
\Room 
?LocNumber/CD 
?LocName/NNP” 
then 
strongly 
suggest 
showing 
page 


Tables 
17 
and 
18 
show 
the 
results 
of 
the 
trained 
Wawa-IE 
agent 
and 
the 
other 
seven 
systems 
for 
the 
speaker 
and 
location 
slots, 
respectively.4 
The 
results 
reported 
are 
the 
averaged 
precision, 
recall, 
and 
F1 
values 
across 
the 
ten 
splits. 
The 
precision, 
recall, 
and 
F1-measure 
for 
a 
split 
are 
determined 
by 
the 
optimal 
threshold 
found 
for 
that 
split 
using 
the 
tuning 
set 
(see 
Section 
6.4 
for 
further 


3 
When 
matching 
preconditions 
of 
rules, 
case 
of 
words 
does 
not 
matter. 
For 
example, 
both 
\Professor 
alan 
turing” 
and 
\Professor 
Alan 
Turning” 
will 
match 
rule 
SR1's 
precondition. 


4 
Due 
to 
the 
lack 
of 
statistical 
information 
on 
the 
other 
methods, 
I 
cannot 
statistically 
measure 
the 
signicance 
of 
the 
dierences 
in 
the 
algorithms. 



details). 
For 
all 
ten 
splits, 
the 
optimal 
thresholds 
on 
Wawa-IE's 
untrained 
agent 
are 
5.0 
for 
the 
speaker 
slot 
and 
9.0 
for 
the 
location 
slot. 
The 
optimal 
threshold 
on 
Wawa-IE's 
trained 
agent 
varies 
from 
one 
split 
to 
the 
next 
in 
both 
the 
speaker 
slot 
and 
the 
location 
slot. 
For 
the 
speaker 
slot, 
the 
optimal 
thresholds 
on 
Wawa-IE's 
trained 
agent 
vary 
from 
0.25 
to 
2.25. 
For 
the 
location 
slot, 
the 
optimal 
thresholds 
on 
Wawa-IE's 
trained 
agent 
range 
from 
-6.25 
to 


0.75. 
Since 
the 
speaker's 
name 
and 
the 
location 
of 
the 
seminar 
may 
appear 
in 
multiple 
forms 
in 
an 
announcement, 
an 
extraction 
is 
considered 
correct 
as 
long 
as 
any 
one 
of 
the 
possible 
correct 
forms 
is 
extracted. 
For 
example, 
if 
the 
speaker 
is 
\John 
Doe 
Smith", 
the 
words 
\Smith", 
\Joe 
Smith", 
\John 
Doe 
Smith", 
\J. 
Smith", 
and 
\J. 
D. 
Smith” 
might 
appear 
in 
a 
document. 
Any 
one 
of 
these 
extractions 
is 
considered 
correct. 
This 
method 
of 
marking 
correct 
extractions 
is 
also 
used 
in 
the 
other 
IE 
systems 
against 
which 
I 
compare 
my 
approach. 


I 
use 
precision, 
recall, 
and 
the 
F1-measure 
to 
compare 
the 
dierent 
systems 
(see 
Section 
6.4 
for 
denitions 
of 
these 
terms). 
Recall 
that 
an 
ideal 
system 
has 
precision 
and 
recall 
of 
100%. 


Table 
17: 
Results 
on 
the 
Speaker 
Slot 
for 
Seminar 
Announcements 
Task 



System 


Precision 


Recall 


F1 



HMM 


77.9 


75.2 


76.6 


Wawa-IE's 
Trained 
Agent 


61.5 


86.6 


71.8 


BWI 


79.1 


59.2 


67.7 


SRV 


54.4 


58.4 


56.3 


RAPIER-WT 


79.0 


40.0 


53.1 


RAPIER 


80.9 


39.4 


53.0 


Wawa-IE's 
Untrained 
Agent 


29.5 


96.8 


45.2 


Naive 
Bayes 


36.1 


25.6 


30.0 


WHISK 


71.0 


15.0 


24.8 


The 
F1-measure 
is 
more 
versatile 
than 
either 
precision 
or 
recall 
for 
explaining 
relative 
performance 
of 
dierent 
systems, 
since 
it 
takes 
into 
account 
the 
inherent 



Table 
18: 
Results 
on 
the 
Location 
Slot 
for 
Seminar 
Announcements 
Task 



System 


Precision 


Recall 


F1 



Wawa-IE's 
Trained 
Agent 


73.9 


84.4 


78.8 


HMM 


83.0 


74.6 


78.6 


BWI 


85.4 


69.6 


76.7 


RAPIER-WT 


91.0 


61.5 


73.4 


SRV 


74.5 


70.1 


72.3 


RAPIER 


91.0 


60.5 


72.7 


WHISK 


93.0 


59.0 


72.2 


RAPIER-W 


90.0 


54.8 


68.1 


Naive 
Bayes 


59.6 


58.8 


59.2 


Wawa-IE's 
Untrained 
Agent 


29.2 


98.2 


45.0 


tradeoff 
that 
exists 
between 
precision 
and 
recall. 
For 
both 
speaker 
and 
location 
slots, 
the 
F1-measure 
of 
Wawa-IE 
is 
considerably 
higher 
than 
Naive 
Bayes 
and 
all 
the 
relational 
learners. 
Wawa-IE's 
trained 
agents 
perform 
competitively 
with 
the 
BWI 
and 
HMM 
learners. 
The 
F1-measures 
on 
Wawa-IE's 
trained 
agents 
are 
high 
because 
I 
generate 
many 
extraction 
candidates 
in 
my 
generate-
and-test 
model. 
Hence, 
the 
trained 
agents 
are 
able 
to 
extract 
a 
lot 
of 
the 
correct 
llers 
from 
the 
data 
set, 
which 
in 
turn 
leads 
to 
higher 
recall 
than 
the 
other 
systems. 


After 
training, 
Wawa-IE 
is 
able 
to 
reject 
enough 
candidates 
so 
that 
it 
obt
ains 
reasonable 
precision. 
Figure 
21 
illustrates 
this 
fact 
for 
the 
speaker 
slot. 
A 
point 
in 
this 
graph 
represents 
the 
averaged 
precision 
and 
recall 
values 
at 
a 
specic 
network 
output 
across 
the 
ten 
splits. 
There 
are 
38 
points 
on 
each 
curve 
in 
Figure 
21 
representing 
the 
network 
outputs 
from 
0.0 
to 
9.0 
with 
increm
ents 
of 
0.25. 
The 
trained 
agent 
generates 
much 
better 
precision 
scores 
than 
the 
untrained 
agent. 
This 
increase 
in 
performance 
from 
Wawa-IE's 
untrained 
agent 
to 
Wawa-IE's 
trained 
agent 
shows 
that 
my 
agent 
is 
not 
\hard-wired” 
to 
perform 
well 
on 
this 
domain 
and 
that 
training 
helped 
my 
performance. 



Trained WAWA-IE Agento

Precisiono

8. 


6. 


4. 


Untrained WAWA-IE Agento
Recall

2. 
4. 
6. 
8010. 


Figure 
21: 
Wawa-IE's 
Speaker 
Extractor: 
Precision 
& 
Recall 
Curves 


Finally, 
I 
should 
note 
that 
several 
of 
the 
other 
systems 
have 
higher 
precision, 
so 
depending 
on 
the 
user's 
tradeoff 
between 
recall 
and 
precision, 
one 
would 
prefer 
dierent 
systems 
on 
this 
testbed. 


7.2 
Biomedical 
Domains 
This 
sections 
presents 
two 
experimental 
studies 
done 
on 
biomedical 
domains. 
Ray 
and 
Craven 
created 
both 
of 
these 
data 
sets 
(2001). 
In 
the 
rst 
domain, 
the 
task 
is 
to 
extract 
protein 
names 
and 
their 
locations 
on 
the 
cell. 
In 
the 
second 
domain, 
the 
task 
is 
to 
extract 
genes 
and 
the 
genetic 
disorders 
with 
which 
they 
are 
associated. 


Subcellular-Localization 
Domain 


For 
my 
second 
experiment, 
the 
task 
is 
to 
extract 
protein 
names 
and 
their 
locat
ions 
on 
the 
cell 
from 
Ray 
and 
Craven's 
subcellular-localization 
data 
set 
(2001). 



Their 
extraction 
template 
is 
called 
the 
subcellular-localization 
relation. 
They 
created 
their 
data 
set 
by 
rst 
collecting 
target 
instances 
of 
the 
subcellular-
localization 
relation 
from 
the 
Yeast 
Protein 
Database 
(YPD) 
Web 
site. 
Then, 
they 
collected 
abstracts 
from 
articles 
in 
the 
MEDLINE 
database 
(NLM 
2001) 
that 
have 
references 
to 
the 
entries 
selected 
from 
YPD. 


In 
the 
subcellular-localization 
data 
set, 
each 
training 
and 
test 
instance 
is 
an 
individual 
sentence. 
A 
positive 
sentence 
is 
labeled 
with 
target 
tuples 
(where 
a 
tuple 
is 
an 
instance 
of 
the 
subcellular-localization 
relation). 
There 
are 
545 
positive 
sentences 
containing 
645 
tuples, 
of 
which 
335 
are 
unique. 
A 
negative 
sentence 
is 
not 
labeled 
with 
any 
tuples. 
There 
are 
6,700 
negative 
sentences 
in 
this 
data 
set. 
Note 
that 
a 
sentence 
that 
does 
not 
contain 
both 
a 
protein 
and 
its 
subcellular 
location 
is 
considered 
to 
be 
negative. 


Wawa-IE 
is 
given 
12 
advice 
rules 
in 
BNF 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
notation 
about 
a 
protein 
and 
its 
subcellular 
location 
(see 
Appendix 
D 
for 
a 
complete 
list 
of 
rules). 
Michael 
Waddell, 
who 
is 
an 
MD/PhD 
student 
at 
the 
University 
of 
Wisconsin-Madison, 
wrote 
these 
advice 
rules 
for 
me. 
Moreover, 
I 
did 
not 
manually 
rene 
these 
rules 
over 
time. 


Ray 
and 
Craven 
(2001) 
split 
the 
subcellular-localization 
data 
set 
into 
ve 
disjoint 
sets 
and 
ran 
ve-fold 
cross-validation. 
I 
use 
the 
same 
folds 
with 
Wawa-
IE 
and 
compare 
my 
results 
to 
theirs. 


Figure 
22 
compares 
the 
following 
systems 
as 
measured 
by 
precision 
and 
recall 
curves: 
(i) 
Wawa-IE's 
agent 
with 
no 
selector 
during 
training, 
(ii) 
Wawa-IE's 
agent 
with 
stochastic 
selector 
picking 
50% 
of 
all 
possible 
negative 
combination-
slots 
candidates 
during 
the 
training 
phase, 
and 
(iv) 
Ray 
and 
Craven's 
system 
(2001). 
In 
the 
Wawa-IE 
runs, 
I 
used 
all 
the 
positive 
training 
examples 
and 
100% 
of 
all 
possible 
test-set 
combination-slots 
candidates. 


The 
trained 
IE 
agent 
without 
any 
selector 
algorithm 
produces 
the 
best 
res
ults. 
But, 
it 
is 
computationally 
expensive 
since 
it 
needs 
to 
take 
the 
cross-
product 
of 
all 
entries 
in 
the 
lists 
of 
individual-slot 
candidates. 
The 
trained 
IE 



agents 
with 
the 
stochastic 
selector 
(picking 
50% 
of 
the 
possible 
train-set 
negat
ive 
combination-slots 
candidates) 
performs 
quite 
well, 
outperforming 
Ray 
and 
Craven's 
system. 


Precisiono

0.8o0.6o0.4o0.2o

No SelectoroStochasticoSelectoroRay & Craveno(2001)o

0o0.2o0.4o0.6o0.8o1oRecallo

Figure 
22: 
Subcellular-Localization 
Domain: 
Precision 
& 
Recall 
Curves 
for 
Ray 
and 
Craven's 
System 
(2001), 
Wawa-IE 
runs 
with 
no 
selector, 
with 
the 
stochastic 
selector 
sampling 
50% 
of 
the 
possible 
negative 
training 
tuples. 


OMIM 
Disorder-Association 
Domain 


For 
my 
third 
experiment, 
the 
task 
is 
to 
extract 
gene 
names 
and 
their 
genetic 
disorders 
from 
Ray 
and 
Craven's 
disorder-association 
data 
set 
(2001). 
Their 
ext
raction 
template 
is 
called 
the 
disorder-association 
relation. 
They 
created 
their 
data 
set 
by 
rst 
collecting 
target 
instances 
of 
the 
disorder-association 
relation 
from 
the 
Online 
Mendelian 
Inheritance 
in 
Man 
(OMIM) 
database 
(Center 
for 
Medical 
Genetics, 
2001). 
Then, 
they 
collected 
abstracts 
from 
articles 
in 
the 
MEDLINE 
database 
(NLM 
2001) 
that 
have 
references 
to 
the 
entries 
selected 
from 
OMIM. 


In 
the 
disorder-association 
data 
set, 
each 
training 
and 
test 
instance 
is 
an 
individual 
sentence. 
A 
positive 
sentence 
is 
labeled 
with 
target 
tuples 
(where 
a 
tuple 
is 
an 
instance 
of 
the 
disorder-association 
relation). 
There 
are 
892 
positive 
sentences 
containing 
899 
tuples, 
of 
which 
126 
are 
unique. 
A 
negative 
sentence 



is 
not 
labeled 
with 
any 
tuples. 
There 
are 
11,487 
negative 
sentences 
in 
this 
data 
set. 
Note 
that 
a 
sentence 
that 
does 
not 
contain 
both 
a 
gene 
and 
its 
genetic 
disorder 
is 
considered 
to 
be 
negative. 


Wawa-IE 
is 
given 
14 
advice 
rules 
in 
BNF 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
notation 
about 
a 
gene 
and 
its 
genetic 
disorder 
(see 
Appendix 
E 
for 
a 
complete 
list 
of 
rules). 
Michael 
Waddell, 
who 
is 
an 
MD/PhD 
student 
at 
the 
University 
of 
Wisconsin-Madison, 
wrote 
these 
advice 
rules 
for 
me. 
Moreover, 
I 
did 
not 
manually 
rene 
these 
rules 
over 
time. 


Ray 
and 
Craven 
(2001) 
split 
the 
subcellular-localization 
data 
set 
into 
ve 
disjoint 
sets 
and 
ran 
ve-fold 
cross-validation. 
I 
use 
the 
same 
folds 
with 
Wawa-
IE 
and 
compare 
my 
results 
to 
theirs. 


Figure 
23 
compares 
Wawa-IE's 
agent 
with 
no 
selector 
and 
stochastic 
sel
ector 
to 
that 
of 
Ray 
and 
Craven's 
(2001) 
as 
measured 
by 
precision 
and 
recall 
curves 
on 
the 
ve 
test 
sets. 
In 
these 
runs, 
I 
used 
all 
the 
positive 
training 
exa
mples 
and 
50% 
of 
the 
negative 
training 
examples. 
The 
stochastic 
selector 
was 
used 
to 
pick 
the 
negative 
examples. 
The 
trained 
IE 
agent 
without 
any 
selector 
algorithm 
is 
competitive 
with 
Ray 
and 
Craven's 
system. 
Wawa-IE 
is 
able 
to 
out-perform 
Ray 
and 
Craven's 
system 
after 
40% 
recall. 
The 
trained 
IE 
agent 
with 
stochastic 
selector 
(picking 
50% 
of 
all 
possible 
negative 
train-set 
combination-slots 
candidates) 
performs 
competitively 
to 
Ray 
and 
Craven's 
at 
around 
60% 
recall. 
In 
this 
domain, 
the 
trained 
IE 
agent 
with 
stochastic 
selector 
cannot 
reach 
the 
precision 
level 
achieved 
by 
Ray 
and 
Craven's 
system. 


Ray 
and 
Craven 
(2001) 
observed 
that 
their 
system 
would 
get 
better 
recall 
if 
the 
sample 
number 
of 
negative 
sentences 
were 
used 
as 
the 
number 
of 
positive 
exa
mples 
during 
training. 
Since 
I 
used 
their 
exact 
folds 
of 
training 
data 
in 
testing 
Wawa-IE, 
this 
means 
that 
the 
number 
of 
training 
sentences 
is 
approximately 
1110 
and 
1800 
in 
the 
yeast 
and 
OMIM 
domains, 
respectively. 


The 
methodology 
used 
in 
the 
next 
two 
sections 
is 
slightly 
dierent 
than 
the 
the 
one 
described 
in 
this 
section. 
I 
made 
this 
modication 
to 
make 
Wawa-IE 
run 
faster. 
Basically, 
I 
consider 
a 
larger 
list 
of 
positive 
tuples 
than 
the 
one 
used 



Precisiono

0.8o0.6o0.4o0.2o

Ray & Craveno(2001)oStochasticoNo SelectoroSelectoro

0o0.2o0.4o0.6o0.8o1oRecallo

Figure 
23: 
Disorder-Association 
Domain: 
Precision 
& 
Recall 
Curves 
for 
Ray 
and 
Craven's 
system 
(2001), 
Wawa-IE 
runs 
with 
no 
selector, 
with 
the 
stochastic 
selector 
sampling 
50% 
of 
the 
possible 
negative 
training 
tuples. 


in 
this 
section's 
experiments. 
For 
example, 
the 
tuples 
\hLOS1, 
nucleii” 
and 
\hLOS1 
protein, 
nucleii” 
are 
thought 
of 
as 
one 
tuple 
in 
the 
experiments 
in 
this 
section 
and 
as 
two 
tuples 
in 
the 
experiments 
reported 
next. 


7.3 
Reducing 
the 
Computational 
Burden 
This 
section 
reports 
on 
Wawa-IE's 
performance 
when 
all 
of 
the 
system-
generated 
negative 
examples 
are 
not 
utilized. 
These 
experiments 
investigate 
whether 
Wawa-IE 
can 
intelligently 
select 
good 
negative 
training 
examples 
and 
hence 
reduce 
the 
computational 
burden 
(during 
the 
training 
process) 
by 
comp
aring 
test 
set 
F1-measures 
to 
the 
case 
where 
all 
possible 
negative 
training 
examples 
are 
used. 


Subcellular-Localization 
Domain 


Figure 
24 
illustrates 
the 
dierence 
in 
F1-measure 
(from 
a 
ve-fold 
cross-
validation 
experiment) 
between 
choosing 
no 
selector 
and 
Section 
6.3's 
stochast
ic, 
hill 
climbing, 
GSAT, 
WalkSAT, 
and 
uniform 
selectors. 
The 
horizontal 
axis 



depicts 
the 
percentage 
of 
negative 
training 
examples 
used 
during 
the 
learning 
process 
(100% 
of 
negative 
training 
examples 
is 
approximately 
53,000 
tuples), 
and 
the 
vertical 
axis 
depicts 
the 
F1-measure 
of 
the 
trained 
IE-agent 
on 
the 
test 
set. 
In 
these 
runs, 
I 
used 
all 
of 
the 
positive 
training 
examples. 


It 
is 
not 
a 
surprise 
that 
the 
trained 
IE 
agent 
without 
any 
selector 
algorithm 
produces 
the 
best 
results. 
But 
as 
mentioned 
before, 
it 
is 
computationally 
quite 
expensive, 
especially 
for 
tasks 
with 
many 
extraction 
slots. 
The 
best-performing 
selector 
was 
my 
stochastic 
selector, 
followed 
by 
the 
hill-climbing 
approach 
with 
multiple 
restarts. 
GSAT 
edged 
WalkSAT 
slightly 
in 
performance. 
The 
uniform 
selector 
has 
the 
worst 
performance. 
The 
stochastic 
selector 
out-performs 
all 
other 
selectors 
since 
it 
adaptively 
selects 
candidates 
based 
on 
the 
probability 
of 
how 
high 
they 
will 
score. 
As 
expected, 
Wawa-IE 
improves 
its 
F1-measure 
on 
the 
test 
set 
(for 
all 
selectors) 
when 
given 
more 
negative 
training 
examples. 


Test Set F1-measureM

60M30M

0%M20%M40%M60%M80%M100%M


No SelectorM

Stochastic SelectorM

Hill Climbing withMultiple RandomMStartsM

GSATMWalkSATM

Uniform SelectorM

Percentage of Negative Training ExamplesM

Figure 
24: 
Subcellular-Localization 
Domain: 
F1-measure 
versus 
Percentage 
of 
Negative 
Training 
Candidates 
Used 
for 
Dierent 
Selector 
Algorithms 


OMIM 
Disorder-Association 
Domain 


Figure 
25 
illustrates 
the 
dierence 
in 
F1-measure, 
again 
from 
a 
ve-fold 
cross-
validation 
experiment, 
between 
choosing 
no 
selector 
and 
Section 
6.3's 
stochastic, 



hill 
climbing, 
GSAT, 
WalkSAT, 
and 
uniform 
selectors 
for 
the 
OMIM 
disorder-
association 
domain. 
The 
horizontal 
axis 
depicts 
the 
percentage 
of 
negative 
training 
examples 
used 
during 
the 
learning 
process 
(100% 
of 
negative 
training 
examples 
is 
approximately 
245,000 
tuples), 
and 
the 
vertical 
axis 
depicts 
the 
F1-measure 
of 
the 
trained 
IE-agent 
on 
the 
test 
set. 
In 
these 
runs, 
I 
used 
all 
of 
the 
positive 
training 
examples. 


Again, 
the 
trained 
IE 
agent 
without 
any 
selector 
algorithm 
produces 
the 
best 
results. 
The 
best-performing 
selector 
was 
my 
stochastic 
selector, 
followed 
by 
the 
hill-climbing 
approach 
with 
multiple 
restarts, 
GSAT, 
WalkSAT, 
and 
the 
uniform 
selector. 
Again, 
the 
stochastic 
selector 
out-performs 
all 
other 
selector 
since 
it 
selects 
candidates 
based 
on 
the 
probability 
of 
how 
high 
they 
will 
score. 
Moreover, 
as 
expected, 
Wawa-IE 
improves 
its 
F1-measure 
on 
the 
test 
set 
(for 
all 
selectors) 
when 
given 
more 
negative 
training 
examples. 


Test Set F1-measureM

60M30M

0%M20%M40%M60%M80%M100%MUniform SelectorMWalkSATMGSATMStochastic SelectorMNo SelectorMHill Climbing withMultiple RandomMStartsM
Percentage of Negative Training ExamplesM

Figure 
25: 
Disorder-Association 
Domain: 
F1-measure 
versus 
Percentage 
of 
Nega
tive 
Training 
Candidates 
Used 
for 
Dierent 
Selector 
Algorithms 


7.4 
Scaling 
Experiments 
This 
section 
reports 
on 
Wawa-IE's 
performance 
when 
all 
of 
the 
positive 
training 
instances 
are 
not 
available 
or 
the 
user 
provides 
fewer 
advice 
rules. 



Subcellular-Localization 
Domain 


Figure 
26 
demonstrates 
Wawa-IE's 
ability 
to 
learn 
when 
positive 
training 
exa
mples 
are 
sparse. 
The 
horizontal 
axis 
shows 
the 
number 
of 
positive 
training 
instances, 
and 
the 
vertical 
axis 
depicts 
the 
F1-measure 
of 
the 
trained 
IE-agent 
averaged 
over 
the 
ve 
test 
sets. 
The 
markers 
on 
the 
horizontal 
axis 
correspond 
to 
10%, 
25%, 
50%, 
75%, 
100% 
of 
positive 
training 
instances, 
respectively. 
I 
meas
ured 
Wawa-IE's 
performance 
on 
an 
agent 
with 
no 
selector. 
That 
is, 
all 
poss
ible 
training 
and 
testing 
candidates 
were 
generated. 
Wawa-IE's 
performance 
degrades 
smoothly 
as 
the 
number 
of 
positive 
training 
examples 
decreases. 
This 
curve 
suggests 
that 
more 
training 
data 
will 
improve 
the 
quality 
of 
the 
results 
(since 
the 
curve 
is 
still 
rising). 


071072073074075076070780790707557137273740975457Test Set F1-measure7
Number of Positive Training Instances7

Figure 
26: 
Subcellular-Localization 
Domain: 
F1-measure 
versus 
Number 
of 
Posi
tive 
Training 
Instances. 
The 
curve 
represents 
Wawa-IE's 
performance 
without 
a 
selector 
during 
training 
and 
testing. 


Figure 
27 
shows 
how 
Wawa-IE 
performs 
on 
the 
test 
set 
F1-measure 
with 
dierent 
types 
and 
number 
of 
advice 
rules. 
There 
are 
only 
two 
rules 
in 
group 


A. 
They 
include 
just 
mentioning 
the 
variables 
that 
represent 
the 
extraction 
slots 
in 
the 
template. 
There 
are 
ve 
rules 
in 
group 
B. 
None 
of 
these 
rules 
refer 
to 
both 
the 
protein 
and 
location 
in 
one 
rule. 
In 
other 
words, 
there 
is 
no 
initial 
advice 
relating 
proteins 
and 
their 
locations. 
There 
are 
12 
rules 
in 

advice 
of 
type 
C. 
They 
include 
rules 
from 
groups 
A 
and 
B, 
plus 
seven 
more 
rules 
giving 
information 
about 
proteins 
and 
locations 
together 
(see 
Appendix 
D 
for 
the 
full 
set 
of 
rules 
used 
in 
this 
experiment). 
I 
used 
all 
the 
positive 
and 
system-generated 
negative 
training 
examples 
for 
this 
experiment. 
Wawa-IE 
with 
no 
selector 
during 
training 
and 
testing 
is 
able 
to 
learn 
quite 
well 
with 
very 
minimal 
advice, 
which 
shows 
that 
the 
advice 
rules 
do 
not 
have 
\hard-wired” 
in 
them 
the 
correct 
answers 
to 
the 
extraction 
task. 


Test Set F1-measureB

60B40B30B

ACB


(2 rules)B(5 rules)B(12 rules)BDifferent Groups of Advice RulesB

Figure 
27: 
F1-measure 
versus 
Dierent 
Groups 
of 
Advice 
Rules 
on 
WAWA-IE 
with 
no 
Selector. 
Groups 
A, 
B, 
and 
C 
have 
2, 
5, 
and 
12 
rules, 
respectively. 
The 
curve 
represnts 
Wawa-IE's 
performance 
without 
a 
selector 
during 
training 
and 
testing. 


OMIM 
Disorder-Association 
Domain 


Figure 
28 
demonstrates 
Wawa-IE's 
ability 
to 
learn 
when 
positive 
training 
exa
mples 
are 
sparse. 
The 
horizontal 
axis 
shows 
the 
number 
of 
positive 
training 
instances, 
and 
the 
vertical 
axis 
depicts 
the 
F1-measure 
of 
the 
trained 
IE-agent 
averaged 
over 
ve 
test 
sets. 
The 
markers 
on 
the 
horizontal 
axis 
correspond 
to 
10%, 
25%, 
50%, 
75%, 
100% 
of 
positive 
training 
instances, 
respectively. 
I 
meas
ured 
Wawa-IE's 
performance 
on 
an 
agent 
with 
no 
selector 
during 
training 
and 
testing. 
Wawa-IE's 
performance 
without 
a 
selector 
degrades 
smoothly 
as 
the 



number 
of 
positive 
training 
examples 
decreases. 
Similar 
to 
the 
curve 
for 
the 
subcellular-localization 
domain, 
this 
curve 
suggests 
that 
more 
labeled 
data 
will 
improve 
the 
quality 
of 
results 
(since 
the 
curve 
is 
still 
steeply 
rising). 


0710720730740750760707079072237446766978927Test Set F1-measure7
Number of Positive Training Instances7

Figure 
28: 
Disorder-Association 
Domain: 
F1-measure 
versus 
Number 
of 
Positive 
Training 
Instances. 
The 
curve 
represents 
Wawa-IE's 
performance 
without 
a 
selector 
during 
training 
and 
testing. 


Figure 
29 
shows 
how 
Wawa-IE 
performs 
on 
the 
test 
set 
F1-measure, 
avera
ged 
across 
the 
ve 
test 
sets, 
with 
dierent 
types 
and 
number 
of 
advice 
rules. 
There 
are 
only 
two 
rules 
in 
group 
A. 
They 
include 
just 
mentioning 
the 
variables 
that 
represent 
the 
extraction 
slots 
in 
the 
template. 
There 
are 
eight 
rules 
in 
group 
B. 
None 
of 
these 
rules 
refer 
to 
both 
genes 
and 
their 
genetic 
disorders 
in 
one 
rule. 
In 
other 
words, 
there 
is 
no 
initial 
advice 
relating 
genes 
and 
their 
gen
etic 
disorders. 
There 
are 
14 
rules 
in 
advice 
of 
type 
C. 
They 
include 
rules 
from 
groups 
A 
and 
B, 
plus 
six 
more 
rules 
giving 
information 
about 
genes 
and 
their 
disorders 
together 
(see 
Appendix 
E 
for 
the 
full 
set 
of 
rules 
used 
in 
this 
experim
ent). 
I 
used 
all 
the 
positive 
and 
system-generated 
negative 
training 
examples 
for 
this 
experiment. 
As 
was 
the 
case 
in 
the 
subcellular-localization 
domain, 
Wawa-IE 
is 
able 
to 
learn 
quite 
well 
with 
very 
minimal 
advice, 
which 
shows 
that 
the 
advice 
rules 
do 
not 
have 
\hard-wired” 
in 
them 
the 
correct 
answers 
to 
the 
extraction 
task. 



Test Set F1-measureB

60B40B30B


ACB(2 rules)B(8 rules)B(14 rules)B

Different Groups of Advice RulesB

Figure 
29: 
Disorder-Association 
Domain: 
F1-measure 
versus 
Dierent 
Groups 
of 
Advice 
Rules 
on 
WAWA-IE 
with 
no 
Selector. 
Groups 
A, 
B, 
and 
C 
have 
2, 
8, 
and 
14 
rules, 
respectively. 
The 
curve 
represents 
Wawa-IE's 
performance 
without 
a 
selector 
during 
training 
and 
testing. 


7.5 
Summary 
The 
main 
reason 
Wawa-IE 
performs 
so 
well 
is 
because 
(i) 
it 
has 
a 
recall 
bias, 


(ii) 
it 
is 
able 
to 
generate 
informative 
negative 
training 
examples 
(which 
are 
extremely 
important 
in 
the 
IE 
task 
since 
there 
are 
a 
lot 
of 
near 
misses 
in 
this 
task), 
and 
(iii) 
using 
prior 
domain 
knowledge, 
positive 
training 
examples, 
and 
its 
system-generated 
negative 
examples, 
it 
is 
able 
to 
improve 
on 
its 
precision 
after 
the 
learning 
process. 

Chapter 
8 


Related 
Work 


Wawa 
is 
closely 
related 
to 
several 
areas 
of 
research. 
The 
rst 
is 
information 
retrieval 
and 
text 
categorization, 
the 
second 
is 
instructable 
software, 
and 
the 
third 
is 
information 
extraction. 
The 
following 
sections 
summarize 
the 
work 
previously 
done 
in 
each 
of 
these 
areas 
and 
relate 
it 
to 
the 
research 
presented 
in 
this 
dissertation. 


8.1 
Learning 
to 
Retrieve 
from 
the 
Web 
Wawa-IE, 
Syskill 
and 
Webert 
(Pazzani, 
Muramatsu, 
and 
Billsus 
1996), 
and 
WebWatcher 
(Joachims, 
Freitag, 
and 
Mitchell 
1997) 
are 
Web 
agents 
that 
use 
machine 
learning 
techniques. 
Syskill 
and 
Webert 
uses 
a 
Bayesian 
classier 
to 
learn 
about 
interesting 
Web 
pages 
and 
hyperlinks. 
WebWatcher 
employs 
a 
reinf
orcement 
learning 
and 
TFIDF 
hybrid 
to 
learn 
from 
the 
Web. 
Unlike 
Wawa-IR, 
these 
systems 
are 
unable 
to 
accept 
(and 
rene) 
advice, 
which 
usually 
is 
simple 
to 
provide 
and 
can 
lead 
to 
better 
learning 
than 
manually 
labeling 
many 
Web 
pages. 


Drummond 
et 
al. 
(1995) 
have 
created 
a 
system 
which 
assists 
users 
browsi
ng 
software 
libraries. 
Their 
system 
learns 
unobtrusively 
by 
observing 
users’ 
actions. 
Letizia 
(Lieberman 
1995) 
is 
a 
system 
similar 
to 
Drummond 
et 
al.'s 
that 
uses 
lookahead 
search 
from 
the 
current 
location 
in 
the 
user's 
Web 
browser. 
Compared 
to 
Wawa-IR, 
Drummond's 
system 
and 
Letizia 
are 
at 
a 
disadvantage 
since 
they 
cannot 
take 
advantage 
of 
advice 
given 
by 
the 
user. 


WebFoot 
(Soderland 
1997) 
is 
a 
system 
similar 
to 
Wawa-IR, 
which 
uses 



89 


HTML 
page-layout 
information 
to 
divide 
a 
Web 
page 
into 
segments 
of 
text. 
Wawa-IR 
uses 
these 
segments 
to 
extract 
input 
features 
for 
its 
neural 
networks 
and 
create 
an 
expressive 
advice 
language. 
WebFoot, 
on 
the 
other 
hand, 
utilizes 
these 
segments 
to 
extract 
information 
from 
Web 
pages. 
Also, 
unlike 
Wawa-IR, 
WebFoot 
only 
learns 
via 
supervised 
learning. 


CORA 
(McCallum, 
Nigam, 
Rennie, 
and 
Seymore 
1999; 
McCallum, 
Nigam, 
Rennie, 
and 
Seymore 
2000) 
is 
a 
domain-specic 
search 
engine 
on 
computer 
scie
nce 
research 
papers. 
Like 
Wawa-IR, 
it 
uses 
reinforcement-learning 
techniques 
to 
eciently 
spider 
the 
Web 
(Rennie 
and 
McCallum 
1999; 
McCallum, 
Nigam, 
Rennie, 
and 
Seymore 
2000). 
CORA's 
reinforcement 
learner 
is 
trained 
o-line 
on 
a 
set 
of 
documents 
and 
hyperlinks 
which 
enables 
its 
Q-function 
to 
be 
learned 
via 
dynamic 
programming, 
since 
both 
the 
reward 
function 
and 
the 
state 
transition 
function 
are 
known. 
Wawa-IR's 
training, 
on 
the 
other 
hand, 
is 
done 
on-line. 
Wawa-IR 
uses 
temporal-dierence 
methods 
to 
evaluate 
the 
reward 
of 
following 
a 
hyperlink. 
In 
addition, 
Wawa-IR's 
reinforcement-learner 
automatically 
gene
rates 
its 
own 
training 
examples 
and 
is 
able 
to 
accept 
and 
rene 
user's 
advice. 
CORA's 
reinforcement-learner 
is 
unable 
to 
perform 
either 
of 
these 
two 
actions. 
To 
classify 
text, 
CORA 
uses 
naive 
Bayes 
in 
combination 
with 
the 
EM 
algor
ithm 
(Dempster, 
Laird, 
and 
Rubin 
1977), 
and 
a 
statistical 
technique 
named 
\shrinkage” 
(McCallum 
and 
Nigam 
1998; 
McCallum, 
Rosenfeld, 
Mitchell, 
and 
Ng 
1998). 
Again, 
unlike 
Wawa-IR, 
CORA's 
text 
classier 
learns 
only 
through 
training 
examples 
and 
cannot 
accept 
and 
rene 
advice. 


8.2 
Instructable 
Software 
Wawa 
is 
closely 
related 
to 
RATLE 
(Maclin 
1995). 
In 
RATLE, 
a 
teacher 
cont
inuously 
gives 
advice 
to 
an 
agent 
using 
a 
simple 
programming 
language. 
The 
advice 
species 
actions 
an 
agent 
should 
take 
under 
certain 
conditions. 
The 
agent 
learns 
by 
using 
connectionist 
reinforcement-learning 
techniques. 
In 
emp
irical 
results, 
RATLE 
outperformed 
agents 
which 
either 
do 
not 
accept 
advice 



or 
do 
not 
rene 
the 
advice. 


Gordon 
and 
Subramanian 
(1994) 
use 
genetic 
search 
and 
high-level 
advice 
to 
rene 
prior 
knowledge. 
An 
advice 
rule 
in 
their 
language 
species 
a 
goal 
which 
will 
be 
achieved 
if 
certain 
conditions 
are 
satised. 


Diederich 
(1989) 
and 
Abu-Mostafa 
(1995) 
generate 
examples 
from 
prior 
knowledge 
and 
mix 
these 
examples 
with 
the 
training 
examples. 
In 
this 
way, 
they 
indirectly 
provide 
advice 
to 
the 
neural 
network. 
This 
method 
of 
giving 
advice 
is 
restricted 
to 
prior 
knowledge 
(i.e., 
it 
is 
not 
continually 
provided). 


Botta 
and 
Piola 
(1999) 
describe 
a 
connectionist 
algorithm 
for 
rening 
num
erical 
constants 
expressed 
in 
rst-order 
logic 
rules. 
The 
initial 
values 
for 
the 
numerical 
constants 
are 
determined 
such 
that 
the 
prediction 
error 
of 
the 
knowle
dge 
base 
on 
the 
training 
set 
is 
minimized. 
Predicates 
containing 
numerical 
constants 
are 
translated 
into 
continuous 
functions, 
which 
are 
tuned 
by 
using 
the 
error 
gradient 
descent 
(Rumelhart 
and 
McClelland 
1986). 
The 
advantage 
of 
their 
system, 
called 
the 
Numerical 
Term 
Rener 
(NTR), 
is 
that 
the 
classical 
logic 
semantics 
of 
the 
rules 
is 
preserved. 
The 
weakness 
of 
their 
system 
is 
that, 
other 
than 
manipulating 
the 
numerical 
constants 
such 
that 
a 
predicate 
is 
always 
false 
or 
a 
literal 
from 
a 
clause 
is 
always 
true, 
they 
are 
not 
able 
to 
change 
the 
structure 
of 
the 
original 
knowledge 
base 
by 
incrementally 
adding 
new 
hidden 
units. 


Fab 
(Balabanovic 
and 
Shoham 
1997) 
is 
a 
recommendation 
system 
for 
the 
Web 
which 
combines 
techniques 
from 
content-based 
systems 
and 
collaborative 
systems. 
Page 
evaluations 
received 
from 
users 
are 
used 
to 
updated 
Fab's 
search 
and 
selection 
heuristics. 
Unlike 
Wawa 
which 
has 
a 
collection 
of 
agents 
for 
just 
one 
user, 
Fab 
consists 
of 
a 
society 
of 
collaborative 
agents 
for 
a 
group 
of 
users. 
That 
is, 
in 
Fab, 
pages 
returned 
to 
one 
user 
are 
inuenced 
by 
page 
ratings 
made 
by 
other 
users 
in 
the 
society. 
This 
inuence 
can 
be 
viewed 
as 
an 
indirect 
form 
of 
advice 
because 
the 
agent 
for 
user 
A 
will 
adapt 
his 
behavior 
upon 
getting 
feedback 
on 
how 
the 
agent 
for 
a 
similar 
user 
B 
reacted 
to 
a 
page. 




91 


8.3 
Learning 
to 
Extract 
Information 
from 
Text 
I 
was 
unable 
to 
nd 
any 
system 
in 
the 
literature 
that 
applies 
theory 
renement 
to 
the 
IE 
task. 
Most 
IE 
systems 
break 
down 
into 
two 
groups. 
The 
rst 
group 
uses 
some 
kind 
of 
relational 
learning 
to 
learn 
extraction 
patterns 
(Califf 
1998; 
Freitag 
1998b; 
Soderland 
1999; 
Freitag 
and 
Kushmerick 
2000). 
The 
second 
group 
learns 
parameters 
of 
hidden 
Markov 
models 
(HMMs) 
and 
uses 
the 
HMMs 
to 
extract 
information 
(Leek 
1997; 
Bikel, 
Schwartz, 
and 
Weischedel 
1999; 
Freitag 
and 
McCallum 
1999; 
Seymore, 
McCallum, 
and 
Rosenfeld 
1999; 
Ray 
and 
Craven 
2001). 
In 
this 
section, 
I 
discuss 
some 
of 
the 
recently 
developed 
IE 
systems 
in 
both 
of 
these 
groups. 
I 
also 
review 
some 
systems 
that 
use 
IE 
to 
do 
IR 
and 
vice 
versa. 
Finally, 
I 
describe 
the 
named-entity 
problem, 
which 
is 
a 
task 
closely 
related 
to 
IE. 


8.3.1 
Relational 
Learners 
in 
IE 
This 
section 
reports 
on 
four 
systems 
that 
use 
some 
form 
of 
relational 
learning 
to 
solve 
the 
IE 
problem. 
They 
are: 
(i) 
RAPIER,(ii) 
SRV,(iii) 
WHISK, 
and 


(iv) 
BWI. 
RAPIER 


RAPIER 
(Califf 
1998; 
Califf 
and 
Mooney 
1999), 
short 
for 
Robust 
Automated 
Production 
of 
IE 
Rules, 
takes 
as 
input 
pairs 
of 
training 
documents 
and 
their 
associated 
lled 
templates 
and 
learns 
extraction 
rules 
for 
the 
slots 
in 
the 
given 
template. 
RAPIER 
performs 
a 
specic-to-general 
(i.e., 
bottom-up) 
search 
to 
nd 
extraction 
rules. 


Each 
extraction 
rule 
in 
RAPIER 
has 
three 
parts: 
(1) 
a 
pattern 
that 
matches 
the 
text 
immediately 
preceding 
the 
slot 
ller, 
(2) 
a 
pattern 
that 
matches 
the 
actual 
slot 
ller, 
and 
(3) 
a 
pattern 
that 
matches 
the 
text 
immediately 
following 
the 
slot 
ller. 
A 
pattern 
consists 
of 
a 
set 
of 
constraints 
on 
either 
one 
word 
or 




a 
list 
of 
words. 
Constraints 
are 
allowed 
on 
specic 
words, 
part-of-speech 
tags 
assigned 
to 
words, 
and 
the 
semantic-class 
of 
words.1 


As 
mentioned 
above, 
RAPIER 
works 
bottom-up. 
For 
each 
training 
instance, 
it 
generates 
a 
rule 
which 
matches 
the 
target 
slots 
for 
that 
instance. 
Then, 
for 
each 
slot, 
pairs 
of 
rules 
are 
randomly 
selected 
and 
the 
least 
general 
generalizat
ion 
of 
the 
pair 
is 
found 
by 
performing 
a 
best-rst 
beam 
search. 
The 
rules 
are 
sorted 
by 
using 
an 
information-gain 
metric, 
which 
prefers 
simpler 
rules. 
When 
the 
best 
scored 
rule 
matches 
all 
of 
the 
llers 
in 
the 
training 
templates 
for 
a 
slot, 
the 
rule 
is 
added 
to 
the 
knowledge 
base. 
If 
the 
value 
of 
the 
best 
scored 
rule 
does 
not 
change 
across 
several 
successive 
iterations, 
then 
that 
rule 
is 
not 
picked 
for 
further 
modications. 
The 
algorithm 
terminates 
if 
a 
rule 
is 
not 
added 
to 
the 
knowledge 
base 
after 
a 
specied 
threshold. 
RAPIER 
can 
only 
produce 
extraction 
patterns 
for 
single-slot 
extraction 
tasks. 


SRV 


Freitag 
(1998a, 
1998b, 
1998c) 
presents 
a 
multi-strategy 
approach 
to 
learn 
ext
raction 
patterns 
from 
text. 
The 
system 
combines 
a 
rote 
learner, 
a 
naive 
Bayes 
classier, 
and 
a 
relational 
learner 
to 
induce 
extraction 
patterns. 


The 
rote 
learner 
matches 
the 
phrase 
to 
be 
extracted 
against 
a 
list 
of 
correct 
slot 
llers 
from 
the 
training 
set. 
The 
naive 
Bayes 
classier 
estimates 
the 
proba
bility 
that 
the 
terms 
in 
a 
phrase 
are 
in 
a 
correct 
slot 
ller. 
The 
hypothesis 
in 
this 
case 
has 
two 
parts: 
(1) 
the 
starting 
position 
of 
the 
slot 
ller 
and 
(2) 
the 
length 
of 
the 
slot 
ller. 
The 
priors 
for 
the 
position 
and 
length 
are 
determined 
by 
the 
training 
set. 
The 
naive 
Bayes 
classier 
assumes 
that 
the 
position 
and 
the 
length 
of 
a 
slot 
are 
independent 
of 
each 
other. 
Therefore, 
it 
calculates 
the 
prior 
for 
a 
hypothesis 
to 
be 
the 
product 
of 
the 
two 
priors 
for 
a 
given 
position 
and 
length. 


1 
Eric 
Brill's 
tagger 
(1994) 
is 
used 
to 
get 
part-of-speech 
tags 
and 
WordNet 
(Miller 
1995) 
is 
used 
to 
get 
semantic-class 
information. 



93 


His 
relational 
learner 
(named 
SRV) 
is 
similar 
to 
FOIL 
(Quinlan 
1990). 
It 
has 
a 
general-to-specic 
(top-down) 
covering 
algorithm. 
Each 
predicate 
in 
SRV 
belongs 
to 
one 
of 
the 
following 
ve 
pre-specied 
predicates. 
The 
rst 
is 
a 
predi
cate 
called 
length, 
which 
checks 
to 
see 
if 
the 
number 
of 
tokens 
in 
a 
fragment 
is 
less 
than, 
greater 
than, 
or 
equal 
to 
a 
given 
integer. 
The 
second 
is 
a 
predicate 
called 
position, 
which 
places 
a 
constraint 
on 
the 
position 
of 
a 
token 
in 
a 
rule. 
The 
third 
is 
a 
predicate 
called 
relops, 
which 
constrains 
the 
relative 
positions 
of 
two 
tokens 
in 
a 
rule. 
The 
fourth 
and 
fth 
predicates 
are 
called 
some 
and 
every, 
respectively. 
They 
check 
whether 
a 
token's 
feature 
matches 
that 
of 
a 
user-dened 
feature 
(such 
as, 
capitalization, 
digits, 
and 
word 
length). 


The 
system 
takes 
as 
input 
a 
set 
of 
annotated 
documents 
and 
does 
not 
require 
any 
syntactic 
analysis. 
However, 
it 
is 
able 
to 
use 
part-of-speech 
and 
semantic 
classes 
when 
they 
are 
provided. 
Freitag's 
system 
produces 
patterns 
for 
single-
slot 
extraction 
tasks 
only. 


WHISK 


WHISK 
(Soderland 
1999) 
uses 
active-learning 
techniques2 
to 
minimize 
the 
need 
for 
a 
human 
to 
supervise 
the 
system. 
That 
is, 
at 
each 
learning 
iteration, 
WHISK 
presents 
the 
user 
with 
a 
set 
of 
untagged 
examples 
which 
will 
convey 
the 
most 
information 
and 
lead 
to 
better 
coverage 
or 
accuracy 
of 
the 
evolving 
rule 
set. 
WHISK 
randomly 
picks 
the 
set 
of 
untagged 
examples 
from 
three 
categories: 


(1) 
examples 
that 
are 
covered 
by 
an 
existing 
rule, 
(2) 
examples 
that 
are 
near 
misses 
of 
a 
rule, 
and 
(3) 
examples 
that 
are 
not 
covered 
by 
any 
rule. 
The 
user 
denes 
the 
proportion 
of 
examples 
taken 
from 
each 
category. 
The 
default 
setting 
1

is 


3 
. 
The 
decision 
to 
stop 
giving 
training 
examples 
to 
WHISK 
is 
made 
by 
the 
user. 
WHISK 
uses 
a 
general-to-specic 
(top-down) 
covering 
algorithm 
to 
learn 


2 
Active-learning 
methods 
select 
untagged 
examples 
that 
are 
near 
decision 
boundaries. 
The 
selected 
examples 
are 
presented 
to 
and 
tagged 
by 
the 
user. 
The 
use 
of 
voting 
schemes 
or 
assignments 
of 
condence 
levels 
to 
classications 
are 
the 
most 
popular 
active 
learning 
methods. 




extraction 
rules. 
Rules 
are 
created 
from 
a 
seed 
instance. 
A 
seed 
instance 
is 
a 
tagged-example 
which, 
is 
not 
covered 
by 
any 
of 
the 
rules 
in 
the 
rule 
set. 
Since 
WHISK 
does 
hill-climbing 
to 
extend 
rules, 
it 
is 
not 
guaranteed 
to 
produce 
an 
optimal 
rule. 


WHISK 
represents 
extraction 
rules 
in 
a 
restricted 
form 
of 
regular 
express
ions. 
It 
can 
produce 
rules 
for 
both 
single-slot 
and 
multi-slot 
extraction 
tasks. 
Moreover, 
WHISK 
is 
able 
to 
extract 
from 
structured, 
semi-structured, 
and 
free-
text. 


BWI 


Freitag 
and 
Kushmerick 
(2000) 
combine 
wrapper 
induction 
techniques 
(Kushme
rick 
2000) 
with 
the 
AdaBoost 
algorithm 
(Schapire 
and 
Singer 
1998) 
to 
create 
an 
extraction 
system 
named 
BWI 
(short 
for 
Boosted 
Wrapper 
Induction). 
Specifi
cally, 
the 
BWI 
algorithm 
iteratively 
learns 
contextual 
patterns 
that 
recognize 
the 
heads 
and 
tails 
of 
slots. 
Then, 
at 
each 
iteration, 
it 
utilizes 
the 
AdaBoost 
algorithm 
to 
reweigh 
the 
training 
examples 
that 
were 
not 
covered 
by 
previous 
patterns. 


Discussion 


SRV 
and 
RAPIER 
build 
rules 
that 
individually 
specify 
an 
absolute 
order 
for 
the 
extraction 
tokens. 
Wawa-IE, 
WHISK, 
and 
BWI 
use 
wildcards 
in 
extraction 
rules 
to 
specify 
a 
relative 
order 
for 
the 
tokens. 


Wawa-IE 
out-performs 
RAPIER, 
SRV 
and 
WHISK 
on 
the 
CMU 
seminar-
announcement 
domain 
(see 
Section 
7.1). 
BWI 
out-performed 
many 
of 
the 
relat
ional 
learners 
and 
was 
competitive 
with 
systems 
using 
HMMs 
and 
WAWA-IE. 


It 
is 
interesting 
to 
note 
that 
BWI 
has 
a 
bias 
towards 
high 
precision 
and 
tries 
to 
improve 
its 
recall 
measure 
by 
learning 
hundreds 
of 
rules. 
Wawa-IE, 
on 
the 
other 
hand, 
has 
a 
bias 
towards 
high 
recall 
and 
tries 
to 
improve 
its 
precision 
through 
learning 
about 
the 
extraction 
slots. 




Only 
Wawa-IE 
and 
WHISK 
are 
able 
to 
handle 
multi-slot 
extraction 
tasks. 
The 
ability 
to 
accept 
and 
rene 
advice 
makes 
Wawa-IE 
less 
of 
a 
burden 
on 
the 
user 
than 
the 
other 
methods 
mentioned 
in 
this 
section. 


8.3.2 
Hidden 
Markov 
Models 
in 
IE 
Leek 
(1997) 
uses 
HMMs 
for 
extracting 
information 
from 
biomedical 
text. 
His 
system 
uses 
a 
lot 
of 
initial 
knowledge 
to 
build 
the 
HMM 
model 
before 
using 
the 
training 
data 
to 
learn 
the 
parameters 
of 
HMM. 
However, 
his 
system 
is 
not 
able 
to 
rene 
the 
knowledge. 


Freitag 
and 
McCallum 
(1999) 
use 
HMMs 
and 
a 
statistical 
method 
called 
\shrinkage"3 
to 
improve 
parameter 
estimation 
when 
only 
a 
small 
amount 
of 
training 
data 
is 
available. 
Each 
extraction 
eld 
has 
its 
own 
HMM 
and 
the 
state-
transition 
structure 
of 
the 
HMM 
is 
hand-coded. 
Recently, 
the 
same 
authors 
purposed 
an 
algorithm 
for 
automating 
the 
process 
of 
nding 
good 
structures 
for 
their 
HMMs 
(Freitag 
and 
McCallum 
2000). 
Their 
algorithm 
starts 
with 
a 
simple 
HMM 
model 
and 
performs 
a 
hill-climbing 
search 
in 
the 
space 
of 
possible 
HMM 
structures. 
A 
move 
in 
the 
space 
is 
a 
split 
in 
a 
state 
of 
the 
HMM 
and 
the 
heuristic 
used 
is 
the 
performance 
of 
the 
resulting 
HMM 
on 
a 
validation 
set. 


Seymore 
et 
al. 
(1999) 
use 
one 
HMM 
to 
extract 
many 
elds. 
The 
state-
transition 
structure 
of 
the 
HMM 
is 
learned 
from 
training 
data. 
This 
extraction 
system 
is 
implemented 
in 
the 
CORA 
search 
engine 
(McCallum, 
Nigam, 
Rennie, 
and 
Seymore 
2000) 
discussed 
in 
Section 
8.1. 
They 
get 
around 
the 
problem 
of 
not 
having 
sucient 
training 
examples 
by 
using 
data 
that 
is 
labeled 
for 
the 
information-retrieval 
task 
in 
their 
system. 


Ray 
and 
Craven 
(2001) 
use 
HMMs 
to 
extract 
information 
from 
free 
text 
domains. 
They 
have 
developed 
an 
algorithm 
for 
incorporating 
grammatical 


3 
Shrinkage 
tries 
to 
nd 
a 
happy 
medium 
between 
the 
size 
of 
the 
training 
data 
and 
the 
number 
of 
states 
used 
in 
a 
HMM. 



structure 
of 
sentences 
in 
an 
HMM 
and 
have 
found 
that 
such 
information 
imp
roves 
performance. 
Moreover, 
instead 
of 
training 
their 
HMMs 
to 
maximize 
the 
likelihood 
of 
the 
data 
given 
the 
model, 
they 
maximize 
the 
likelihood 
of 
predicti
ng 
correct 
sequences 
of 
slot 
llers. 
This 
objective 
function 
has 
also 
improved 
their 
performance. 


Discussion 


Wawa-IE 
and 
the 
HMM 
produced 
by 
Freitag 
and 
McCallum 
(2000) 
are 
comp
etitive 
in 
the 
CMU 
seminar-announcement 
data 
set. 
Wawa-IE 
was 
able 
to 
out-perform 
Ray 
and 
Craven's 
HMMs 
on 
the 
two 
biomedical 
domains 
described 
in 
Section 
7. 


Wawa-IE 
has 
three 
advantages 
over 
the 
systems 
that 
use 
HMM. 
The 
rst 
advantage 
of 
Wawa-IE 
is 
that 
it 
is 
able 
to 
utilize 
and 
rene 
prior 
knowledge, 
which 
reduces 
the 
need 
for 
a 
large 
number 
of 
labeled 
training 
examples. 
Howe
ver, 
Wawa-IE 
does 
not 
depend 
on 
the 
initial 
knowledge 
being 
100% 
correct 
(due 
to 
its 
learning 
abilities). 
I 
believe 
that 
it 
is 
relatively 
easy 
for 
users 
to 
articulate 
some 
useful 
domain-specic 
advice 
(especially 
when 
a 
user-friendly 
interface 
is 
provided 
that 
converts 
their 
advice 
into 
the 
specics 
of 
WAWA's 
advice 
language). 
The 
second 
advantage 
of 
Wawa-IE 
is 
that 
the 
entire 
content 
of 
the 
document 
is 
used 
to 
estimate 
the 
correctness 
of 
a 
candidate 
extraction. 
This 
allows 
Wawa-IE 
to 
learn 
about 
the 
extraction 
slots 
and 
the 
documents 
in 
which 
they 
appear. 
The 
third 
advantage 
of 
WAWA-IE 
is 
that 
it 
is 
able 
to 
utilize 
the 
untrained 
ScorePage 
network 
to 
produce 
some 
informative 
negat
ive 
training 
examples 
(i.e., 
near 
misses), 
which 
are 
usually 
not 
provided 
in 
IE 
tasks. 


8.3.3 
Using 
Extraction 
Patterns 
to 
Categorize 
Riloff 
and 
Lehnert 
(1994, 
1996) 
describe 
methods 
for 
using 
the 
patterns 
gene
rated 
by 
an 
information-extraction 
system 
to 
classify 
text. 
During 
training, 



a 
signature 
is 
produced 
by 
pairing 
each 
extraction 
pattern 
with 
the 
words 
in 
the 
training 
set 
that 
satisfy 
that 
pattern. 
This 
signature 
is 
then 
labeled 
as 
a 
relevancy 
signature 
if 
it 
is 
highly 
correlated 
with 
the 
relevant 
documents 
in 
the 
training 
set. 
In 
the 
testing 
phase, 
a 
document 
is 
labeled 
as 
relevant 
only 
if 
it 
contains 
one 
of 
the 
generated 
relevancy 
signatures. 


There 
is 
a 
big 
dierence 
between 
the 
advice 
rules 
given 
by 
the 
user 
to 
Wawa 
and 
the 
extraction 
patterns 
generated 
by 
the 
information-extraction 
systems 
used 
in 
Riloff 
and 
Lehnert 
(1994) 
and 
Riloff 
(1996). 
Wawa's 
advice 
rules 
dene 
the 
behavior 
of 
an 
IE 
agent 
upon 
encountering 
documents. 
The 
extraction 
patterns 
generated 
in 
Riloff 
and 
Lehnert's 
work 
are 
learned 
from 
a 
set 
of 
labeled 
training 
examples 
and 
are 
used 
to 
extract 
words 
and 
phrases 
from 
sentences. 


8.3.4 
Using 
Text 
Categorizers 
to 
Extract 
Only 
a 
few 
researches 
have 
investigated 
using 
text 
classiers 
to 
solve 
the 
extract
ion 
problem. 
Craven 
and 
Kumlien 
(1999) 
use 
a 
sentence 
classier 
to 
extract 
instances 
of 
the 
subcellular-localization 
relation 
from 
an 
earlier 
version 
of 
the 
subcellular-localization 
data 
set 
described 
in 
Section 
7.2. 
Specically, 
if 
r(X, 
Y 
) 
is 
the 
target 
binary 
relation, 
then 
the 
goal 
is 
to 
nd 
instances 
x 
and 
y 
where 
x 
and 
y 
are 
in 
the 
semantic 
lexicons 
of 
X 
and 
Y 
respectively. 
For 
example, 
the 
binary 
relation 
cell-localization(protein, 
cell-type) 
describes 
the 
cell 
types 
in 
which 
a 
particular 
protein 
is 
located. 


Craven 
and 
Kumlien 
(1999) 
use 
the 
naive 
Bayes 
algorithm 
with 
a 
bag-ofw
ords 
representation 
(Mitchell 
1997) 
to 
classify 
sentences. 
A 
sentence 
is 
class
ied 
as 
a 
positive 
example 
if 
it 
contains 
at 
least 
one 
instance 
of 
the 
target 
relation. 
Then, 
to 
represent 
linguistic 
structure 
of 
documents, 
they 
learned 
IE 
rules 
by 
using 
a 
relational 
learning 
algorithm 
similar 
to 
FOIL 
(Quinlan 
1990). 
Craven 
and 
Kumlien 
(1999) 
use 
\weakly” 
labeled 
training 
data 
to 
reduce 
the 
need 
for 
labeled 
training 
examples. 



Wawa-IE 
is 
similar 
to 
Craven 
and 
Kumlien's 
system 
in 
that 
I 
use 
essent
ially 
a 
text 
classier 
to 
extract 
information. 
However, 
in 
Craven 
and 
Kumlien 
(1999), 
the 
text 
classier 
processes 
small 
chunks 
of 
text 
(such 
as 
sentences) 
and 
extracts 
binary 
relations 
of 
the 
words 
that 
are 
in 
the 
given 
semantic 
lexicons. 
In 
Wawa-IE, 
the 
text 
classier 
is 
able 
to 
classify 
a 
text 
document 
as 
a 
whole 
and 
generates 
a 
lot 
of 
extraction 
candidates 
without 
the 
need 
for 
semantic 
lexic
ons. 
Another 
dierence 
between 
Wawa-IE 
and 
Craven 
and 
Kumlien's 
system 
is 
that 
their 
text 
classier 
is 
built 
for 
the 
sole 
purpose 
of 
extracting 
information. 
However, 
Wawa 
was 
initially 
built 
to 
classify 
text 
documents 
(of 
any 
size) 
and 
its 
extraction 
ability 
is 
a 
side 
eect 
of 
the 
way 
the 
classier 
was 
implemented. 


Zaragoza 
and 
Gallinari 
(1998) 
use 
hierarchical 
information-retrieval 
methods 
to 
reduce 
the 
amount 
of 
data 
given 
to 
their 
stochastic 
information-extraction 
system. 
The 
standard 
IR 
technique 
of 
TF/IDF 
weighting 
(see 
Section 
2.4) 
is 
used 
to 
eliminate 
irrelevant 
documents. 
Then, 
the 
same 
IR 
process 
is 
used 
to 
eliminate 
irrelevant 
paragraphs 
from 
relevant 
documents. 
Relevant 
paragraphs 
have 
at 
least 
one 
extraction 
template 
associated 
with 
them. 
Finally, 
Hidden 
Markov 
Models 
are 
used 
to 
extract 
patterns 
at 
the 
word 
level 
from 
a 
set 
of 
relev
ant 
paragraphs. 
Wawa-IE 
is 
dierent 
than 
Zaragoza 
and 
Gallinari's 
system 
in 
that 
I 
do 
not 
use 
a 
text 
classier 
to 
lter 
out 
data 
and 
then 
use 
another 
model 
to 
extract 
data. 
A 
Wawa-IE 
agent 
is 
trained 
to 
directly 
extract 
by 
rating 
an 
extraction 
within 
the 
context 
of 
the 
entire 
document. 


8.3.5 
The 
Named-Entity 
Problem 
A 
task 
close 
to 
IE 
is 
the 
named-entity 
problem. 
The 
named-entity 
task 
is 
the 
problem 
of 
recognizing 
all 
names 
(i.e., 
people, 
locations, 
and 
organizations), 
dates, 
times, 
monetary 
amounts, 
and 
percentages 
in 
text. 
I 
have 
come 
across 
two 
learning 
systems 
that 
focus 
on 
extracting 
names. 


The 
rst 
is 
IdentiF 
inderTM 
(Bikel, 
Schwartz, 
and 
Weischedel 
1999), 
which 
uses 
a 
Hidden 
Markov 
Model 
and 
textual 
information 
(such 
as 
capitalization 



and 
punctuation) 
to 
learn 
to 
recognize 
and 
classify 
names, 
dates, 
times, 
and 
numerical 
quantities. 
A 
name 
is 
classied 
into 
three 
categories: 
the 
name 
of 
a 
person, 
the 
name 
of 
a 
location, 
and 
the 
name 
of 
an 
organization. 
Numerical 
quantities 
are 
classied 
into 
monetary 
amounts 
or 
percentages. 
IdentiFinderTM 
is 
independent 
of 
the 
case 
of 
the 
text 
(i.e., 
all 
lower-case, 
all 
capitalized, 
or 
mixed) 
and 
was 
applied 
to 
text 
in 
English 
and 
Spanish. 


In 
the 
second 
system, 
Baluja 
et 
al. 
(1999) 
use 
a 
decision-tree 
classier 
in 
conjunction 
with 
information 
from 
part-of-speech 
tagging, 
dictionary 
lookup, 
and 
textual 
information 
(such 
as 
capitalization) 
to 
extract 
names. 
Their 
syst
em 
does 
not 
attempt 
to 
distinguish 
between 
names 
of 
persons, 
locations, 
and 
organizations. 


Wawa-IE 
does 
not 
use 
dictionary 
lookups 
as 
evidence. 
One 
advantage 
of 
my 
system 
compared 
to 
IdentiF 
inderTM 
and 
Baluja 
et 
al.'s 
system 
is 
that 
I 
do 
not 
need 
a 
large 
number 
of 
labeled 
training 
examples 
to 
achieve 
high 
performance. 
This 
is 
because 
I 
address 
the 
problem 
by 
using 
theory-renement 
techniques, 
which 
allow 
users 
to 
easily 
provide 
task-specic 
information 
via 
approximately 
correct 
inference 
rules 
(e.g., 
Wawa's 
advice 
rules). 


8.3.6 
Adaptive 
and 
Intelligent 
Sampling 
The 
interest 
in 
being 
able 
to 
intelligently 
search 
a 
large 
space 
is 
not 
new. 
Howe
ver, 
one 
of 
the 
recent 
advances 
in 
this 
area 
was 
done 
by 
Boyan 
and 
Moore 
(1998, 
2000). 
They 
describe 
an 
intelligent 
search 
algorithm 
called 
STAGE, 
which 
utilizes 
features 
of 
the 
search 
space4 
to 
predict 
the 
outcome 
of 
a 
local 
search 
algorithm 
(e.g., 
hillclimbing 
or 
WalkSAT). 
In 
particular, 
STAGE 
learns 
an 
evaluation 
function 
for 
a 
specic 
problem 
and 
uses 
it 
to 
bias 
future 
search 
paths 
toward 
optima 
in 
the 
search 
space. 
More 
recently, 
Boyan 
and 
Moore 
(2000) 
present 
an 
algorithm, 
called 
XSTAGE, 
which 
is 
able 
to 
utilize 
previously 


4 
An 
example 
of 
a 
feature 
used 
in 
STAGE 
is 
the 
number 
of 
bins 
in 
the 
bin-packing 
problem 
(Coman, 
Garey, 
and 
Johnson 
1996). 



learned 
evaluation 
functions 
on 
new 
and 
similar 
search 
problems. 
Wawa-IE's 
selector 
use 
the 
trained 
ScorePage 
network 
to 
nd 
good 
search 
trajectories. 



Chapter 
9 


Conclusions 


In 
this 
thesis, 
I 
argue 
that 
a 
promising 
way 
to 
create 
eective 
and 
intelligent 
IR 
and 
IE 
agents 
is 
to 
involve 
both 
the 
user's 
ability 
to 
do 
direct 
programming 
(i.e., 
provide 
approximately 
correct 
instructions 
of 
some 
sort) 
along 
with 
the 
agent's 
ability 
to 
accept 
and 
automatically 
create 
training 
examples. 
Due 
to 
the 
largely 
unstructured 
nature 
and 
the 
size 
of 
on-line 
textual 
information, 
such 
a 
hybrid 
approach 
is 
more 
appealing 
than 
ones 
solely 
based 
on 
either 
non-adaptive 
agent 
programming 
languages 
or 
large 
amounts 
of 
annotated 
examples. 


This 
chapter 
presents 
the 
contributions 
of 
my 
thesis 
along 
with 
its 
limitations 
and 
some 
ideas 
about 
future 
work. 


9.1 
Contributions 
The 
underlying 
contribution 
of 
this 
thesis 
is 
in 
its 
use 
of 
theory-renement 
techniques 
for 
the 
information 
retrieval 
and 
information 
extraction 
tasks. 
The 
specic 
contributions 
of 
this 
thesis 
to 
each 
task 
are 
listed 
below. 


Wawa-IR 


The 
signicant 
contribution 
of 
Wawa's 
IR 
system 
is 
in 
its 
ability 
to 
utilize 
the 
following 
three 
sources 
of 
information 
when 
learning 
to 
convert 
from 
a 
general 
search 
engine 
to 
a 
personalized 
and 
specialized 
IR 
agent: 


1. 
users’ 
prior 
and 
continual 
instruction 
2. 
users’ 
manually 
labeled 
(Web) 
pages 

3. 
system-measured 
feedback 
from 
the 
Web 
The 
advantages 
of 
utilizing 
users’ 
prior 
and 
continual 
instruction 
are 
seen 
in 
two 
ways: 
(i) 
the 
agent 
performs 
reasonably 
well 
initially 
and 
(ii) 
the 
agent 
does 
not 
need 
to 
process 
a 
large 
number 
of 
training 
examples 
to 
specialize 
in 
the 
desired 
IR 
task. 
However, 
if 
labeled 
examples 
become 
available, 
then 
Wawa-IR 
can 
use 
them 
during 
the 
agent's 
training 
process. 


Wawa-IR 
utilizes 
the 
feedback 
it 
gets 
from 
the 
Web 
by 
using 
temporal-
dierence 
methods 
(that 
are 
based 
on 
beam-search 
as 
opposed 
to 
hill 
climbi
ng) 
to 
evaluate 
the 
reward 
of 
following 
a 
hyperlink. 
In 
this 
way, 
Wawa-IR's 
reinforcement-learner 
automatically 
generates 
its 
own 
training 
examples 
and 
is 
able 
to 
accept 
and 
rene 
users’ 
advice. 


Wawa-IE 


The 
ve 
major 
contributions 
of 
Wawa's 
IE 
system 
are 
as 
follows: 


1. 
By 
using 
theory 
renement, 
it 
is 
able 
to 
reduce 
the 
need 
for 
large 
number 
of 
annotated 
examples 
(which 
are 
very 
expensive 
to 
obtain 
in 
extraction 
tasks). 
2. 
By 
using 
the 
untrained 
neural 
network 
(which 
only 
contains 
user's 
initial 
advice), 
it 
is 
able 
to 
create 
informative 
negative 
training 
examples 
(which 
are 
rare 
in 
IE 
tasks) 
. 
3. 
By 
using 
an 
IR 
agent 
to 
score 
each 
possible 
extraction, 
it 
uses 
the 
content 
of 
the 
entire 
document 
to 
learn 
about 
extraction 
template 
4. 
By 
using 
a 
generate-and-test 
approach 
to 
information 
extraction, 
Wawa-
IE 
has 
a 
bias 
towards 
high 
recall. 
It 
improves 
its 
precision 
on 
the 
IE 
task 
during 
the 
training 
process. 
The 
bias 
towards 
high 
recall 
plus 
the 
improvement 
on 
precision 
lead 
to 
high 
F1-measures 
on 
Wawa-IE's 
trained 
agents. 

103 


5. 
By 
using 
heuristically 
inclined 
sampling 
techniques, 
Wawa-IE 
has 
been 
able 
to 
reduce 
the 
computational 
burden 
inherent 
in 
generate-and-test 
approaches. 
9.2 
Limitations 
and 
Future 
Directions 
The 
main 
limitations 
of 
Wawa 
are: 
(i) 
speed 
and 
(ii) 
comprehensibility. 
One 
cost 
of 
using 
Wawa-IR 
is 
that 
I 
fetch 
and 
analyze 
many 
Web 
pages. 
In 
this 
thesis, 
I 
have 
not 
focused 
on 
speed 
of 
an 
IR 
agent, 
ignoring 
such 
questions 
as 
how 
well 
does 
an 
Wawa-IR 
agent 
perform 
if 
it 
only 
used 
the 
capsule 
summaries 
that 
the 
search 
engines 
return, 
etc. 
Making 
Wawa-IR 
faster 
is 
part 
of 
future 
directions. 


Due 
to 
my 
use 
of 
articial 
neural 
networks, 
it 
is 
dicult 
to 
understand 
what 
was 
learned 
(Craven 
and 
Shavlik 
1996). 
It 
would 
be 
nice 
if 
a 
Wawa-IR 
agent 
could 
explain 
its 
reasoning 
to 
the 
user. 
In 
an 
attempt 
to 
alleviate 
this 
problem, 
I 
have 
built 
a 
\visualizer” 
for 
each 
neural 
network 
in 
Wawa-IR. 
The 
visualizer 
draws 
the 
neural 
network 
and 
graphically 
displays 
information 
on 
all 
nodes 
and 
links 
in 
the 
network. 


The 
main 
directions 
of 
future 
work 
are 
as 
follows: 


1. 
Better 
understand 
what 
people 
would 
like 
to 
say 
to 
an 
instructable 
Web 
agent 
such 
as 
Wawa 
and 
improve 
Wawa's 
advice 
language 
accordingly. 
2. 
Embed 
Wawa 
into 
a 
major, 
existing 
Web 
browser, 
thereby 
minimizing 
new 
interface 
features 
that 
users 
must 
learn 
in 
order 
to 
interact 
with 
my 
system. 
3. 
Develop 
methods 
whereby 
Wawa 
can 
automatically 
infer 
plausible 
traini
ng 
examples 
by 
observing 
users’ 
normal 
use 
of 
their 
browsers 
(Goecks 
and 
Shavlik 
2000). 

104 


4. 
Incorporate 
the 
candidate 
generation 
and 
selection 
steps 
directly 
into 
Wawa's 
connectionist 
framework, 
whereby 
the 
current 
ScorePage 
netw
ork 
would 
nd 
new 
candidate 
extractions 
during 
the 
training 
process. 
5. 
Learn 
to 
rst 
reduce 
the 
number 
of 
candidates 
per 
slot 
and 
then 
apply 
Wawa-IE's 
selectors 
to 
the 
reduced 
list 
of 
individual-slot 
extraction 
cand
idates. 
6. 
Test 
Wawa-IE's 
scaling 
abilities 
(e.g. 
run 
Wawa-IE 
on 
a 
combination-
slots 
extraction 
task 
that 
has 
more 
than 
two 
extraction 
slots). 
7. 
Use 
other 
supervised 
learning 
algorithms 
such 
as 
support 
vector 
machines 
(Cristianini 
and 
Shawe-Taylor 
2000) 
or 
hidden 
Markov 
models 
(Rabiner 
and 
Juang 
1993) 
as 
Wawa's 
supervised 
learner. 
Moreover, 
explore 
ways 
in 
which 
theory-renement 
techniques 
can 
be 
applied 
to 
these 
other 
algor
ithms. 
9.3 
Final 
Remarks 
At 
this 
moment 
in 
time, 
the 
task 
of 
retrieving 
and 
extracting 
information 
from 
on-line 
documents 
is 
like 
trying 
to 
nd 
a 
couple 
of 
needles 
in 
a 
colossal 
haystack. 
When 
encountered 
with 
such 
hard 
problems, 
one 
should 
utilize 
all 
of 
the 
availa
ble 
resources. 
Such 
is 
the 
case 
in 
the 
system 
that 
I 
have 
presented 
in 
this 
dissertation. 
I 
hope 
that 
the 
experimental 
successes 
of 
Wawa 
will 
persuade 
more 
researchers 
to 
investigate 
theory-renement 
approaches 
in 
their 
learning 
systems. 



Appendix 
A 


WAWA's 
Advice 
Language 


This 
appendix 
presents 
Wawa's 
advice 
language 
and 
is 
divided 
into 
four 
segm
ents. 
The 
rst 
segment 
introduces 
the 
sentences 
in 
Wawa's 
advice 
language, 
which 
are 
conditional 
statements 
(i.e., 
if 
condition 
then 
action). 
The 
second 
and 
third 
segments 
list 
the 
conditions 
and 
the 
actions 
in 
Wawa's 
advice 
lang
uage, 
respectively. 
The 
last 
segment 
describes 
the 
dierent 
user-interfaces 
for 
giving 
advice. 


Sentences 


A 
sentence 
in 
Wawa's 
advice 
language 
has 
one 
of 
the 
following 
forms: 


S1. 
WHEN 
condition 
THEN 
action 
This 
statement 
advises 
the 
agent 
that 
when 
the 
condition 
is 
true, 
it 
should 
pursue 
the 
given 
action. 


S2. 
ScaleLinearlyBy( 
conditions 
) 
THEN 
action 
In 
this 
statement, 
the 
agent 
is 
advised 
to 
consider 
the 
conditions 
listed 
in 
the 
ScaleLinearlyBy 
function. 
The 
ScaleLinearlyBy 
function 
takes 
a 
set 
of 
conditions 
and 
returns 
a 
number 
between 
0 
and 
1, 
proportional 
to 
the 
number 
of 
conditions 
satised. 
The 
conditions 
are 
given 
the 
same 
weights 
inititally; 
however 
these 
weights 
can 
change 
during 
training. 
The 
activation 
function 
of 
the 
hidden 
unit 
created 
by 
this 
type 
of 
advice 
is 
linear. 


S3. 
WHEN 
guarded 
condition 
ScaleLinearlyBy( 
conditions 
) 
THEN 
action 
This 
statement 
advises 
the 
agent 
to 
consider 
the 
conditions 
listed 
in 
the 
Scale-
LinearlyBy 
function 
and 
consequently 
pursue 
the 
given 
action 
only 
if 
the 


guarded 


condition 
is 
true. 




Conditions 


The 
conditions 
used 
in 
advice 
rules 
are 
presented 
in 
BNF, 
short 
for 
Backus-
Naur 
Form, 
notation 
(Aho, 
Sethi, 
and 
Ullman 
1986). 
The 
non-terminals 
are 
bold 
face. 
The 
terminals 
are 
surrounded 
by 
quotation 
marks. 
Predicates 
and 
functions 
are 
italicized. 
Parameters 
are 
in 
typewriter 
font. 


conditions 
. 


condition 
| 


conditions 
| 


condition 
connective 
conditions 
| 


(conditions) 
connective 
conditions 


connective 
. 
\AND” 
| 
\And” 
| 
\and” 
| 
\&” 
| 
\&&” 
| 
\OR” 
| 
\Or” 
| 
\or” 
| 
\j” 
| 
\jj” 


Among 
the 
predicates 
listed 
under 
the 
non-terminal 
condition, 
NofM 
looks 
for 
(Web) 
pages 
where 
at 
least 
N 
of 
the 
M 
given 
conditions 
are 
true. 


condition 
. 


[\NOT” 
| 
\not” 
| 
\Not” 
] 
condition 
| 


NofM 
( 
integer, 
conditions 
) 
| 


word 
locations 
constructs 
| 


phrase 
constructs 
| 


nearness 
constructs 
| 


general 
features 
| 


numeric 
operations 


The 
predicate 
anyOf 
is 
true 
if 
any 
of 
the 
listed 
words 
is 
present. 
The 
predicate 
noneOf 
is 
true 
if 
none 
of 
the 
listed 
words 
is 
present. 
The 
anyOf 
and 
noneOf 
predicates 
may 
be 
used 
in 
conditions 
where 
one 
or 
more 
words 
are 
required. 


terms 
. 


word 
| 
words 
| 
anyOf 
( 
words 
) 
| 
noneOf 
( 
words 
) 
| 


terms 
\” 
terms 
| 
terms 
\” 
terms 




The 
predicates 
listed 
under 
word 
locations 
constructs 
look 
for 
words 
in 
specic 
locations 
on 
a 
Web 
page. 
Here 
is 
an 
explanation 
of 
some 
of 
the 
non-
obvious 
predicates 
for 
this 
group. 
The 
predicate 
anywhereInSection 
is 
true 
if 
the 
listed 
terms 
are 
found 
in 
the 
given 
section 
depth 
relative 
to 
the 
current 
nesting 
of 
the 
sliding 
window. 
The 
value 
of 
relative 
section 
depth 
must 
be 
either 
1 
(for 
the 
parent 
section), 
2 
(for 
the 
grandparent 
section), 
or 
3 
(for 
the 
great-grandparent 
section). 


The 
predicates 
anywhereInLeftSideOfWindow 
and 
anywhereInRightSide-
OfWindow 
are 
true 
if 
the 
given 
terms 
are 
in 
the 
bags 
to 
the 
left 
and 
right 
of 
the 
current 
window 
position, 
respectively. 


The 
predicates 
atBackPosInHyperlinkHost 
and 
atBackPosInUrlHost 
det
ermine 
whether 
the 
given 
word 
is 
found 
at 
the 
specied 
position 
from 
the 
end 
of 
the 
hyperlink's 
or 
URL's 
host, 
respectively. 
For 
example, 
atBackPosInHyperlinkHost(1, 
edu) 
looks 
for 
hyperlinks 
that 
have 
\edu” 
as 
the 
last 
word 
of 
their 
host 
(e.g., 
www.wisc.edu). 


The 
predicates 
atLeftSpotInWindow 
and 
atRightSpotInWindow 
are 
true 
when 
the 
given 
word 
is 
at 
the 
specied 
position 
to 
the 
left 
or 
right 
of 
the 
center 
of 
the 
sliding 
window, 
respectively. 


word 
locations 
constructs 
. 


anywhereOnPage( 
terms 
) 
| 


anywhereInTitle( 
terms 
) 
| 


anywhereInUrlHost( 
terms 
) 
| 


anywhereInUrlNonHost( 
terms 
) 
| 


anywhereInSections( 
terms 
) 
| 


anywhereInCurrentSection( 
terms 
) 
| 



anywhereInSection( 
relative 
section 
depth, 
terms 
) 
| 


anywhereInHyperlinkHost( 
terms 
) 
| 


anywhereInHyperlinkNonHost( 
terms 
) 
| 


anywhereInHypertext( 
terms 
) 
| 


anywhereInLeftSideOfWindow( 
terms 
) 
| 




anywhereInWindow( 
terms 
) 
| 
anywhereInRightSideOfWindow( 
terms 
) 
| 
titleStartsWith( 
terms 
) 
| 
titleEndsWith( 
terms 
) 
| 
urlHostStartsWith( 
terms 
) 
| 
urlHostEndsWith( 
terms 
) 
| 
urlNonHostStartsWith( 
terms 
) 
| 
urlNonHostEndsWith( 
terms 
) 
| 
currentSectionStartsWith( 
terms 
) 
| 
currentSectionEndsWith( 
terms 
) 
| 
hyperlinkHostStartsWith( 
terms 
) 
| 
hyperlinkHostEndsWith( 
terms 
) 
| 
hyperlinkNonHostStartsWith( 
terms 
) 
| 
hyperlinkNonHostEndsWith( 
terms 
) 
| 
atLeftSpotInTitle( 
position, 
word 
) 
| 
atRightSpotInTitle( 
position, 
word 
) 
| 
atBackSpotInURLHost( 
position, 
word 
) 
| 
atLeftSpotInURL( 
position, 
word 
) 
| 
atRightSpotInURL( 
position, 
word 
) 
| 
atLeftSpotInCurrentSection( 
position, 
word 
) 
| 
atRightSpotInCurrentSection( 
position, 
word 
) 
| 
atBackSpotInHyperlinkHost( 
position, 
word 
) 
| 
atLeftSpotInHyperlink( 
position, 
word 
) 
| 
atRightSpotInHyperlink( 
position, 
word 
) 
| 
atLeftSpotInWindow( 
position, 
word 
) 
| 
atCenterOfWindow( 
word 
) 
| 
atRightSpotInWindow( 
position, 
word 
) 
| 
POSatLeftSpotInWindow( 
position, 
part 
of 
speech 
tag 
) 
| 
POSatCenterOfWindow( 
part 
of 
speech 
tag 
) 
| 
POSatRightSpotInWindow( 
position, 
part 
of 
speech 
tag 
) 




The 
predicates 
listed 
under 
phrase 
constructs 
determine 
where 
a 
phrase 
is 
present 
on 
a 
page 
or 
in 
localized 
parts 
of 
it 
(e.g., 
the 
page's 
title). 
The 
predi
cate 
consecutiveInTitle 
is 
true 
if 
the 
given 
phrase 
is 
in 
the 
title 
of 
the 
page. 
The 
predicates 
consecutiveInUrlHost 
and 
consecutiveInUrlNonHost 
determine 
if 
the 
specied 
phrase 
is 
in 
the 
host 
or 
the 
non-host 
segments 
of 
the 
page's 
url, 
respectively. 
The 
predicates 
consecutiveInHyperlinkHost 
and 
consecutiveInHyp
erlinkNonHost 
are 
similar 
to 
consecutiveInUrlHost 
and 
consecutiveInUrlNon-
Host 
except 
that 
they 
look 
for 
the 
given 
phrase 
in 
the 
host 
or 
the 
non-host 
segments 
of 
the 
page's 
hyperlinks, 
respectively. 


phrase 
constructs 
. 


consecutive( 
terms 
) 
| 


consecutiveInTitle( 
terms 
) 
| 


consecutiveInUrlHost( 
terms 
) 
| 


consecutiveInUrlNonHost( 
terms 
) 
| 


consecutiveInaSection( 
terms 
) 
| 


consecutiveInHyperlinkHost( 
terms 
) 
| 


consecutiveInHyperlinkNonHost( 
terms 
) 
| 


consecutiveInHypertext( 
terms 
) 


The 
predicates 
listed 
under 
nearness 
constructs 
can 
be 
divided 
into 
the 
following 
four 
subgroups: 


1. 
nearbyInLOCATION 
predicates, 
which 
are 
true 
when 
the 
given 
terms 
are 
near 
each 
other 
(e.g., 
nearybyInTitle) 
2. 
nearbyInLOCATIONInOrder 
predicates, 
which 
are 
true 
when 
the 
given 
terms 
are 
in 
order 
and 
near 
each 
other 
(e.g., 
nearbyInTitleInOrder) 
3. 
nearbyInLOCATIONWithin 
predicates, 
which 
are 
true 
when 
the 
given 
terms 
are 
within 
the 
specied 
distance 
of 
each 
other 
(e.g., 
nearbyInTitleWithin) 
4. 
nearbyInLOCATIONInOrderWithin 
predicates, 
which 
are 
true 
when 
the 
given 
terms 
are 
in 
order 
and 
within 
the 
specied 
distance 
of 
each 
other 
(e.g., 
nearbyInTitleInOrderWithin) 

nearness 
constructs 
. 
nearby( 
terms 
) 
| 
nearbyInTitle( 
terms 
) 
| 
nearbyInUrlHost( 
terms 
) 
| 
nearbyInUrlNonHost( 
terms 
) 
| 
nearbyInSection( 
terms 
) 
| 
nearbyInHostHyperlink( 
terms 
) 
| 
nearbyInNonHostHyperlink( 
terms 
) 
| 
nearbyInHypertext( 
terms 
) 
| 
nearbyInOrder( 
terms 
) 
| 
nearbyInTitleInOrder( 
terms 
) 
| 
nearbyInUrlHostInOrder( 
terms 
) 
| 
nearbyInUrlNonHostInOrder( 
terms 
) 
| 
nearbyInSectionInOrder( 
terms 
) 
| 
nearbyInHyperlinkHostInOrder( 
terms 
) 
| 
nearbyInHyperlinkNonHostInOrder( 
terms 
) 
| 
nearbyInHypertextInOrder( 
terms 
) 
| 
nearbyWithin( 
distance, 
terms 
) 
| 
nearbyInTitleWithin( 
distance, 
terms 
) 
| 
nearbyInUrlHostWithin( 
distance, 
terms 
) 
| 
nearbyInUrlNonHostWithin( 
distance, 
terms 
) 
| 
nearbyInSectionWithin( 
distance, 
terms 
) 
| 
nearbyInHyperlinkHostWithin( 
distance, 
terms 
) 
| 
nearbyInHyperlinkNonHostWithin( 
distance, 
terms 
) 
| 
nearbyInHypertextWithin( 
distance, 
terms 
) 
| 
nearbyInOrderWithin( 
distance, 
terms 
) 
| 
nearbyInTitleInOrderWithin( 
distance, 
terms 
) 
| 
nearbyInUrlHostInOrderWithin( 
distance, 
terms 
) 
| 
nearbyInUrlNonHostInOrderWithin( 
distance, 
terms 
) 
| 
nearbyInSectionInOrderWithin( 
distance, 
terms 
) 
| 



nearbyInHyperlinkHostInOrderWithin( 
distance, 
terms 
) 
| 
nearbyInHyperlinkNonHostInOrderWithin( 
distance, 
terms 
) 
| 
nearbyInHypertextInOrderWithin( 
distance, 
terms 
) 


Among 
the 
predicates 
listed 
under 
general 
features, 
isaQueryWord 
is 
true 
if 
the 
given 
word 
was 
in 
the 
user's 
search 
query. 
The 
predicates 
query-
WordAtLeftSpotInWindow, 
queryWordAtCenterOfWindow, 
and 
queryWordAt
RightSpotInWindow 
determine 
whether 
one 
of 
the 
user's 
query 
words 
is 
at 
a 
specic 
location 
in 
the 
sliding 
window. 


general 
features 
. 


isaQueryWord( 
word 
) 
| 
queryWordAtLeftSpotInWindow( 
position 
) 
| 
queryWordAtCenterOfWindow 
| 
queryWordAtRightSpotInWindow( 
position 
) 
| 
insideTitle 
| 
insideURL 
| 
insideSection 
| 
insideHypertext 
| 
insideHyperlink 
| 
insideForm 
| 
insideFrameSet 
| 
insideTable 
| 
insideImageCaption 
| 
insideEmailAddress 
| 
insideMetaWords 
| 
insideEmphasized 
| 
insideAddress 
| 
knownWhenPageExpires 
| 
knownWhenLastModied 




Among 
the 
predicates 
listed 
under 
numeric 
valued 
features, 
fractPosi


tionOnPage 
determines 
the 
ratio 
of 
the 
position 
of 
the 
word 
in 
the 
center 
of 


window 
to 
the 
total 
number 
of 
words 
on 
the 
page. 


numeric 
operations 
. 
numeric 
valued 
features 
numeric 
operators 
numeric 
valued 
features 
| 
numeric 
valued 
features 
numeric 
operators 
integer 


numeric 
operators 
. 
\<” 
| 
\” 
| 
\==” 
| 
\>” 
| 
\” 


numeric 
valued 
features 
. 
numberOfImagesOnPage 
| 
numberOfURLsOnPage 
| 
numberOfTablesOnPage 
| 
numberOfFormsOnPage 
| 
numberOfFramesOnPage 
| 
numberOfQueryWordsOnPage 
| 
numberOfQueryWordsInTitle 
| 
numberOfQueryWordsInURL 
| 
numberOfQueryWordsInCurrentSectionTitle 
| 
numberOfQueryWordsInCurrentHyperlink 
| 
numberOfQueryWordsInWindow 
| 
numberOfQueryWordsInLeftSideOfWindow 
| 
numberOfQueryWordsInRightSideOfWindow 
| 
numberOfWordsOnPage 
| 
numberOfWordsInTitle 
| 
numberOfFieldsInUrl 
| 
numberOfWordsInCurrentSectionTitle 
| 
numberOfFieldsInCurrentHyperlink 
| 
numberOfWordsInWindow 
| 
numberOfWordsInLeftSideOfWindow 
| 
numberOfWordsInRightSideOfWindow 
| 
numberOfWordsInSectionTitle( 
integer 
) 
| 
numberOfQueryWordsInSectionTitle( 
integer 
) 
| 




fractPositionOnPage 
| 
pageExpiresAt 
| 
pageLastModiedAt 


part 
of 
speech 
tag 
. 
\commonNoun” 
| 
\properNoun” 
| 
\pluralNoun” 
| 
\pluralProperNoun” 
| 
\baseFormVerb” 
| 
\pastTenseVerb” 
| 
\presentParticipleVerb” 
| 
\pastParticipleVerb” 
| 
\nonThirdPersonSingPresentVerb” 
| 
\thirdPersonSingPresentVerb” 
| 
\otherAdjective” 
| 
\comparativeAdjective” 
| 
\superlativeAdjective” 
| 
\otherAdverb” 
| 
\comparativeAdverb” 
| 
\superlativeAdverb” 
| 
\hyphenatedWord” 
| 
\hyphenDroppedAfter” 
| 
\hyphenDroppedBefore” 
| 
\allCaps” 
| 
\leadingCaps” 
| 
\title” 
| 
\cardinalNumber” 
| 
\punctuation” 
| 
\personalPronoun” 
| 
\possessivePronoun” 
| 
\preposition” 
| 
\determiner” 
| 
\coordinatingConjuntion” 
| 
\existentialThere” 
| 
\whDeterminer” 
| 
\whPronoun” 
| 
\possessiveWhPronoun” 
| 
\whAdverb” 
| 
\foreignWord” 
| 
\unknownWord” 


Actions 


The 
actions 
used 
in 
advice 
rules 
are 
as 
follows: 


A1. 
suggest 
doing 
both 


This 
action 
adds 
a 
moderately 
weighted 
link 
from 
the 
rule's 
new 
hidden 
unit 
in 
both 
the 
ScorePage 
and 
the 
ScoreLink 
networks 
into 
the 
output 
units 
of 
these 
networks. 


A2. 
suggest 
showing 
page 


This 
action 
adds 
a 
moderately 
weighted 
link 
from 
the 
rule's 
new 
hidden 
unit 
in 
the 
ScorePage 
network 
into 
the 
network's 
output 
unit. 


A3. 
suggest 
following 
link 


This 
action 
adds 
a 
moderately 
weighted 
link 
from 
the 
rule's 
new 
hidden 
unit 
in 
the 
ScoreLink 
network 
into 
the 
network's 
output 
unit. 




A4. 
avoid 
both 
This 
action 
adds 
a 
link 
with 
a 
moderately 
negative 
weight 
from 
the 
rule's 
new 
hidden 
unit 
in 
both 
the 
ScorePage 
and 
the 
ScoreLink 
networks 
into 
the 
output 
units 
of 
these 
networks. 


A5. 
avoid 
showing 
page 
This 
action 
adds 
a 
link 
with 
a 
moderately 
negative 
weight 
from 
the 
rule's 
new 
hidden 
unit 
in 
the 
ScorePage 
network 
into 
the 
network's 
output 
unit. 


A6. 
avoid 
following 
link 
This 
action 
adds 
a 
link 
with 
a 
moderately 
negative 
weight 
from 
the 
rule's 
new 
hidden 
unit 
in 
the 
ScoreLink 
network 
into 
the 
network's 
output 
unit. 


Actions 
can 
be 
prexed 
by 
the 
following 
four 
modiers: 


• 
Denitely: 
Assuming 
the 
conditions 
of 
a 
`denite’ 
rule 
are 
fully 
met, 
the 
link 
out 
of 
the 
sigmoidal 
hidden 
unit 
representing 
the 
rule 
will 
have 
a 
weight 
of 
. 
11.25, 
for 
actions 
A1 
through 
A3, 
and 
. 
-11.25, 
for 
actions 
A4 
through 
A6. 
• 
Strongly: 
Assuming 
the 
conditions 
of 
a 
`strong’ 
rule 
are 
fully 
met, 
the 
link 
out 
of 
the 
sigmoidal 
hidden 
unit 
representing 
the 
rule 
will 
have 
a 
weight 
of 
. 
7.5, 
for 
actions 
A1 
through 
A3, 
and 
. 
-7.5, 
for 
actions 
A4 
through 
A6. 
• 
Moderately: 
Assuming 
the 
conditions 
of 
a 
`moderate’ 
rule 
are 
fully 
met, 
the 
link 
out 
of 
the 
sigmoidal 
hidden 
unit 
representing 
the 
rule 
will 
have 
a 
weight 
of 
. 
2.5, 
for 
actions 
A1 
through 
A3, 
and 
. 
-2.5, 
for 
actions 
A4 
through 
A6. 
When 
an 
action 
does 
not 
have 
modier, 
then 
\moderately” 
is 
used 
as 
the 
default 
modier. 



• 
Weakly: 
Assuming 
the 
conditions 
of 
a 
`weak’ 
rule 
are 
fully 
met, 
the 
link 
out 
of 
the 
sigmoidal 
hidden 
unit 
representing 
the 
rule 
will 
have 
a 
weight 
of 
. 
0.75, 
for 
actions 
A1 
through 
A3, 
and 
. 
-0.75, 
for 
actions 
A4 
through 
A6. 
• 
With 
zero-valued 
weights: 
Assuming 
the 
conditions 
of 
a 
`zero-weight’ 
rule 
are 
fully 
met, 
the 
link 
out 
of 
the 
sigmoidal 
hidden 
unit 
representing 
the 
rule 
will 
have 
a 
weight 
of 
0 
for 
actions 
A1 
through 
A6. 
Advice 
Interfaces 


The 
user 
can 
give 
advice 
to 
Wawa 
through 
three 
dierent 
interfaces: 
(i) 
the 
\basic” 
interface, 
(ii) 
the 
\intermediate” 
interface, 
and 
(iii) 
the 
\advanced” 
interface. 
All 
three 
of 
these 
interfaces 
are 
menu-driven 
and 
dier 
only 
in 
the 
number 
of 
advice 
constructs 
that 
they 
oer. 
The 
user 
can 
also 
compose 
her 
advice 
in 
her 
favorite 
text 
editor 
and 
give 
a 
text 
le 
to 
Wawa. 


Wawa's 
advice 
parser 
is 
quite 
user-friendly 
by 
forgiving 
many 
of 
the 
user's 
syntactical 
mistakes. 
For 
example, 
the 
advice 
parser 
does 
not 
require 
the 
word 
\THEN” 
before 
an 
action 
and 
accepts 
the 
word 
\IF” 
as 
opposed 
to 
\WHEN” 
in 
advice 
rules. 
Also, 
when 
a 
major 
syntactical 
error 
is 
made, 
Wawa's 
advice 
parser 
just 
ignores 
the 
\bad” 
rule 
and 
proceeds 
with 
normal 
operations. 


Finally, 
since 
advice 
les 
can 
get 
long, 
Wawa's 
advice 
language 
allows 
the 
user 
to 
annotate 
advice 
by 
placing 
comments 
inside 
brackets 
or 
after 
the 
# 
symbol. 



Appendix 
B 


Advice 
Used 
in 
the 
Home-Page 
Finder 


This 
appendix 
presents 
the 
rules 
used 
to 
create 
the 
home-page 
nder 
from 
the 
generic 
Wawa-IR 
(see 
Chapter 
5 
for 
details 
on 
this 
case 
study). 
These 
rules 
contain 
the 
following 
13 
variables: 


1. 
?FirstName 
. 
First 
name 
of 
a 
person 
(e.g., 
Robert) 
2. 
?FirstInitial 
. 
First 
initial 
of 
a 
person 
(e.g., 
R) 
3. 
?NickNameA 
. 
First 
nickname 
of 
a 
person 
(e.g., 
Rob) 
4. 
?NickNameB 
. 
Second 
nickname 
of 
a 
person 
(e.g., 
Bob) 
5. 
?MiddleName 
. 
Middle 
name 
of 
a 
person 
(e.g., 
Eric) 
6. 
?MiddleInitial 
. 
Middle 
initial 
of 
a 
person 
(e.g., 
E) 
7. 
?LastName 
. 
Last 
name 
of 
a 
person 
(e.g., 
Smith) 
8. 
?MiscWord1 
. 
First 
miscellaneous 
word 
9. 
?MiscWord2 
. 
Second 
miscellaneous 
word 
10. 
?MiscWord3 
. 
Third 
miscellaneous 
word 
11. 
?UrlHostWord1 
. 
Third 
word 
from 
the 
end 
of 
a 
host 
url 
(e.g., 
cs 
in 
http://www.cs.wisc.edu) 
12. 
?UrlHostWord2 
. 
Second 
word 
from 
the 
end 
of 
a 
host 
url 
(e.g., 
wisc 
in 
http://www.cs.wisc.edu) 
13. 
?UrlHostWord3 
. 
Last 
word 
in 
a 
host 
(e.g., 
edu 
in 
http://www.cs.wisc.edu) 
In 
my 
experiments, 
I 
only 
used 
variables 
numbered 
1 
through 
7 
since 
I 
wanted 
to 
fairly 
compare 
Wawa's 
home-page 
nder 
to 
existing 
alternative 
approaches. 
That 
is, 
I 
did 
not 
provide 
any 
values 
for 
variables 
numbered 
8 
through 
13. 



I 
introduced 
these 
variables 
and 
even 
wrote 
some 
rules 
about 
them 
only 
to 
illustrate 
that 
there 
is 
other 
information 
besides 
a 
person's 
name 
that 
might 
be 
helpful 
in 
nding 
his/her 
home-page. 


The 
syntax 
and 
semantics 
of 
the 
advice 
language 
used 
below 
are 
described 
in 
Appendix 
A. 
Before 
I 
present 
my 
home-page 
nder 
rules, 
I 
will 
dene 
some 
non-terminal 
tokens 
(used 
in 
my 
rules) 
in 
Backus-Naur 
form 
(Aho, 
Sethi, 
and 
Ullman 
1986). 
These 
non-terminal 
tokens 
are: 


rst 
names 
. 
?FirstName 
| 
?FirstInitial 
| 
?NicknameA 
| 
?NicknameB 


middle 
names 
. 
?MiddleName 
| 
?MiddleInitial 


full 
name 
. 
rst 
names 
middle 
names 
?LastName 


regular 
name 
. 
rst 
names 
?LastName 


home 
page 
words 
. 
home 
| 
homepage 
| 
home-page 


person 
. 
full 
name 
| 
regular 
name 


The 
following 
rules 
look 
for 
pages 
with 
the 
person's 
name 
and 
either 
the 
phrase 
\home 
page 
of” 
or 
the 
words 
\home", 
\homepage", 
or 
\home-page” 
in 
the 
title. 
Recall 
that 
the 
function 
consecutiveInTitle 
takes 
a 
sequences 
of 
words 
and 
returns 
true 
if 
they 
form 
a 
phrase 
inside 
the 
title 
of 
a 
page. 
The 
symbol 
\.” 
is 
Wawa's 
\wild 
card” 
symbol. 
It 
is 
a 
placeholder 
that 
matches 
any 
single 
word 
or 
punctuation. 


home 
page 
rules 
A 
. 


WHEN 
consecutiveInTitle( 
\home 
page 
of” 
person 
) 


THEN 
denitely 
suggest 
showing 
page 
| 


WHEN 
consecutiveInTitle( 
person 
\'s” 
home 
page 
words 
) 


THEN 
denitely 
suggest 
showing 
page 
| 


WHEN 
consecutiveInTitle( 
person 
home 
page 
words 
) 


THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutiveInTitle( 
rst 
names 
. 
?LastName 
\'s” 
home 
page 
words 
) 
THEN 
suggest 
showing 
page 
| 




WHEN 
consecutiveInTitle( 
\home 
page 
of” 
rst 
names 
. 
?LastName 
) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutiveInTitle( 
\home 
page 
of” 
. 
?LastName 
) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutiveInTitle( 
\home 
page 
of” 
.. 
?LastName 
) 
THEN 
weakly 
suggest 
showing 
page 
| 


WHEN 
consecutiveInTitle( 
person 
. 
home 
page 
words 
) 
THEN 
suggest 
showing 
page 


The 
next 
set 
of 
rules 
look 
for 
links 
to 
the 
person's 
home-page. 
Recall 
that 
the 
function 
consecutive 
takes 
a 
sequences 
of 
words 
and 
returns 
true 
if 
the 
words 
appear 
as 
a 
phrase 
on 
the 
page. 


home 
page 
rules 
B 
. 


WHEN 
consecutive( 
person 
\'s” 
home 
page 
words 
) 
THEN 
denitely 
suggest 
following 
link 
| 


WHEN 
consecutive( 
\home 
page 
of” 
person 
) 
THEN 
denitely 
suggest 
following 
link 
| 


WHEN 
consecutive(rst 
names 
. 
?LastName 
\'s” 
home 
page 
words) 
THEN 
strongly 
suggest 
following 
link 
| 


WHEN 
consecutive( 
\home 
page 
of” 
rst 
names 
. 
?LastName 
) 
THEN 
strongly 
suggest 
following 
link 
| 


WHEN 
consecutive( 
\home 
page 
of” 
. 
?LastName 
) 
THEN 
suggest 
following 
link 
| 


WHEN 
consecutive( 
person 
) 
THEN 
strongly 
suggest 
following 
link 
| 


WHEN 
consecutive( 
?LastName 
) 
THEN 
suggest 
following 
link 




The 
following 
rules 
look 
for 
pages 
and 
links 
leading 
to 
pages 
with 
the 
person's 
name 
in 
the 
title, 
but 
not 
in 
a 
question 
format. 
I 
do 
not 
want 
both 
the 
person's 
name 
and 
a 
question 
mark 
in 
a 
page's 
title 
because 
it 
could 
represent 
a 
query 
on 
that 
person 
and 
not 
the 
person's 
home-page. 


home 
page 
rules 
C 
. 


WHEN 
( 
NOT( 
anywhereInTitle( 
\?” 
) 
) 
AND 
consecutiveInTitle(regular 
name 
noneOf(home 
page 
words))) 
THEN 
strongly 
suggest 
doing 
both 
| 


WHEN 
( 
NOT( 
anywhereInTitle( 
\?” 
) 
) 
AND 
consecutiveInTitle(rst 
names 
. 
?LastName 
noneOf(home 
page 
words))) 
THEN 
strongly 
suggest 
doing 
both 
| 


WHEN 
( 
NOT( 
anywhereInTitle( 
\?” 
) 
) 
AND 
consecutiveInTitle( 
rst 
names 
) 
AND 
anywhereInTitle( 
?LastName 
)) 


THEN 
suggest 
doing 
both 
| 


WHEN 
( 
NOT( 
anywhereInTitle( 
\?” 
) 
) 
AND 
consecutiveInTitle( 
?LastName 
\,” 
rst 
names 
)) 
THEN 
suggest 
doing 
both 
| 


WHEN 
consecutive( 
rst 
names 
\'s” 
home 
page 
words 
) 
THEN 
suggest 
doing 
both 
| 


WHEN 
consecutive( 
?LastName 
home 
page 
words 
) 
THEN 
suggest 
doing 
both 
| 


WHEN 
consecutive( 
\my” 
home 
page 
words 
) 
THEN 
suggest 
doing 
both 




This 
next 
rule 
looks 
for 
home-pages 
that 
might 
lead 
to 
other 
home-pages. 


home 
page 
rules 
D 
. 


WHEN 
( 
NOT(anywhereInTitle(\?")) 
AND 


( 
anywhereInTitle(\home 
page") 
OR 
anywhereInTitle(\home-page") 
OR 
anywhereInTitle(\homepage") 
) 
) 


THEN 
suggest 
following 
link 


The 
rule 
below 
seeks 
pages 
that 
have 
the 
person's 
last 
name 
near 
an 
image. 
I 
conjecture 
that 
the 
image 
might 
be 
that 
person's 
picture. 


home 
page 
rules 
E 
. 


WHEN 
( 
insideImageCaption() 
AND 
consecutive( 
?LastName 
)) 
THEN 
suggest 
doing 
both 


The 
next 
set 
of 
rules 
look 
for 
pages 
and 
links 
that 
include 
some 
of 
the 
query 
words 
given 
by 
the 
user 
(i.e., 
bindings 
for 
some 
of 
the 
variables). 
These 
rules 
use 
functions 
like 
numberOfQueryWordsOnPage, 
which 
(obviously) 
returns 
the 
number 
of 
query 
words 
on 
the 
page. 
This 
set 
also 
includes 
rules 
that 
check 
for 
urls 
and 
hyperlinks 
containing 
the 
\?” 
symbol. 
Such 
urls 
and 
hyperlinks 
point 
to 
pages 
generated 
by 
search 
engines 
(aka, 
query 
pages) 
and 
should 
be 
avoided. 


home 
page 
rules 
F 
. 


WHEN 
( 
( 
insideEmailAddress() 
OR 
insideAddress() 
) 
AND 
( 
numberOfQueryWordsInWindow() 
. 
1)) 
THEN 
weakly 
suggest 
doing 
both 
| 


WHEN 
( 
numberOfQueryWordsOnPage() 
< 
1) 
THEN 
avoid 
following 
link 
AND 
denitely 
avoid 
showing 
page 
| 


WHEN 
anywhereInURL( 
\?” 
) 
THEN 
strongly 
avoid 
following 
link 
& 
denitely 
avoid 
showing 
page 
| 




WHEN 
anywhereInCurrentHyperlink( 
\?” 
) 
THEN 
strongly 
avoid 
following 
link 
| 


WHEN 
( 
anywhereInURL( 
. 
) 
AND 
NOT(anywhereInURL( 
\?” 
)) 
AND 
( 
numberOfQueryWordsInURL() 
. 
1)) 
THEN 
weakly 
suggest 
both 


The 
next 
set 
of 
rules 
look 
for 
pages 
and 
links 
that 
include 
some 
of 
the 
query 
words 
given 
by 
the 
user. 
They 
use 
a 
function 
called 
ScaleLinearlyBy 
which 
takes 
a 
set 
of 
conditions 
and 
returns 
a 
number 
between 
0 
and 
1, 
proportional 
to 
the 
number 
of 
conditions 
satised 
(see 
Appendix 
A 
for 
details). 


home 
page 
rules 
G 
. 


ScaleLinearlyBy( 
numberOfQueryWordsInWindow() 
) 
THEN 
suggest 
both 
| 


ScaleLinearlyBy( 
numberOfQueryWordsInTitle() 
) 
THEN 
suggest 
showing 
page 
AND 
weakly 
suggest 
following 
link 


The 
following 
rules 
look 
for 
pages 
and 
links 
that 
include 
a 
combination 
of 
the 
words 
\home 
page,” 
\home-page,” 
\homepage,” 
\home,” 
\page,” 
\directory,” 
and 
\people” 
either 
on 
the 
page 
itself 
or 
in 
its 
URL. 


home 
page 
rules 
H 
. 


WHEN 
consecutive( 
home 
page 
words 
\directory” 
) 
THEN 
strongly 
suggest 
following 
link 
| 


WHEN 
consecutiveInaSection( 
home 
page 
words 
\directory” 
) 
THEN 
suggest 
following 
link 
| 


WHEN 
consecutiveInHyperText( 
home 
page 
words 
) 
THEN 
suggest 
following 
link 
| 


WHEN 
consecutiveInaSection( 
home 
page 
words 
) 
THEN 
weakly 
suggest 
doing 
both 
| 




WHEN 
( 
consecutive( 
\home 
page” 
) 
OR 


consecutive( 
anyOf( 
\homepage” 
\home-page” 
) 
) 
) 


THEN 
weakly 
suggest 
doing 
both 
| 


WHEN 
( 
anywhereInURL(\home") 
OR 
anywhereInURL(\page") 
OR 


anywhereInURL(\people") 
OR 
anywhereInURL(\homepage") 
OR 


anywhereInURL(\home-page") 
) 


THEN 
suggest 
doing 
both 


The 
subsequent 
two 
rules 
attempt 
to 
nd 
when 
a 
page 
was 
last 
modied. 
The 
function 
pageLastModiedAt() 
determines 
the 
number 
of 
days 
from 
today 
since 
the 
page 
was 
modied. 
If 
the 
page 
does 
not 
specify 
when 
it 
was 
last 
modied, 
the 
value 
for 
the 
function 
pageLastModiedAt() 
is 
zero. 
If 
the 
page 
reports 
the 
last 
time 
it 
was 
modied, 
I 
convert 
the 
time 
into 
the 
interval 
[0, 
1], 
where 
0 
means 
never 
and 
1 
means 
the 
page 
was 
changed 
today. 
Then, 
the 
value 
of 
pageLastModiedAt() 
is 
1 
minus 
the 
scaled 
value. 


home 
page 
rules 
I 
. 


ScaleLinearlyBy( 
pageLastModiedAt() 
) 
THEN 
weakly 
suggest 
showing 
page 
AND 
very 
weakly 
suggest 
following 
link 


The 
next 
three 
rules 
use 
the 
function 
consecutiveInURL. 
As 
described 
in 
Appendix 
A, 
this 
function 
takes 
a 
sequences 
of 
words 
and 
returns 
true 
if 
the 
words 
form 
a 
phrase 
in 
the 
URL 
of 
the 
page. 
These 
rules 
look 
for 
pages 
that 
have 
a 
URL 
containing 
the 
person's 
name 
and 
possibly 
the 
words 
\htm” 
or 
\html.” 
In 
a 
URL, 
when 
a 
login 
name 
is 
prexed 
with 
, 
it 
usually 
stands 
for 
the 
given 
user's 
home 
directory. 


home 
page 
rules 
J 
. 


WHEN 
consecutiveInURL( 
\” 
anyOf( 
rst 
names 
?LastName 
)) 
THEN 
weakly 
suggest 
doing 
both 
| 


WHEN 
consecutiveInURL( 
person 
anyOf( 
\htm” 
\html” 
) 
) 
THEN 
denitely 
suggest 
showing 
page 




The 
following 
ve 
rules 
look 
for 
pages 
and 
links 
that 
have 
the 
phrase 
\?First-
Name 
?LastName” 
anywhere 
on 
them 
or 
in 
their 
title, 
their 
URL, 
one 
of 
their 
hypertexts, 
or 
one 
of 
their 
hyperlinks. 


home 
page 
rules 
K 
. 


WHEN 
consecutive( 
?FirstName 
?LastName 
) 
THEN 
strongly 
do 
both 
| 


WHEN 
consecutiveInURL( 
?FirstName 
?LastName 
) 
THEN 
strongly 
do 
both 
| 


WHEN 
consecutiveInTitle( 
?FirstName 
?LastName 
) 
THEN 
strongly 
do 
both 
| 


WHEN 
consecutiveInHypertext( 
?FirstName 
?LastName 
) 
THEN 
strongly 
do 
both 
| 


WHEN 
consecutiveInHyperlink( 
?FirstName 
?LastName 
) 
THEN 
strongly 
do 
both 


The 
next 
three 
rules 
avoid 
pages 
that 
have 
the 
phrase 
\404 
not 
found” 
in 
their 
title. 


home 
page 
rules 
L 
. 


WHEN 
titleStartsWith( 
anyOf( 
\404” 
\le” 
) 
\not 
found” 
) 
THEN 
strongly 
avoid 
both 
| 


WHEN 
titleEndsWith( 
anyOf( 
\404” 
\le” 
) 
\not 
found” 
) 
THEN 
strongly 
avoid 
both 
| 


WHEN 
anywhereInTitle( 
\404 
not 
found” 
) 
THEN 
avoid 
both 


The 
following 
rules 
contain 
advice 
about 
commonly 
used 
words 
on 
a 
person's 
homepage 
like 
\cv", 
\resume", 
etc. 
The 
function 
NofM 
used 
in 
this 
set 
of 
rules 
takes 
an 
integer, 
N 
and 
a 
list 
of 
conditions 
of 
size 
M. 
It 
returns 
true 
if 
at 
least 
N 
of 
the 
M 
conditions 
are 
true. 
The 
function 
anywhereOnPage(\555 
. 
1234") 




is 
true 
if 
a 
telephone 
number 
is 
on 
the 
page. 
Otherwise, 
it 
returns 
false. 
These 
rules 
were 
removed 
from 
the 
original 
set 
of 
76 
advice 
rules 
to 
create 
a 
new 
set 
of 
initial 
advice 
containing 
only 
48 
rules 
(see 
Section 
5.3 
for 
more 
details). 
I 
marked 
the 
non-terminal 
representing 
these 
rules 
with 
the 
† 
symbol 
to 
distinguish 
them 
from 
the 
other 
rules. 


. 


home 
page 
rules 
M 
. 


WHEN 
( 
insideMetaWords() 
AND 


( 
consecutive( 
\home 
page” 
) 
OR 
consecutive( 
anyOf( 
\homepage” 
\home-page” 
) 
) 
OR 
consecutive( 
\personal” 
anyOf( 
\info” 
\information” 
) 
) 
) 
) 


THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive( 
\curriculum” 
anyOf( 
\vitae” 
\vita” 
) 
) 
THEN 
weakly 
suggest 
doing 
both 
| 


WHEN 
consecutiveInHypertext( 
\curriculum” 
anyOf(\vitae” 
\vita") 
) 
THEN 
suggest 
doing 
both 
| 


WHEN 
consecutiveInHypertext( 
\my” 
anyOf(\vitae” 
\vita") 
) 
THEN 
suggest 
following 
link 
| 


WHEN 
consecutiveInHypertext( 
\my” 
anyOf(\cv” 
\resume") 
) 
THEN 
suggest 
following 
link 
| 


WHEN 
consecutive( 
\my” 
anyOf( 
\resume” 
\cv” 
\vita” 
\vitae” 
) 
) 
THEN 
suggest 
both 
| 


WHEN 
consecutive( 
\my” 
anyOf( 
\homepage” 
\home” 
) 
) 
THEN 
suggest 
both 
| 


WHEN 
consecutiveInaSection(personal 
anyOf(\info” 
\information")) 
THEN 
weakly 
suggest 
doing 
both 
| 




WHEN 
NofM( 
2, 
anywhereInSections( 
\personal” 
) 
anywhereInSections( 
\information” 
) 
anywhereInSections( 
\info” 
) 
anywhereInSections( 
\projects” 
) 
anywhereInSections( 
\interests” 
) 
) 


THEN 
weakly 
suggest 
doing 
both 
| 


ScaleLinearlyBy( 


consecutive(anyOf(?LastName 
?MiscWord1 
?MiscWord2 
?MiscWord3 
) 
), 
( 
anywhereInWindow(\email") 
OR 
anywhereInWindow(\e-mail") 
OR 
anywhereInWindow(\mailto") 
), 
( 
anywhereInWindow(\phone") 
OR 
anywhereInWindow(\555-1234") 
OR 
anywhereInWindow(\fax") 
OR 
anywhereInWindow(\telephone") 
), 
( 
anywhereInWindow(\department") 
OR 
anywhereInWindow(\work") 
OR 
anywhereInWindow(\oce") 
OR 
anywhereInWindow(\dept") 
), 
( 
anywhereInWindow(\address") 
OR 
anywhereInWindow(\mailing") 
) 
) 
) 
THEN 
strongly 
suggest 
doing 
both 
| 


ScaleLinearlyBy( 


consecutive(anyOf(?LastName 
?MiscWord1 
?MiscWord2 
?MiscWord3 
) 
), 
( 
anywhereOnPage(\email") 
OR 
anywhereOnPage(\e-mail") 
OR 
anywhereOnPage(\mailto") 
), 
( 
anywhereOnPage(\phone") 
OR 
anywhereOnPage(\fax") 
OR 
anywhereOnPage(\555-1234") 
OR 
anywhereOnPage(\telephone") 
), 
( 
anywhereOnPage(\department") 
OR 
anywhereOnPage(\work") 
OR 
anywhereOnPage(\oce") 
OR 
anywhereOnPage(\dept") 
), 
( 
anywhereOnPage(\address") 
OR 
anywhereOnPage(\mailing") 
) 
) 
) 
THEN 
suggest 
doing 
both 
| 


WHEN 
consecutive(anyOf(\research” 
\recent") 
anyOf(\summary” 
\publications") 
) 
THEN 
weakly 
suggest 
doing 
both 
| 


WHEN 
consecutive( 
\recent 
publications” 
) 
THEN 
weakly 
suggest 
doing 
both 
| 


WHEN 
( 
insideEmailAddress() 
AND 
consecutive( 
?UrlHostWord1 
?UrlHostWord2 
?UrlHostWord3 
)) 
THEN 
suggest 
both 
| 



WHEN 
( 
insideEmailAddress() 
AND 
consecutive( 
?UrlHostWord2 
?UrlHostWord3 
)) 


THEN 
suggest 
both 
| 


WHEN 
consecutiveInURL(?UrlHostWord1 
?UrlHostWord2 
?UrlHostWord3 
) 


THEN 
suggest 
showing 
page 
| 


WHEN 
consecutiveInUrl( 
?UrlHostWord2 
?UrlHostWord3 
) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutiveInHyperlink(?UrlHostWord1 
?UrlHostWord2 
?UrlHostWord3 
) 


THEN 
suggest 
following 
link 
| 


WHEN 
consecutiveInHyperlink( 
?UrlHostWord2 
?UrlHostWord3 
) 
THEN 
suggest 
following 
link 
| 


ScaleLinearlyBy( 
anywhereOnPage(\bio"), 
anywhereOnPage(\interests"), 
anywhereOnPage(\hobbies"), 
anywhereOnPage(\resume"), 
anywhereOnPage(\cv"), 
anywhereOnPage(\vita"), 
anywhereOnPage(\vitae"), 
anywhereOnPage(\degrees"), 
anywhereOnPage(\employment"), 
anywhereOnPage( 
\oce"), 
anywhereOnPage(\courses"), 
anywhereOnPage(\classes"), 
anywhereOnPage(\education"), 
anywhereOnPage(\dept") 
) 


THEN 
strongly 
suggest 
showing 
page 
| 



ScaleLinearlyBy( 
anywhereInWindow(\bio"), 
anywhereInWindow(\interests"), 
anywhereInWindow(\hobbies"), 
anywhereInWindow(\resume"), 
anywhereInWindow(\cv"), 
anywhereInWindow(\vita"), 
anywhereInWindow(\vitae"), 
anywhereInWindow(\degrees"), 
anywhereInWindow(\employment"), 
anywhereInWindow(\oce"), 
anywhereInWindow(\courses"), 
anywhereInWindow(\classes"), 
anywhereInWindow(\education"), 
anywhereInWindow(\dept") 
) 


THEN 
strongly 
suggest 
following 
link 
| 


WHEN 
( 
anywhereInWindow(\links") 
AND 
consecutive(anyOf(\interests” 
\interesting” 
\cool")) 
) 


THEN 
suggest 
showing 
page 
| 


WHEN 
( 
anywhereInWindow(\links") 
AND 
consecutive(anyOf(\recommended” 
\stu")) 
) 


THEN 
suggest 
showing 
page 
| 


WHEN 
consecutiveInaSection(anyOf(\topics” 
\areas") 
\of 
interest") 
THEN 
suggest 
doing 
both 
| 


WHEN 
consecutiveInTitle( 
\main 
page” 
) 
THEN 
weakly 
suggest 
doing 
both 
| 


WHEN 
( 
consecutive( 
\contact 
information” 
) 
AND 
anywhereOnPage( 
?LastName 
)) 


THEN 
strongly 
do 
both 
| 


WHEN 
consecutive( 
\check 
your 
spelling” 
) 
THEN 
strongly 
avoid 
both 
| 


WHEN 
consecutive( 
\search 
tips” 
) 
THEN 
strongly 
avoid 
both 



The 
union 
of 
the 
rules 
in 
the 
non-terminal 
tokens 
home 
page 
rules 
A 
through 
home 
page 
rules 
M 
create 
the 
76 
advice 
rules 
used 
in 
most 
of 
the 
home-page 
nder 
experiments. 
The 
union 
of 
the 
rules 
in 
home 
page 
rules 
A 
through 
home 
page 
rules 
L 
create 
the 
48 
advice 
rules 
used 
in 
some 
of 
the 
1998 
home-page 
nder 
experiments. 




Appendix 
C 


Advice 
Used 
in 
the 
Seminar-Announcement 
Extractor 


This 
appendix 
presents 
the 
advice 
rules 
for 
the 
speaker 
and 
location 
slots 
of 
the 
seminar-announcement 
extractor 
agent. 
Recall 
that 
the 
function 
named 
consecutive 
takes 
a 
sequence 
of 
words 
and 
returns 
true 
if 
they 
appear 
as 
a 
phrase 
on 
the 
page. 
Otherwise, 
it 
returns 
false. 


To 
extract 
the 
speaker 
name, 
I 
used 
the 
following 
four 
variables: 


1. 
?FirstName 
. 
First 
name 
or 
initial 
of 
a 
person 
2. 
?NickName 
. 
Nickname 
of 
a 
person 
3. 
?MiddleName 
. 
Middle 
name 
or 
initial 
of 
a 
person 
4. 
?LastName 
. 
Last 
name 
of 
a 
person 
The 
advice 
rules 
used 
for 
the 
speaker 
slot 
in 
Backus-Naur 
form 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
are 
listed 
below. 
The 
non-terminal 
tokens 
talk 
VB 
and 
talk 
VBG 
used 
in 
the 
rules 
refer 
to 
verbs 
in 
base 
form 
and 
present 
participle 
form, 
respectively. 
Recall 
that 
I 
choose 
not 
to 
do 
stemming 
of 
words 
in 
this 
study. 


spk 
rules 
. 


WHEN 
consecutive(title 
spk 
name) 


THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(spk 
name 
, 
degree) 
THEN 
strongly 
suggest 
showing 
page 
| 




WHEN 
consecutive(spk 
intro 
. 
spk 
name) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(spk 
name 
\'s” 
talk 
noun) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(spk 
name 
\will” 
talk 
VB) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(spk 
name 
\will 
be” 
talk 
VBG) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(\presented 
by” 
spk 
name) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(\talk 
by” 
spk 
name) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
spk 
name 
THEN 
weakly 
suggest 
showing 
page 


spk 
name 
. 


?LastName/NNP 
| 


?FirstName/NNP 
?LastName/NNP 
| 


?NickName/NNP 
?LastName/NNP 
| 


?FirstName/NNP 
?MiddleName/NNP 
?LastName/NNP 
| 


?NickName/NNP 
?MiddleName/NNP 
?LastName/NNP 


title 
. 
\mr” 
| 
\ms” 
| 
\mrs” 
| 
\dr” 
| 
\prof” 
| 
\professor” 
| 
\mr.” 
| 
\ms.” 
| 
\mrs.” 
| 
\dr.” 
| 
\prof.” 
degree 
. 
\ba” 
| 
\bs” 
| 
\ms” 
| 
\ma” 
| 
\jd” 
| 
\md” 
| 
\phd” 
| 
\b.a.” 
| 
\b.s.” 
| 
\m.s.” 
| 
\m.a.” 
| 
\j.d.” 
| 
\m.d.” 
| 
\ph.d.” 
spk 
intro 
. 
\visitor” 
| 
\who” 
| 
\seminar” 
| 
\lecturer” 
| 
\colloquium” 
| 
\speaker” 
| 
\talk” 


talk 
noun 
. 
\talk” 
| 
\presentation” 
| 
\lecture” 
| 
\speech” 
talk 
VB 
. 
\talk” 
| 
\lecture” 
| 
\speak” 
| 
\present” 




talk 
VBG 
. 
\talking” 
| 
\lecturing” 
| 
\speaking” 
| 
\presenting” 


To 
extract 
the 
location 
name, 
I 
used 
the 
following 
four 
variables: 


1. 
?LocNumber 
. 
A 
cardinal 
number 
representing 
a 
room 
number, 
a 
building 
number, 
etc 
2. 
?LocNameA 
. 
First 
word 
in 
the 
name 
of 
a 
building, 
a 
street, 
etc 
3. 
?LocNameB 
. 
Second 
word 
in 
the 
name 
of 
a 
building, 
a 
street, 
etc 
4. 
?LocNameC 
. 
Third 
word 
in 
the 
name 
of 
a 
building, 
a 
street, 
etc 
The 
advice 
rules 
used 
for 
the 
location 
slot 
in 
Backus-Naur 
form 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
are: 


loc 
rules 
. 
WHEN 
consecutive(loc 


name 
tagged) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(loc 


name) 
THEN 
weakly 
suggest 
showing 
page 
| 


WHEN 
consecutive(loc 


name 
tagged 
loc 
tokens) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(\in” 
loc 
name 
loc 
tokens) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(loc 
tokens 
loc 
name 
tagged) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(loc 
tokens 
loc 
name) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(loc 
tokens 
. 
loc 
name 
tagged) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(loc 
tokens 
. 
loc 
name) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(loc 
intro 
. 
loc 
name 
tagged) 
THEN 
strongly 
suggest 
showing 
page 
| 




WHEN 
consecutive(loc 
intro 
. 
loc 
name) 
THEN 
suggest 
showing 
page 


loc 
name 
tagged 
. 
?LocNumber/CD 
| 
?LocNameA/NNP 
| 
?LocNumber/CD 
?LocNameA/NNP 
| 
?LocNameA/NNP 
?LocNumber/CD 
| 
?LocNameA/NNP 
?LocNameB/NNP 
?LocNumber/CD 
| 
?LocNumber/CD 
?LocNameA/NNP 
?LocNameB/NNP 
?LocNameC 
/NNP| 
?LocNameA/NNP 
?LocNameB/NNP 
?LocNameC 
/NNP 
?LocNumber/CD 


loc 
name 
. 
?LocNumber 
| 
?LocNameA 
| 
?LocNumber 
?LocNameA 
| 
?LocNameA 
?LocNumber 
| 
?LocNumber 
?LocNameA 
?LocNameB 
| 
?LocNameA 
?LocNameB 
?LocNumber 
| 
?LocNumber 
?LocNameA 
?LocNameB 
?LocNameC 
| 
?LocNameA 
?LocNameB 
?LocNameC 
?LocNumber 


loc 
tokens 
. 
\hall” 
| 
\auditorium” 
| 
\building” 
| 
\bldg” 
| 
\center” 
| 
\campus” 
| 
\school” 
| 
\university” 
| 
\conference” 
| 
\conf” 
| 
\room” 
| 
\rm” 
| 
\oor” 
| 
\inst” 
| 
\institute” 
| 
\wing” 
| 
\union” 
| 
\college” 
| 
\oce” 
| 
\lounge” 
| 
\lab” 
| 
\laboratory” 
| 
\library"| 
\classroom” 
| 
\tower” 
| 
\street” 
| 
\avenue” 
| 
\alley” 
| 
\road” 
| 
\drive"| 
\circle” 
| 
\trail” 
| 
\st” 
| 
\ave” 
| 
\rd” 
| 
\dr” 
| 
\cr” 
| 
\tr” 


loc 
intro 
. 
\place” 
| 
\where” 
| 
\location” 




Appendix 
D 


Advice 
Used 
in 
the 
Subcellular-Localization 
Extractor 


This 
appendix 
presents 
two 
sets 
of 
rules 
for 
the 
subcellular-localization 
ext
ractor. 
The 
rst 
set 
of 
rules 
were 
written 
by 
Michael 
Waddell, 
who 
is 
an 
M.D./Ph.D. 
student 
at 
University 
of 
Wisconsin-Madison. 
The 
second 
set 
of 
rules 
were 
written 
by 
me. 
Experiments 
on 
the 
second 
set 
of 
advice 
rules 
are 
reported 
in 
Eliassi-Rad 
and 
Shavlik 
(2001b). 


D.1 
Michael 
Waddell's 
Rules 
As 
mentioned 
above, 
the 
rules 
in 
this 
section 
were 
written 
by 
Michael 
Waddell, 
who 
is 
an 
M.D./Ph.D. 
student 
at 
University 
of 
Wisconsin-Madison. 
When 
writing 
these 
rules, 
Mr. 
Waddell 
focused 
only 
on 
how 
to 
teach 
the 
task 
to 
another 
person 
who 
could 
read 
basic 
English, 
but 
was 
unfamiliar 
with 
the 
eld 
of 
biochemistry 
and 
its 
terminology. 


Recall 
that 
the 
function 
named 
consecutive 
takes 
a 
sequence 
of 
words 
and 
returns 
true 
if 
they 
appear 
as 
a 
phrase 
on 
the 
page. 
Otherwise, 
it 
returns 
false. 
Also, 
· 
is 
one 
of 
Wawa's 
wild 
card 
tokens 
and 
represents 
any 
single 
word 
or 
punctuation. 
Wawa's 
other 
wild 
card 
token 
is 
, 
which 
represents 
zero 
of 
more 
words 
or 
punctuations. 



For 
this 
domain, 
I 
removed 
the 
stop 
words 
and 
stemmed 
the 
remaining 
words. 
The 
advice 
rules 
in 
Backus-Naur 
form 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
are 
listed 
below. 


protein 
location 
rules 
A 
. 


WHEN 
nounPhrase(?ProteinName) 
THEN 
strongly 
suggest 
show 
page 
| 


WHEN 
nounPhrase(?LocationName] 
THEN 
strongly 
suggest 
show 
page 


protein 
location 
rules 
B 
. 


WHEN 
(protein 
name/unknownWord 
or 
protein 
name/cardinalNumber) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
(consecutive(protein 
name 
protein 
associates) 
OR 
consecutive(protein 
associates 
protein 
name)) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(location 
marker 
. 
location 
name) 
THEN 
suggest 
showing 
page 


protein 
location 
rules 
C 
. 


WHEN 
consecutive(protein 
name 
\in” 
location 
name) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
. 
location 
marker) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
. 
location 
marker 
. 
location 
name) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
(consecutive(protein 
name 
. 
negatives 
. 
location 
marker) 
OR 
consecutive(protein 
name 
. 
location 
marker 
. 
negatives)) 
THEN 
avoid 
showing 
page 
| 




WHEN 
(consecutive(location 
marker 
. 
negatives 
. 
location 
name) 
OR 
consecutive(negatives 
. 
location 
marker 
. 
location 
name)) 
THEN 
avoid 
showing 
page 
| 


WHEN 
(consecutive(protein 
name 
. 
negatives 
. 
location 
marker 
. 
location 
name) 
OR 
consecutive(protein 
name 
. 
location 
marker 
. 
negatives 
. 
location 
name)) 
THEN 
strongly 
avoid 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
. 
passive 
voice 
location 
name) 
THEN 
suggest 
showing 
page 


protein 
name 
. 
?ProteinName/NounPhrase 


location 
name 
. 
?LocationName/NounPhrase 


protein 
associates 
. 
\protein” 
j\enzyme” 
| 
\mutant” 
| 
\gene” 


locations 
words 
. 
\accompany” 
| 
\accumulate” 
| 
\adhere” 
| 
\anchor” 
| 
\associate” 
| 
\attach” 
| 
\be” 
| 
\bind” 
| 
\coexist” 
| 
\cofractionate” 
| 
\conne” 
| 
\connect” 
| 
\contain” 
| 
\couple” 
| 
\deposit” 
| 
\discover” 
| 
\embed” 
| 
\encase” 
| 
\enclose” 
| 
\establish” 
| 
\exist” 
| 
\export” 
| 
\nd” 
| 
\x” 
| 
\imbed” 
| 
\incase” 
| 
\inclose"| 
\inltrate” 
| 
\infuse” 
| 
\inhabit” 
| 
\join” 
| 
\juxtapose” 
| 
\lie” 
| 
\localize” 
| 
\locate” 
| 
\lodge” 
| 
\notice” 
| 
\perch” 
| 
\place” 
| 
\plant” 
| 
\posit” 
| 
\position” 
| 
\put” 
| 
\remain” 
| 
\reposit” 
| 
\root” 
| 
\seat” 
| 
\see” 
| 
\set” 
| 
\settle” 
| 
\situate” 
| 
\station” 
| 
\surround” 
| 
\touch” 


location 
noun 
. 
\association” 
| 
\coexistence” 
| 
\contact” 
| 
\localization” 
| 
\location” 
| 
\presence” 




location 
marker 
. 


location 
words/Verb 
| 


location 
words/Adjective 
| 


location 
noun 


negatives 
. 
\not” 
| 
\do 
not” 
| 
\no” 
| 
\don 
't” 


passive 
voice 
. 
/pastTenseVerb 
| 
/pastParticipleVerb 


In 
Section 
7.4, 
Figure 
27 
refers 
to 
three 
dierent 
groups 
of 
advice 
rules. 
The 
rst 
group, 
called 
Group 
A, 
only 
included 
the 
advice 
rules 
listed 
in 
protein 
location 
rules 
A. 
The 
second 
group, 
called 
Group 
B, 
cont
ained 
the 
union 
of 
the 
advice 
rules 
in 
protein 
location 
rules 
A 
and 
prot
ein 
location 
rules 
B. 
Finally, 
the 
third 
group, 
called 
Group 
C, 
had 
the 
union 
of 
the 
advice 
rules 
in 
protein 
location 
rules 
A, 
protein 
location 
rules 
B, 
and 
protein 
location 
rules 
C. 


The 
following 
advice 
rules 
indicate 
properties 
of 
proteins 
and 
their 
locations. 
These 
rules 
were 
not 
compiled 
into 
the 
network. 
Instead, 
they 
were 
used 
as 
special 
words 
in 
the 
prior 
mean 
score 
for 
the 
stochastic 
selector 
(see 
Section 
6.3). 


• 
Common 
locations 
within 
a 
cell 
are 
\nucleus,i,” 
\membrane(s),” 
\golgi,” 
\vesic
le(s),” 
\cytoskeleton(s),” 
\mitochondrion,a,” 
\endoplasmic 
reticulum,i,” 
\cyt
oplasm,” 
\vacuole(s),” 
\cell 
wall(s).” 
• 
Terms 
\locus,” 
\chromosome,” 
or 
\band” 
are 
found 
within 
any 
of 
the 
protein 
or 
location 
phrases. 
D.2 
My 
Rules 
I 
wrote 
the 
following 
rules 
for 
the 
subcellular-localization 
domain. 
Experiments 
on 
these 
advice 
rules 
are 
reported 
in 
Eliassi-Rad 
and 
Shavlik 
(2001b). 
Please 
note 
that 
except 
for 
the 
rules 
referring 
to 
the 
terminals 
in 
protein 
associates, 
the 
rest 
of 
the 
rules 
can 
be 
used 
in 
any 
task 
that 
is 
about 
locating 
an 
object. 




protein 
location 
rules 
. 


WHEN 
nounPhrase(?ProteinName) 
THEN 
strongly 
suggest 
show 
page 
| 


WHEN 
nounPhrase(?LocationName] 
THEN 
strongly 
suggest 
show 
page 
| 


WHEN 
consecutive(protein 
name 
protein 
associates) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
/VerbPhrase 
location 
name) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
/VerbPhrase 
/PrepositionalPhrase 
location 
name) 


THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
/VerbPhrase) 
THEN 
weakly 
suggest 
showing 
page 
| 


WHEN 
consecutive(/VerbPhrase 
location 
name) 
THEN 
weakly 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
\at” 
location 
name) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(· 
\in” 
location 
name 
\of” 
protein 
name) 
THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
\and” 
/NounPhrase 
/VerbPhrase 
location 
name) 


THEN 
suggest 
showing 
page 
| 


WHEN 
consecutive(protein 
name 
\and” 
/NounPhrase 
\at” 
location 
name) 


THEN 
suggest 
showing 
page 
| 




WHEN 
consecutive(protein 
name 
\and” 
/NounPhrase 
\in” 
location 
name) 
THEN 
suggest 
showing 
page 
protein 
name 
. 
?ProteinName/NounPhrase 
location 
name 
. 
?LocationName/NounPhrase 
protein 
associates 
. 
\protein” 
| 
\mutant” 
| 
\gene” 




Appendix 
E 


Advice 
Used 
in 
the 
Disorder-Association 
Extractor 


This 
appendix 
presents 
the 
advice 
rules 
for 
the 
disorder-association 
extractor 
agent. 
These 
rules 
were 
written 
by 
Michael 
Waddell, 
who 
is 
an 
M.D./Ph.D. 
student 
at 
University 
of 
Wisconsin-Madison. 
When 
writing 
these 
rules, 
Mr. 
Waddell 
focused 
only 
on 
how 
to 
teach 
the 
task 
to 
another 
person 
who 
could 
read 
basic 
English, 
but 
was 
unfamiliar 
with 
the 
eld 
of 
biochemistry 
and 
its 
terminology. 


Recall 
that 
the 
function 
named 
consecutive 
takes 
a 
sequence 
of 
words 
and 
returns 
true 
if 
they 
appear 
as 
a 
phrase 
on 
the 
page. 
Otherwise, 
it 
returns 
false. 
Also, 
· 
is 
one 
of 
Wawa's 
wild 
card 
tokens 
and 
represents 
any 
single 
word 
or 
punctuation. 
Wawa's 
other 
wild 
card 
token 
is 
, 
which 
represents 
zero 
of 
more 
words 
or 
punctuations. 


For 
this 
domain, 
I 
removed 
the 
stop 
words 
and 
stemmed 
the 
remaining 
words. 
The 
advice 
rules 
in 
Backus-Naur 
form 
(Aho, 
Sethi, 
and 
Ullman 
1986) 
are 
listed 
below. 


gene 
disease 
rules 
A 
. 


WHEN 
nounPhrase(?GeneName) 
THEN 
strongly 
suggest 
show 
page 
| 


WHEN 
nounPhrase(?DiseaseName) 
THEN 
strongly 
suggest 
show 
page 



gene 
disease 
rules 
B 
. 


WHEN 
(gene 
name/unknownWord 
or 
gene 
name/cardinalNumber) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(gene 
name 
gene 
trailers) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
(consecutive(disease 
name 
disease 
associates) 
OR 
consecutive(disease 
associates 
disease 
name)) 


THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
consecutive(disease 
name 
disease 
trailers) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
(consecutive(verb 


or 
adj 
. 
gene 
name) 
OR 
consecutive(gene 
name 
. 
verb 
or 
adj)) 
THEN 
suggest 
showing 
page 
| 


WHEN 
(consecutive(verb 


or 
adj 
. 
disease 
name) 
OR 
consecutive(disease 


name 
. 
verb 
or 
adj 
)) 
THEN 
suggest 
showing 
page 


gene 
disease 
rules 
C 
. 


WHEN 
(consecutive(gene 
name 
, 
disease 
name) 
OR 
consecutive(disease 


name 
, 
gene 
name)) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
(consecutive(gene 
name 
. 
verb 
or 
adj 
. 
disease 
name) 
OR 
consecutive(disease 


name 
. 
verb 
or 
adj 
. 
gene 
name)) 
THEN 
strongly 
suggest 
showing 
page 
| 


WHEN 
(consecutive(verb 


or 
adj 
. 
negatives 
. 
gene 
name) 
OR 
consecutive(gene 
name 
. 
negatives 
. 
verb 
or 
adj)) 
THEN 
avoid 
showing 
page 
| 




WHEN 
(consecutive(verb 


or 
adj 
. 
negatives 
. 
disease 
name) 
OR 
consecutive(disease 


name 
. 
negatives 
. 
verb 
or 
adj)) 
THEN 
avoid 
showing 
page 
| 


WHEN 
(consecutive(gene 


name 
. 
verb 
or 
adj 
negatives 
. 
disease 
name) 
OR 
consecutive(gene 
name 
. 
negatives 
verb 
or 
adj 
. 
disease 
name) 
OR 
consecutive(disease 
name 
. 
negatives 
verb 
or 
adj 
. 
gene 
name) 
OR 
consecutive(disease 
name 
. 
verb 
or 
adj 
negatives 
. 
gene 
name)) 


THEN 
strongly 
avoid 
showing 
page 
| 


WHEN 
consecutive(gene 
name 
. 
passive 
voice 
. 
disease 
name) 
THEN 
suggest 
showing 
page 
gene 
name 
. 
?GeneName/NounPhrase 
disease 
name 
. 
?DiseaseName/NounPhrase 
gene 
trailers 
. 
\deletion” 
| 
\inversion” 
| 
\mutation” 
| 
\carrier” 
| 
\transporter” 
| 
\receptor” 
| 
\gene” 
| 
\locus” 
| 
\loci” 
| 
\variant” 
| 
\allele” 
gene 
words 
. 
\present” 
| 
\isolate” 
| 
\associate” 
| 
\mediate” 
| 
\link” 
| 
\mutate” 
| 
\complement” 
| 
\predispose” 
| 
\cause” 
| 
\lead” 
| 
\explain” 
| 
\increase” 
| 
\produce” 
| 
\result” 
disease 
associates 
. 
\cancer” 
| 
\tumor” 
| 
\tumour” 
| 
\dysplasia” 
| 
\syndrome” 
| 
\disorder” 
| 
\disease” 
| 
\dystrophy” 
| 
"deciency” 
| 
\familial” 
| 
\human” 
disease 
trailers 
. 
\patient” 
| 
\family” 
verb 
or 
adj 
. 
gene 
words/Verb 
| 
gene 
words/Adjectives 
negatives 
. 
\not” 
| 
\do 
not” 
| 
\no” 
| 
\don 
't” 
passive 
voice 
. 
/pastTenseVerb 
| 
/pastParticipleVerb 




In 
Section 
7.4, 
Figure 
29 
refers 
to 
three 
dierent 
groups 
of 
advice 
rules. 
The 
rst 
group, 
called 
Group 
A, 
only 
included 
the 
advice 
rules 
listed 
in 
gene 
disease 
rules 
A. 
The 
second 
group, 
called 
Group 
B, 
contained 
the 
union 
of 
the 
advice 
rules 
in 
gene 
disease 
rules 
A 
and 
gene 
disease 
rules 
B. 
Fin
ally, 
the 
third 
group, 
called 
Group 
C, 
had 
the 
union 
of 
the 
advice 
rules 
in 
gene 
disease 
rules 
A, 
gene 
disease 
rules 
B, 
and 
gene 
disease 
rules 
C. 


The 
following 
advice 
rules 
indicate 
properties 
of 
genes 
and 
their 
genetic 
disorders. 
These 
rules 
were 
not 
compiled 
into 
the 
network. 
Instead, 
they 
were 
used 
as 
special 
words 
in 
the 
prior 
mean 
score 
for 
the 
stochastic 
selector 
(see 
Section 
6.3). 


• 
Sometimes 
the 
gene 
name 
and 
the 
disease 
name 
are 
the 
same. 
• 
Disease 
names 
commonly 
have 
one 
of 
the 
following 
suxes: 
\-oma,” 
\-uria,” 
\-emia,” 
\-ism,” 
\-opathy,” 
\-osis,” 
and 
\-itis.” 
• 
A 
common 
type 
of 
disease 
is 
a 
syndrome. 
That 
is, 
disease 
name 
. 
\gene 
name 
syndrome” 
| 
\gene 
name 
syndromes” 


• 
A 
common 
type 
of 
disease 
is 
a 
deciency 
in 
a 
gene. 
That 
is, 
disease 
name 
. 
\gene 
name 
deciency” 
| 
\gene 
name 
deciencies” 
| 
\deciency 
in 
gene 
name” 
| 
\deciencies 
in 
gene 
name” 




Bibliography 


Y. 
Abu-Mostafa 
(1995). 
Hints. 
Neural 
Computation 
7, 
639{671. 
A. 
Aho, 
R. 
Sethi, 
and 
J. 
Ullman 
(1986). 
Compilers, 
Principles, 
Techniques 
and 
Tools. 
Reading, 
MA: 
Addison 
Wesley. 
M. 
Balabanovic 
and 
Y. 
Shoham 
(1997). 
Fab: 
Content{based, 
collaborative 
recommendation. 
Communications 
of 
ACM 
40, 
66{72. 
S. 
Baluja, 
V. 
Mittal, 
and 
R. 
Sukthankar 
(1999). 
Applying 
machine 
learni
ng 
for 
high 
performance 
named-entity 
extraction. 
In 
Proceedings 
of 
Pacic 
Association 
for 
Computational 
Linguistics, 
Ontario, 
Canada. 
Blackwell. 
R. 
K. 
Belew 
(2000). 
Finding 
Out 
About: 
A 
Cognitive 
Perspective 
on 
Search 
Engine 
Technology 
and 
the 
WWW. 
New 
York, 
NY: 
Cambridge 
University 
Press. 
D. 
Bikel, 
R. 
Schwartz, 
and 
R. 
Weischedel 
(1999). 
An 
algorithm 
that 
learns 
what's 
in 
a 
name. 
Machine 
Learning: 
Special 
Issue 
on 
Natural 
Language 
Learning 
34 
(1/3), 
211{231. 
M. 
Botta 
and 
R. 
Piola 
(1999). 
Rening 
numerical 
constants 
in 
rst 
order 
logic 
theories. 
Machine 
Learning 
38, 
109{131. 
J. 
Boyan 
and 
A. 
Moore 
(1998). 
Learning 
evaluation 
functions 
for 
global 
opt
imization 
and 
boolean 
satisability. 
In 
Proceedings 
of 
the 
Fifteenth 
National 
Conference 
on 
Articial 
Intelligence, 
Madison, 
WI, 
3{10. 
AAAI 
Press. 
J. 
Boyan 
and 
A. 
Moore 
(2000). 
Learning 
evaluation 
functions 
to 
improve 
optimization 
by 
local 
search. 
Journal 
of 
Machine 
Learning 
Research 
1, 
77– 
112. 

E. 
Brill 
(1994). 
Some 
advances 
in 
rule-based 
part 
of 
speech 
tagging. 
In 
Proc
eedings 
of 
the 
Twelfth 
National 
Conference 
on 
Articial 
Intelligence, 
Seattle, 
WA, 
722{727. 
AAAI 
Press. 
S. 
Brin 
and 
L. 
Page 
(1998). 
The 
anatomy 
of 
a 
large-scale 
hypertextual 
Web 
search 
engine. 
Computer 
Networks 
and 
ISDN 
Systems 
30, 
107{117. 
M. 
E. 
Califf 
(1998). 
Relational 
Learning 
Techniques 
for 
Natural 
Language 
Information 
Extraction. 
Ph. 
D. 
Thesis, 
Department 
of 
Computer 
Sciences, 
University 
of 
Texas, 
Austin, 
TX. 
M. 
E. 
Califf 
and 
R. 
Mooney 
(1999). 
Relational 
learning 
of 
pattern-match 
rules 
for 
information 
extraction. 
In 
Proceedings 
of 
the 
Sixteenth 
National 
Conference 
on 
Articial 
Intelligence, 
Orlando, 
FL, 
328{334. 
AAAI 
Press. 
E. 
Coman, 
M. 
Garey, 
and 
D. 
Johnson 
(1996). 
Approximation 
Algorithms 
for 
Bin 
Packing: 
A 
Survey. 
In 
D. 
Hochbaum 
(Ed.), 
Approximation 
Algorithms 
for 
NP-Hard 
Problems. 
PWS 
Publishing. 
M. 
Craven 
and 
J. 
Kumlien 
(1999). 
Constructing 
biological 
knowledge-bases 
by 
extracting 
information 
from 
text 
sources. 
In 
Proceedings 
of 
the 
Seventh 
International 
Conference 
on 
Intelligent 
Systems 
for 
Molecular 
Biology, 
Heid
elberg, 
Germany, 
77{86. 
AAAI 
Press. 
M. 
W. 
Craven 
and 
J. 
W. 
Shavlik 
(1992). 
Visualizing 
learning 
and 
comp
utation 
in 
articial 
neural 
networks. 
International 
Journal 
on 
Articial 
Intelligence 
Tools 
1 
(3), 
399{425. 
M. 
W. 
Craven 
and 
J. 
W. 
Shavlik 
(1996). 
Extracting 
tree-structured 
represent
ations 
of 
trained 
networks. 
In 
Advances 
in 
Neural 
Information 
Processing 
Systems, 
Volume 
8, 
Denver, 
CO, 
24{30. 
MIT 
Press. 
N. 
Cristianini 
and 
J. 
Shawe-Taylor 
(2000). 
An 
Introduction 
to 
Support 
Vector 

Machines 
and 
Other 
Kernel-Based 
Learning 
Methods. 
Cambridge 
University 
Press. 


W. 
Croft, 
H. 
Turtle, 
and 
D. 
Lewis 
(1991). 
The 
use 
of 
phrases 
and 
structured 
queries 
in 
information 
retrieval. 
In 
Proceedings 
of 
the 
Fourteenth 
International 
ACM 
SIGIR 
Conference 
on 
R 
& 
D 
in 
Information 
Retrieval, 
Chicago, 
IL, 
32– 
45. 
ACM 
Press. 
A. 
Dempster, 
N. 
Laird, 
and 
D. 
Rubin 
(1977). 
Maximum 
likelihood 
from 
incomplete 
data 
via 
the 
EM 
algorithm. 
Journal 
of 
the 
Royal 
Statistical 
Socie
ty 
39 
(1), 
1{38. 
J. 
Diederich 
(1989). 
Learning 
by 
instruction 
in 
connectionist 
systems. 
In 
Proc
eedings 
of 
the 
Sixth 
International 
Workshop 
on 
Machine 
Learning, 
Ithaca, 
NY, 
66{68. 
Morgan 
Kaufmann. 
C. 
Drummond, 
D. 
Ionescu, 
and 
R. 
Holte 
(1995). 
A 
learning 
agent 
that 
assists 
the 
browsing 
of 
software 
libraries. 
Technical 
Report 
TR-95-12, 
University 
of 
Ottawa, 
Ottawa, 
Canada. 
T. 
Eliassi-Rad 
and 
J. 
Shavlik 
(2001a). 
A 
system 
for 
building 
intelligent 
agents 
that 
learn 
to 
retrieve 
and 
extract 
information. 
International 
Journal 
on 
User 
Modeling 
and 
User-Adapted 
Interaction, 
Special 
Issue 
on 
User 
Modeling 
and 
Intelligent 
Agents. 
(To 
appear). 
T. 
Eliassi-Rad 
and 
J. 
Shavlik 
(2001b). 
A 
theory-renement 
approach 
to 
inform
ation 
extraction. 
In 
Proceedings 
of 
the 
Eighteenth 
International 
Conference 
on 
Machine 
Learning, 
Williamstown, 
MA, 
130{137. 
Morgan 
Kaufmann. 
J. 
Elman 
(1991). 
Distributed 
representations, 
simple 
recurrent 
networks, 
and 
grammatical 
structure. 
Machine 
Learning 
7, 
195{225. 
O. 
Etzioni 
and 
D. 
Weld 
(1995). 
Intelligent 
agents 
on 
the 
internet: 
Fact, 
ction, 
and 
forecast. 
IEEE 
Expert 
10, 
44{49. 

W. 
B. 
Frakes 
and 
R. 
Baeza-Yates 
(1992). 
Information 
Retrieval: 
Data 
Struct
ures 
& 
Algorithms. 
Prentice 
Hall. 
D. 
Freitag 
(1998a). 
Information 
extraction 
from 
HTML: 
Application 
of 
a 
general 
machine 
learning 
approach. 
In 
Proceedings 
of 
the 
Fifteenth 
National 
Conference 
on 
Articial 
Intelligence, 
Madison, 
WI, 
517{523. 
AAAI 
Press. 
D. 
Freitag 
(1998b). 
Machine 
Learning 
for 
Information 
Extraction 
in 
Informal 
Domains. 
Ph. 
D. 
Thesis, 
Computer 
Science 
Department, 
Carnegie 
Mellon 
University, 
Pittsburgh, 
PA. 
D. 
Freitag 
(1998c). 
Relational 
learning 
of 
pattern-match 
rules 
for 
informat
ion 
extraction. 
In 
Proceedings 
of 
the 
Fifteenth 
International 
Conference 
on 
Machine 
Learning, 
Madison, 
WI, 
161{169. 
Morgan 
Kaufmann. 
D. 
Freitag 
and 
N. 
Kushmerick 
(2000). 
Boosted 
wrapper 
induction. 
In 
Proc
eedings 
of 
the 
Seventeenth 
National 
Conference 
on 
Articial 
Intelligence, 
Austin, 
TX, 
577{583. 
AAAI 
Press. 
D. 
Freitag 
and 
A. 
McCallum 
(1999). 
Information 
extraction 
with 
HMMs 
and 
shrinkage. 
In 
Notes 
of 
the 
Sixteenth 
National 
Conference 
on 
Articial 
Intellig
ence 
Workshop 
on 
Machine 
Learning 
for 
Information 
Extraction, 
Orlando, 
FL, 
31{36. 
AAAI 
Press. 
D. 
Freitag 
and 
A. 
McCallum 
(2000). 
Information 
extraction 
with 
HMM 
struct
ures 
learned 
by 
stochastic 
optimization. 
In 
Proceedings 
of 
the 
Seventeenth 
National 
Conference 
on 
Articial 
Intelligence, 
Austin, 
TX, 
584{589. 
AAAI 
Press. 
J. 
Goecks 
and 
J. 
Shavlik 
(2000). 
Learning 
users’ 
interests 
by 
unobtrusively 
observing 
their 
normal 
behavior. 
In 
Proceedings 
of 
the 
2000 
International 
Conference 
on 
Intelligent 
User 
Interfaces, 
New 
Orleans, 
LA, 
129{132. 
ACM 
Press. 

D. 
Gordon 
and 
D. 
Subramanian 
(1994). 
A 
multistrategy 
learning 
scheme 
for 
agent 
knowledge 
acquisition. 
Informatica 
17, 
331{346. 
T. 
Joachims, 
D. 
Freitag, 
and 
T. 
Mitchell 
(1997). 
WebWatcher: 
A 
tour 
guide 
for 
the 
World 
Wide 
Web. 
In 
Proceedings 
of 
the 
Sixteenth 
International 
Joint 
Conference 
on 
Articial 
Intelligence, 
Nagoya, 
Japan, 
770{775. 
AAAI 
Press. 
N. 
Kushmerick 
(2000). 
Wrapper 
induction: 
Eciency 
and 
expressiveness. 
Articial 
Intelligence 
118, 
15{68. 
S. 
Lawrence 
and 
C. 
L. 
Giles 
(1999). 
Accessibility 
of 
information 
on 
the 
web. 
Nature 
400 
(6740), 
107{109. 
T. 
Leek 
(1997). 
Information 
extraction 
using 
hidden 
markov 
models. 
Mast
er's 
Thesis, 
Department 
of 
Computer 
Science 
& 
Engineering, 
University 
of 
California, 
San 
Diego. 
W. 
Lehnert 
(2000). 
Information 
Extraction. 
http://wwwn
lp.cs.umass.edu/nlpie.html. 
H. 
Lieberman 
(1995). 
Letzia: 
An 
agent 
that 
assists 
web 
browsing. 
In 
Proc
eedings 
of 
the 
Fourteenth 
International 
Joint 
Conference 
on 
Articial 
Intell
igence, 
Montreal, 
Canada, 
924{929. 
AAAI 
Press. 
R. 
Maclin 
(1995). 
Learning 
from 
Instruction 
and 
Experience: 
Methods 
for 
Inc
orporating 
Procedural 
Domain 
Theories 
into 
Knowledge-Based 
Neural 
Netw
orks. 
Ph. 
D. 
Thesis, 
Department 
of 
Computer 
Sciences, 
University 
of 
Wisc
onsin, 
Madison, 
WI. 
(Also 
appears 
as 
UW 
Technical 
Report 
CS-TR-951
285). 
R. 
Maclin 
and 
J. 
Shavlik 
(1996). 
Creating 
advice-taking 
reinforcement 
learne
rs. 
Machine 
Learning 
22, 
251{281. 
A. 
McCallum 
and 
K. 
Nigam 
(1998). 
A 
comparison 
of 
event 
models 
for 
naive 
Bayes 
text 
classication. 
In 
Notes 
of 
the 
Fifteenth 
National 
Conference 
on 

Articial 
Intelligence 
Workshop 
on 
Learning 
for 
Text 
Categorization, 
Madis
on, 
WI, 
41{48. 
AAAI 
Press. 


A. 
McCallum, 
K. 
Nigam, 
J. 
Rennie, 
and 
K. 
Seymore 
(1999). 
Building 
domain-
specic 
search 
engines 
with 
machine 
learning 
techniques. 
In 
American 
Associa
tion 
for 
Articial 
Intelligence 
Spring 
1999 
Symposium, 
Stanford 
University, 
CA, 
28{39. 
AAAI 
Press. 
A. 
McCallum, 
K. 
Nigam, 
J. 
Rennie, 
and 
K. 
Seymore 
(2000). 
Automating 
the 
construction 
of 
internet 
portals 
with 
machine 
learning. 
Information 
Retrieval 
Journal 
3, 
127{163. 
A. 
McCallum, 
R. 
Rosenfeld, 
T. 
Mitchell, 
and 
A. 
Ng 
(1998). 
Improving 
text 
classication 
by 
shrinkage 
in 
a 
hierarchy 
of 
classes. 
In 
Proceedings 
of 
the 
Fifteenth 
International 
Conference 
on 
Machine 
Learning, 
Madison, 
WI, 
359– 
367. 
Morgan 
Kaufmann. 
G. 
Miller 
(1995). 
WordNet: 
A 
lexical 
database 
for 
English. 
Communications 
of 
the 
ACM 
38, 
39{41. 
T. 
Mitchell 
(1997). 
Machine 
Learning. 
McGraw-Hill. 
S. 
Muggleton 
(1995). 
Foundations 
of 
inductive 
logic 
programming. 
Englewood 
Clis, 
NJ: 
Prentice 
Hall. 
NLM 
(2001). 
National 
Library 
of 
Medicine's 
MEDLINE 
Database. 
http://www.ncbi.nlm.nih.gov/PubMed/. 


D. 
Ourston 
and 
R. 
Mooney 
(1994). 
Theory 
renement: 
Combining 
analytical 
and 
empirical 
methods. 
Articial 
Intelligence 
66, 
273{309. 
M. 
Pazzani 
and 
D. 
Kibler 
(1992). 
The 
utility 
of 
knowledge 
in 
inductive 
learning. 
Machine 
Learning 
9, 
57{94. 

M. 
Pazzani, 
J. 
Muramatsu, 
and 
D. 
Billsus 
(1996). 
Syskill 
& 
Webert: 
Identifyi
ng 
interesting 
web 
sites. 
In 
Proceedings 
of 
the 
Thirteenth 
National 
Conference 
on 
Articial 
Intelligence, 
Portland, 
OR, 
54{61. 
AAAI 
Press. 
M. 
F. 
Porter 
(1980). 
An 
algorithm 
for 
sux 
stripping. 
Program 
14 
(3), 
130– 
137. 
J. 
Quinlan 
(1990). 
Learning 
logical 
denitions 
from 
relations. 
Machine 
Learni
ng 
5, 
239{266. 
L. 
Rabiner 
and 
B. 
Juang 
(1993). 
Fundamentals 
of 
Speech 
Recognition. 
Eng
lewood 
Clis, 
NJ: 
Prentice-Hall. 
S. 
Ray 
and 
M. 
Craven 
(2001). 
Representing 
sentence 
structure 
in 
hidden 
markov 
models 
for 
information 
extraction. 
In 
Proceedings 
of 
the 
Seventeenth 
International 
Joint 
Conference 
on 
Articial 
Intelligence, 
Seattle, 
WA, 
1273– 
1279. 
AAAI 
Press. 
J. 
Rennie 
and 
A. 
McCallum 
(1999). 
Using 
reinforcement 
learning 
to 
spider 
the 
web 
eciently. 
In 
Proceedings 
of 
the 
Sixteenth 
International 
Conference 
on 
Machine 
Learning, 
Bled, 
Slovenia, 
335{343. 
Morgan 
Kaufmann. 
E. 
Rich 
and 
K. 
Knight 
(1991). 
Articial 
Intelligence. 
McGraw 
Hill. 
E. 
Riloff 
(1996). 
Using 
learned 
extraction 
patterns 
for 
text 
classication. 
In 
S. 
Wermter, 
E. 
Rilo, 
and 
G. 
Scheler 
(Eds.), 
Connectionist, 
Statistical, 
and 
Symbolic 
Approaches 
to 
Learning 
for 
Natural 
Language 
Processing, 
275{289. 
Springer-Verlag. 
E. 
Riloff 
(1998). 
The 
Sundance 
Sentence 
Analyzer. 
http://www.cs.utah.edu/projects/nlp/. 
E. 
Riloff 
and 
W. 
Lehnert 
(1994). 
Information 
extraction 
as 
a 
basis 
for 
high-precision 
text 
classication. 
ACM 
Transactions 
on 
Information 
Syst
ems 
12 
(3), 
296{333. 

D. 
E. 
Rose 
(1994). 
A 
Symbolic 
and 
Connectionist 
Approach 
to 
Legal 
Inform
ation 
Retrieval. 
Lawrence 
Erlbaum 
Associates. 
D. 
Rumelhart, 
G. 
Hinton, 
and 
R. 
Williams 
(1986). 
Learning 
internal 
repres
entations 
by 
error 
propagation. 
In 
D. 
Rumelhart 
and 
J. 
McClelland 
(Eds.), 
Parallel 
Distributed 
Processing: 
Explorations 
in 
the 
Microstructure 
of 
Cognit
ion, 
Volume 
1, 
318{363. 
MIT 
Press. 


D. 
Rumelhart 
and 
J. 
McClelland 
(1986). 
Parallel 
Distributed 
Processing: 
Explorations 
in 
the 
Microstructure 
of 
Cognition. 
MIT 
Press. 
S. 
Russell 
and 
P. 
Norvig 
(1995). 
Articial 
Intelligence: 
A 
Modern 
Approach. 
Prentice 
Hall. 
G. 
Salton 
(1991). 
Developments 
in 
automatic 
text 
retrieval. 
Science 
253, 
974{979. 
G. 
Salton 
and 
C. 
Buckley 
(1988). 
Term-weighting 
approaches 
in 
automatic 
text 
retrieval. 
Information 
Processing 
and 
Management 
24 
(5), 
513{523. 
R. 
Schapire 
and 
Y. 
Singer 
(1998). 
Improved 
boosting 
algorithms 
using 
condence-rated 
predictions. 
In 
Proceedings 
of 
the 
Eleventh 
Annual 
Confere
nce 
on 
Computational 
Learning 
Theory, 
Madison, 
WI, 
80{91. 
ACM 
Press. 
T. 
Sejnowski 
and 
C. 
Rosenberg 
(1987). 
Parallel 
networks 
that 
learn 
to 
pron
ounce 
English 
text. 
Complex 
Systems 
1, 
145{168. 
B. 
Selman, 
H. 
Kautz, 
and 
B. 
Cohen 
(1996). 
Local 
search 
strategies 
for 
sati
sability 
testing. 
DIMACS 
Series 
in 
Discrete 
Mathematics 
and 
Theoretical 
CS 
26, 
521{531. 
K. 
Seymore, 
A. 
McCallum, 
and 
R. 
Rosenfeld 
(1999). 
Learning 
hidden 
Markov 
model 
structure 
for 
information 
extraction. 
In 
Notes 
of 
the 
Sixteenth 
National 
Conference 
on 
Articial 
Intelligence 
Workshop 
on 
Machine 
Learning 
for 
Inf
ormation 
Extraction, 
Orlando, 
FL, 
37{42. 
AAAI 
Press. 

J. 
Shakes, 
M. 
Langheinrich, 
and 
O. 
Etzioni 
(1997). 
Dynamic 
reference 
sifting: 
A 
case 
study 
in 
the 
homepage 
domain. 
In 
Proceedings 
of 
the 
Sixth 
Internat
ional 
World 
Wide 
Web 
Conference, 
Santa 
Clara, 
CA, 
189{200. 
J. 
Shavlik, 
S. 
Calcari, 
T. 
Eliassi-Rad, 
and 
J. 
Solock 
(1999). 
An 
instructable, 
adaptive 
interface 
for 
discovering 
and 
monitoring 
information 
on 
the 
worldw
ide 
web. 
In 
Proceedings 
of 
the 
1999 
International 
Conference 
on 
Intelligent 
User 
Interfaces, 
Redondo 
Beach, 
CA, 
157{160. 
ACM 
Press. 
J. 
Shavlik 
and 
T. 
Eliassi-Rad 
(1998a). 
Building 
intelligent 
agents 
for 
web-
based 
tasks: 
A 
theory-renement 
approach. 
In 
Proceedings 
of 
the 
Conference 
on 
Automated 
Learning 
and 
Discovery 
Workshop 
on 
Learning 
from 
Text 
and 
the 
Web, 
Pittsburgh, 
PA. 
J. 
Shavlik 
and 
T. 
Eliassi-Rad 
(1998b). 
Intelligent 
agents 
for 
web-based 
tasks: 
An 
advice-taking 
approach. 
In 
Notes 
of 
the 
Fifteenth 
National 
Conference 
on 
Articial 
Intelligence 
Workshop 
on 
Learning 
for 
Text 
Categorization, 
Madis
on, 
WI, 
63{70. 
AAAI 
Press. 
S. 
Soderland 
(1997). 
Learning 
to 
extract 
text-based 
information 
from 
the 
World 
Wide 
Web. 
In 
Proceedings 
of 
the 
Third 
International 
Conference 
on 
Knowledge 
Discovery 
and 
Data 
Mining, 
Newport 
Beach, 
CA, 
251{254. 
AAAI 
Press. 
S. 
Soderland 
(1999). 
Learning 
information 
extraction 
rules 
for 
semi-
structured 
and 
free 
text. 
Machine 
Learning: 
Special 
Issue 
on 
Natural 
Lang
uage 
Learning 
34 
(1/3), 
233{272. 
R. 
Sutton 
(1988). 
Learning 
to 
predict 
by 
the 
methods 
of 
temporal 
dierences. 
Machine 
Learning 
3, 
9{44. 
R. 
S. 
Sutton 
and 
A. 
G. 
Barto 
(1998). 
Reinforcement 
Learning. 
MIT 
Press. 

G. 
G. 
Towell 
and 
J. 
W. 
Shavlik 
(1994). 
Knowledge-based 
articial 
neural 
networks. 
Articial 
Intelligence 
70 
(1/2), 
119{165. 
C. 
J. 
van 
Rijsbergen 
(1979). 
Information 
Retrieval 
(second 
ed.). 
London: 
Buttersworths. 
C. 
Watkins 
(1989). 
Learning 
from 
Delayed 
Rewards. 
Ph. 
D. 
Thesis, 
King's 
College, 
Cambridge. 
M. 
Wooldridge 
and 
N. 
Jennings 
(1995). 
Intelligent 
agents: 
Theory 
and 
pract
ice. 
Knowledge 
Engineering 
Review 
10, 
45{52. 
H. 
Zaragoza 
and 
P. 
Gallinari 
(1998). 
An 
automatic 
surface 
information 
ext
raction 
system 
using 
hierarchical 
IR 
and 
stochastic 
IE. 
In 
Notes 
of 
the 
Tenth 
European 
Conference 
on 
Machine 
Learning 
Workshop 
on 
Text 
Mining, 
Chemnitz, 
Germany. 
Springer-Verlag. 

